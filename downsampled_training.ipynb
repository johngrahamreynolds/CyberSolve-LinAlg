{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a3f8ad7a-c089-4134-bc82-10d87634c17b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Downsampled Training\n",
    "Before we potentially (and expensively) train on all 2M examples, let's see how training works on a smaller, downsampled dataset. We'll evaluate our main `exact_match` result and compare it to the initial benchmark. From here we can begin trying to better understand the trends within the partial correctness scores.\n",
    "\n",
    "Just as we did in the initial Benchmarking notebook, we install Nvidia `apex`. As noted before, the `apex` package and its `optimizers` and `normalization` modules will be useful for expedited training. We can leverage the `normalization.FusedRMSLayer` class for acclerated normalization computations while also constructing an improved `FusedAdam` (Adam or AdamW) optimizer in favor of the standard `torch.optim.AdamW` optimizer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "95d45270-d5b4-4e76-9460-737a22b24278",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr></tr></thead><tbody></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "dataframeName": null
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# create a volume to store our wheels that are time consuming to build\n",
    "%sql\n",
    "CREATE VOLUME workspace_dogfood.jgr.wheels;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bab48c9e-9701-4b0d-814d-fdd8b9b1189d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /databricks/python3/lib/python3.11/site-packages (4.41.2)\nCollecting transformers\n  Obtaining dependency information for transformers from https://files.pythonhosted.org/packages/f2/3a/8bdab26e09c5a242182b7ba9152e216d5ab4ae2d78c4298eb4872549cd35/transformers-4.47.1-py3-none-any.whl.metadata\n  Using cached transformers-4.47.1-py3-none-any.whl.metadata (44 kB)\nRequirement already satisfied: filelock in /databricks/python3/lib/python3.11/site-packages (from transformers) (3.13.4)\nCollecting huggingface-hub<1.0,>=0.24.0 (from transformers)\n  Obtaining dependency information for huggingface-hub<1.0,>=0.24.0 from https://files.pythonhosted.org/packages/61/8c/fbdc0a88a622d9fa54e132d7bf3ee03ec602758658a2db5b339a65be2cfe/huggingface_hub-0.27.0-py3-none-any.whl.metadata\n  Using cached huggingface_hub-0.27.0-py3-none-any.whl.metadata (13 kB)\nRequirement already satisfied: numpy>=1.17 in /databricks/python3/lib/python3.11/site-packages (from transformers) (1.23.5)\nRequirement already satisfied: packaging>=20.0 in /databricks/python3/lib/python3.11/site-packages (from transformers) (23.2)\nRequirement already satisfied: pyyaml>=5.1 in /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.11/site-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /databricks/python3/lib/python3.11/site-packages (from transformers) (2022.7.9)\nRequirement already satisfied: requests in /databricks/python3/lib/python3.11/site-packages (from transformers) (2.31.0)\nCollecting tokenizers<0.22,>=0.21 (from transformers)\n  Obtaining dependency information for tokenizers<0.22,>=0.21 from https://files.pythonhosted.org/packages/22/06/69d7ce374747edaf1695a4f61b83570d91cc8bbfc51ccfecf76f56ab4aac/tokenizers-0.21.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n  Using cached tokenizers-0.21.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\nRequirement already satisfied: safetensors>=0.4.1 in /databricks/python3/lib/python3.11/site-packages (from transformers) (0.4.2)\nRequirement already satisfied: tqdm>=4.27 in /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.11/site-packages (from transformers) (4.67.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /databricks/python3/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (2023.5.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /databricks/python3/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (4.10.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /databricks/python3/lib/python3.11/site-packages (from requests->transformers) (2.0.4)\nRequirement already satisfied: idna<4,>=2.5 in /databricks/python3/lib/python3.11/site-packages (from requests->transformers) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.11/site-packages (from requests->transformers) (2.0.7)\nRequirement already satisfied: certifi>=2017.4.17 in /databricks/python3/lib/python3.11/site-packages (from requests->transformers) (2023.7.22)\nUsing cached transformers-4.47.1-py3-none-any.whl (10.1 MB)\nUsing cached huggingface_hub-0.27.0-py3-none-any.whl (450 kB)\nUsing cached tokenizers-0.21.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\nInstalling collected packages: huggingface-hub, tokenizers, transformers\n  Attempting uninstall: huggingface-hub\n    Found existing installation: huggingface-hub 0.23.4\n    Not uninstalling huggingface-hub at /databricks/python3/lib/python3.11/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-199b0440-0871-47dc-b7fe-f77bf9ed2bb6\n    Can't uninstall 'huggingface-hub'. No files were found to uninstall.\n  Attempting uninstall: tokenizers\n    Found existing installation: tokenizers 0.19.0\n    Not uninstalling tokenizers at /databricks/python3/lib/python3.11/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-199b0440-0871-47dc-b7fe-f77bf9ed2bb6\n    Can't uninstall 'tokenizers'. No files were found to uninstall.\n  Attempting uninstall: transformers\n    Found existing installation: transformers 4.41.2\n    Not uninstalling transformers at /databricks/python3/lib/python3.11/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-199b0440-0871-47dc-b7fe-f77bf9ed2bb6\n    Can't uninstall 'transformers'. No files were found to uninstall.\nSuccessfully installed huggingface-hub-0.27.0 tokenizers-0.21.0 transformers-4.47.1\n\u001B[43mNote: you may need to restart the kernel using %restart_python or dbutils.library.restartPython() to use updated packages.\u001B[0m\nRequirement already satisfied: ninja in /databricks/python3/lib/python3.11/site-packages (1.11.1.1)\n\u001B[43mNote: you may need to restart the kernel using %restart_python or dbutils.library.restartPython() to use updated packages.\u001B[0m\nCollecting git+https://github.com/NVIDIA/apex.git\n  Cloning https://github.com/NVIDIA/apex.git to /tmp/pip-req-build-z480xbts\n  Running command git clone --filter=blob:none --quiet https://github.com/NVIDIA/apex.git /tmp/pip-req-build-z480xbts\n  Resolved https://github.com/NVIDIA/apex.git to commit 73375b3bbcb59a5d6ff43f2fafd00b9ecdbe0417\n  Running command git submodule update --init --recursive -q\n  Preparing metadata (pyproject.toml): started\n  Preparing metadata (pyproject.toml): finished with status 'done'\nCollecting packaging>20.6 (from apex==0.1)\n  File was already downloaded /tmp/wheels/packaging-24.2-py3-none-any.whl\nBuilding wheels for collected packages: apex\n  Building wheel for apex (pyproject.toml): started\n  Building wheel for apex (pyproject.toml): still running...\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-21 05:51:02.583206: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\nTo enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Building wheel for apex (pyproject.toml): still running...\n  Building wheel for apex (pyproject.toml): still running...\n  Building wheel for apex (pyproject.toml): still running...\n  Building wheel for apex (pyproject.toml): still running...\n  Building wheel for apex (pyproject.toml): still running...\n  Building wheel for apex (pyproject.toml): still running...\n  Building wheel for apex (pyproject.toml): still running...\n  Building wheel for apex (pyproject.toml): still running...\n  Building wheel for apex (pyproject.toml): still running...\n  Building wheel for apex (pyproject.toml): finished with status 'done'\n  Created wheel for apex: filename=apex-0.1-cp311-cp311-linux_x86_64.whl size=35430838 sha256=c4f1044261ccc7ee405c2d058c401f9dc6d3c41052af431fba8d587b79dc37a4\n  Stored in directory: /tmp/pip-ephem-wheel-cache-org6fkc8/wheels/79/b8/83/5235f93f5bca64242106bf00bd06a198b5c54b8df578ca2f99\nSuccessfully built apex\n"
     ]
    }
   ],
   "source": [
    "# install Nvidia Apex for improved normalization computation and optimization with fused kernels\n",
    "# ensure we have ninja installed to speed up Nvidia apex source compilation\n",
    "!pip install ninja\n",
    "\n",
    "# first build and save the binary for speed up in future installs\n",
    "!pip wheel git+https://github.com/NVIDIA/apex.git --no-cache-dir --no-build-isolation --config-settings \"--build-option=--cpp_ext\" --config-settings \"--build-option=--cuda_ext\" -w /tmp/wheels/\n",
    "\n",
    "# Move the wheel file to the Unity Catalog volume\n",
    "dbutils.fs.mv(\"file:/tmp/wheels/apex-0.1-cp311-cp311-linux_x86_64.whl\", \"dbfs:/Volumes/workspace_dogfood/jgr/wheels/apex-0.1-cp311-cp311-linux_x86_64.whl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fda361cd-344a-47f0-9c7a-460e64cbd84e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /databricks/python3/lib/python3.11/site-packages (4.41.2)\nCollecting transformers\n  Obtaining dependency information for transformers from https://files.pythonhosted.org/packages/f2/3a/8bdab26e09c5a242182b7ba9152e216d5ab4ae2d78c4298eb4872549cd35/transformers-4.47.1-py3-none-any.whl.metadata\n  Downloading transformers-4.47.1-py3-none-any.whl.metadata (44 kB)\n\u001B[?25l     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/44.1 kB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\n\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m44.1/44.1 kB\u001B[0m \u001B[31m2.6 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hRequirement already satisfied: filelock in /databricks/python3/lib/python3.11/site-packages (from transformers) (3.13.4)\nCollecting huggingface-hub<1.0,>=0.24.0 (from transformers)\n  Obtaining dependency information for huggingface-hub<1.0,>=0.24.0 from https://files.pythonhosted.org/packages/61/8c/fbdc0a88a622d9fa54e132d7bf3ee03ec602758658a2db5b339a65be2cfe/huggingface_hub-0.27.0-py3-none-any.whl.metadata\n  Downloading huggingface_hub-0.27.0-py3-none-any.whl.metadata (13 kB)\nRequirement already satisfied: numpy>=1.17 in /databricks/python3/lib/python3.11/site-packages (from transformers) (1.23.5)\nRequirement already satisfied: packaging>=20.0 in /databricks/python3/lib/python3.11/site-packages (from transformers) (23.2)\nRequirement already satisfied: pyyaml>=5.1 in /databricks/python3/lib/python3.11/site-packages (from transformers) (6.0)\nRequirement already satisfied: regex!=2019.12.17 in /databricks/python3/lib/python3.11/site-packages (from transformers) (2022.7.9)\nRequirement already satisfied: requests in /databricks/python3/lib/python3.11/site-packages (from transformers) (2.31.0)\nCollecting tokenizers<0.22,>=0.21 (from transformers)\n  Obtaining dependency information for tokenizers<0.22,>=0.21 from https://files.pythonhosted.org/packages/22/06/69d7ce374747edaf1695a4f61b83570d91cc8bbfc51ccfecf76f56ab4aac/tokenizers-0.21.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n  Downloading tokenizers-0.21.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\nRequirement already satisfied: safetensors>=0.4.1 in /databricks/python3/lib/python3.11/site-packages (from transformers) (0.4.2)\nRequirement already satisfied: tqdm>=4.27 in /databricks/python3/lib/python3.11/site-packages (from transformers) (4.65.0)\nRequirement already satisfied: fsspec>=2023.5.0 in /databricks/python3/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (2023.5.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /databricks/python3/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (4.10.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /databricks/python3/lib/python3.11/site-packages (from requests->transformers) (2.0.4)\nRequirement already satisfied: idna<4,>=2.5 in /databricks/python3/lib/python3.11/site-packages (from requests->transformers) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /databricks/python3/lib/python3.11/site-packages (from requests->transformers) (1.26.16)\nRequirement already satisfied: certifi>=2017.4.17 in /databricks/python3/lib/python3.11/site-packages (from requests->transformers) (2023.7.22)\nDownloading transformers-4.47.1-py3-none-any.whl (10.1 MB)\n\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/10.1 MB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\n\u001B[2K   \u001B[91m━━━━━━━━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m3.6/10.1 MB\u001B[0m \u001B[31m108.7 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\n\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m \u001B[32m10.0/10.1 MB\u001B[0m \u001B[31m146.5 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\n\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m \u001B[32m10.1/10.1 MB\u001B[0m \u001B[31m144.5 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\n\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m10.1/10.1 MB\u001B[0m \u001B[31m77.8 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hDownloading huggingface_hub-0.27.0-py3-none-any.whl (450 kB)\n\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/450.5 kB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\n\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m450.5/450.5 kB\u001B[0m \u001B[31m23.8 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hDownloading tokenizers-0.21.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/3.0 MB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\n\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m \u001B[32m3.0/3.0 MB\u001B[0m \u001B[31m193.7 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\n\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m3.0/3.0 MB\u001B[0m \u001B[31m79.8 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hInstalling collected packages: huggingface-hub, tokenizers, transformers\n  Attempting uninstall: huggingface-hub\n    Found existing installation: huggingface-hub 0.23.4\n    Not uninstalling huggingface-hub at /databricks/python3/lib/python3.11/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-4b26884f-680b-4a70-a491-f35e6dad2f39\n    Can't uninstall 'huggingface-hub'. No files were found to uninstall.\n  Attempting uninstall: tokenizers\n    Found existing installation: tokenizers 0.19.0\n    Not uninstalling tokenizers at /databricks/python3/lib/python3.11/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-4b26884f-680b-4a70-a491-f35e6dad2f39\n    Can't uninstall 'tokenizers'. No files were found to uninstall.\n  Attempting uninstall: transformers\n    Found existing installation: transformers 4.41.2\n    Not uninstalling transformers at /databricks/python3/lib/python3.11/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-4b26884f-680b-4a70-a491-f35e6dad2f39\n    Can't uninstall 'transformers'. No files were found to uninstall.\nSuccessfully installed huggingface-hub-0.27.0 tokenizers-0.21.0 transformers-4.47.1\n\u001B[43mNote: you may need to restart the kernel using %restart_python or dbutils.library.restartPython() to use updated packages.\u001B[0m\nProcessing /Volumes/workspace_dogfood/jgr/wheels/apex-0.1-cp311-cp311-linux_x86_64.whl\nRequirement already satisfied: packaging>20.6 in /databricks/python3/lib/python3.11/site-packages (from apex==0.1) (23.2)\nInstalling collected packages: apex\nSuccessfully installed apex-0.1\n\u001B[43mNote: you may need to restart the kernel using %restart_python or dbutils.library.restartPython() to use updated packages.\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "#install / upgrade transformers and install apex\n",
    "!pip install -U transformers\n",
    "!pip install /Volumes/workspace_dogfood/jgr/wheels/apex-0.1-cp311-cp311-linux_x86_64.whl\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3f98d2fc-1985-459d-811a-c1d3e245ed5e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# confirm the apex library is available\n",
    "from apex import normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6c0038e8-bbe1-4b6a-bd34-c987f4e80705",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/python_shell/dbruntime/huggingface_patches/datasets.py:14: UserWarning: During large dataset downloads, there could be multiple progress bar widgets that can cause performance issues for your notebook or browser. To avoid these issues, use `datasets.utils.logging.disable_progress_bar()` to turn off the progress bars.\n  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# load the full preprocessed training and evaluation datasets\n",
    "from datasets import load_dataset\n",
    "\n",
    "tokenized_train_dataset = load_dataset(\"MarioBarbeque/DeepMind-LinAlg-1D-train\")\n",
    "tokenized_eval_dataset = load_dataset(\"MarioBarbeque/DeepMind-LinAlg-1D-eval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2128ee02-bb98-4d53-8649-8963ffbf163c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 1999998\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_train_dataset = tokenized_train_dataset[\"train\"]\n",
    "tokenized_train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "33f629d0-8cdd-4f31-8c7c-9c76b318b1e9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 100000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 10000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we downsample the training dataset to 100k examples for training and use another 10k for evaluation\n",
    "train_size = 100_000\n",
    "test_size = int(0.1 * train_size)\n",
    "\n",
    "downsampled_dataset = tokenized_train_dataset.train_test_split(\n",
    "    train_size=train_size, test_size=test_size, seed=20\n",
    ")\n",
    "downsampled_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "692e0283-41d7-40c1-9b74-84a2617b64b3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# set the downsampled format to numpy in order to pass it to the seq2seq datacollator\n",
    "# the DataCollatorForSeq2Seq uses numpy arrays to pad the labels\n",
    "downsampled_dataset.set_format(\"numpy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d3e3745c-78fc-43e7-98fe-25490b1112b1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'input_ids': array([array([5175,  162,    3,   18, 4201, 1935,    9, 1768,  668, 1935,    9,\n",
       "               1768,  204, 3951, 3274, 4475, 1935,    9, 1768, 1401, 3166,   21,\n",
       "                  3,    9,    5,    1,    0,    0,    0,    0,    0,    0,    0,\n",
       "                  0,    0,    0])                                               ,\n",
       "        array([5175,  162,    3,  632, 3274,    3, 3708, 1935,  208,    3,   18,\n",
       "                850, 1935,  208,    3,   18,  850, 5062, 1768,  898, 4305,   21,\n",
       "                  3,  208,    5,    1,    0,    0,    0])                       ,\n",
       "        array([5175,  162, 1630, 3274,    3,   18,  102, 1768, 2059,   21,    3,\n",
       "                102,    5,    1,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "                  0,    0,    0])                                               ,\n",
       "        array([ 5175,   162,  1003,  3274,     3,    18, 16169,  1935,     9,\n",
       "                   3,    18,     3,  4729,    21,     3,     9,     5,     1,\n",
       "                   0,     0,     0,     0,     0,     0,     0])             ,\n",
       "        array([ 5175,   162,     3,   632,  3274,     3, 10086,  1935,    52,\n",
       "                1768,  1179,     3,    18,     3, 17864,    21,     3,    52,\n",
       "                   5,     1,     0,     0,     0,     0,     0,     0,     0,\n",
       "                   0,     0,     0])                                         ],\n",
       "       dtype=object),\n",
       " 'attention_mask': array([array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "               1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])                       ,\n",
       "        array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "               1, 1, 1, 1, 0, 0, 0])                                            ,\n",
       "        array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "               0, 0, 0])                                                        ,\n",
       "        array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,\n",
       "               0, 0, 0])                                                        ,\n",
       "        array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n",
       "               0, 0, 0, 0, 0, 0, 0, 0])                                         ],\n",
       "       dtype=object),\n",
       " 'labels': array([array([   3,   18, 3341,    1]), array([   3, 6039,    1,    0]),\n",
       "        array([431,   1,   0]), array([   3, 2292,    1]),\n",
       "        array([   3, 3486,    1,    0])], dtype=object)}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# peek some records\n",
    "downsampled_dataset[\"train\"][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "06f437c0-260e-41cc-8667-43e494eaf3cf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-31 17:35:13.483371: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\nTo enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\nYou are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    }
   ],
   "source": [
    "# reinstantiate our tokenizer and model in bfloat16\n",
    "import torch\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "checkpoint = \"google/flan-t5-large\"\n",
    "tokenizer = T5Tokenizer.from_pretrained(checkpoint)\n",
    "# load the model onto the CPU and let \uD83E\uDD17 accelerate take care of device placement in our training loop\n",
    "# model = T5ForConditionalGeneration.from_pretrained(checkpoint, torch_dtype=torch.bfloat16)\n",
    "model = T5ForConditionalGeneration.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c92fa436-aa03-4c0e-a176-34c9e4abe9af",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# confirm our model weights are in bfloat16\n",
    "# model.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0131a25d-1ed3-4745-beb2-5e210a385f0e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/.ipykernel/2444/command-1975419433870059-2491816732:4: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library \uD83E\uDD17 Evaluate: https://huggingface.co/docs/evaluate\n  exact_match_metric = load_metric(\"exact_match\")\n/databricks/python/lib/python3.11/site-packages/datasets/load.py:759: FutureWarning: The repository for exact_match contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.19.1/metrics/exact_match/exact_match.py\nYou can avoid this message in future by passing the argument `trust_remote_code=True`.\nPassing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# grab our exact match metric\n",
    "from datasets import load_metric\n",
    "\n",
    "exact_match_metric = load_metric(\"exact_match\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a80cd844-36fc-40e5-b180-d1d1e03e8f2d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# define our hyperparameters\n",
    "\n",
    "# the T5 documentation states: \n",
    "# T5 models need a slightly higher learning rate than the default one set in the Trainer when using the AdamW optimizer. Typically, 1e-4 and 3e-4 work well for most problems (classification, summarization, translation, question answering, question generation). Note that T5 was pre-trained using the AdaFactor optimizer.\n",
    "\n",
    "hyperparameters = {\n",
    "    \"learning_rate\": 1e-4, # see the T5 documentation on finetuning learning rate for AdamW\n",
    "    \"num_epochs\": 3,\n",
    "    \"train_batch_size\": 256, # NOTE: 32 originally # Actual batch size will this x num gpus\n",
    "    \"eval_batch_size\": 256, # Actual batch size will this x num gpus\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d52dff9a-6d88-4e00-bcb3-ab3b39834b9f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def mem_status_distributed():\n",
    "    rank = torch.cuda.current_device()\n",
    "    properties = torch.cuda.get_device_properties(rank)\n",
    "    total_memory = properties.total_memory / (1024 ** 3)  # Convert to GB\n",
    "    allocated_memory = torch.cuda.memory_allocated(rank) / (1024 ** 3)\n",
    "    reserved_memory = torch.cuda.memory_reserved(rank) / (1024 ** 3)\n",
    "    available_memory = total_memory - reserved_memory\n",
    "    print(f\"GPU {rank}: | \")\n",
    "    print(f\"  Total memory: {total_memory:.2f} GB |\")\n",
    "    print(f\"  Allocated memory: {allocated_memory:.2f} GB |\")\n",
    "    print(f\"  Reserved memory: {reserved_memory:.2f} GB |\")\n",
    "    print(f\"  Available memory: {available_memory:.2f} GB |\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8e6d4a04-6f39-4107-b79f-de27014f485d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Original Downsampled Training and Evaluation without Decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "76b6af91-3029-4e68-b87a-2c73eada8fe0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def accelerated_training_function(model, downsampled_dataset, tokenzier, metric, hyperparameters):\n",
    "    \n",
    "    from accelerate import Accelerator\n",
    "    from apex.optimizers import FusedAdam\n",
    "    import datasets\n",
    "    import torch\n",
    "    # from torch.optim import AdamW\n",
    "    from torch.utils.data import DataLoader\n",
    "    from tqdm.notebook import tqdm\n",
    "    import transformers\n",
    "    from transformers import DataCollatorForSeq2Seq, get_scheduler\n",
    "\n",
    "    # initialize our accelerator as early as possible for configuring the distributed backend\n",
    "    accelerator = Accelerator()\n",
    "\n",
    "    # To have only one message (and not 2) per logs of Transformers or Datasets, we set the logging verbosity to INFO for the main process only.\n",
    "    if accelerator.is_main_process:\n",
    "        datasets.utils.logging.set_verbosity_warning()\n",
    "        transformers.utils.logging.set_verbosity_info()\n",
    "    else:\n",
    "        datasets.utils.logging.set_verbosity_error()\n",
    "        transformers.utils.logging.set_verbosity_error()\n",
    "\n",
    "    # define the output dir\n",
    "    output_dir = \"/Volumes/workspace_dogfood/jgr/hugging_face_cache/CyberSolve-DeepMind-LinAlg-1D-downsampled\"\n",
    "\n",
    "    # Collate our datasets\n",
    "    data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, label_pad_token_id=tokenizer.pad_token_id, pad_to_multiple_of=2)\n",
    "\n",
    "    downsampled_train_dataloader = DataLoader(\n",
    "        downsampled_dataset[\"train\"],\n",
    "        shuffle=True, # add shuffling\n",
    "        batch_size=hyperparameters[\"train_batch_size\"],\n",
    "        collate_fn=data_collator\n",
    "    )\n",
    "    downsampled_eval_dataloader = DataLoader(\n",
    "        downsampled_dataset[\"test\"], \n",
    "        batch_size=hyperparameters[\"eval_batch_size\"], \n",
    "        collate_fn=data_collator\n",
    "    )\n",
    "\n",
    "    # use the apex optimized version of AdamW with a fused kernel\n",
    "    # NOTE T5 was pretrained with the AdaFactor optimizer - perhaps we should compare this optimizer in a separate training\n",
    "    optimizer = FusedAdam(model.parameters(), lr=hyperparameters[\"learning_rate\"], adam_w_mode=True)\n",
    "    # optimizer = AdamW(model.parameters(), lr=hyperparameters[\"learning_rate\"])\n",
    "\n",
    "    model.to(accelerator.device)\n",
    "\n",
    "    downsampled_train_dataloader, downsampled_eval_dataloader, model, optimizer = accelerator.prepare(downsampled_train_dataloader, downsampled_eval_dataloader, model, optimizer)\n",
    "\n",
    "    num_epochs = 3\n",
    "    num_training_steps = num_epochs * len(downsampled_train_dataloader)\n",
    "    lr_scheduler = get_scheduler(\n",
    "        \"linear\",\n",
    "        optimizer=optimizer,\n",
    "        num_warmup_steps=0,\n",
    "        num_training_steps=num_training_steps\n",
    "    )\n",
    "\n",
    "    mem_status_distributed()\n",
    "    for epoch in range(num_epochs):\n",
    "        # training\n",
    "        model.train()\n",
    "        for batch in tqdm(downsampled_train_dataloader, desc=f\"Epoch {epoch}\", position=0, leave=True):\n",
    "            batch = {k: v.to(accelerator.device) for k, v in batch.items()}\n",
    "            outputs = model(**batch)\n",
    "            loss = outputs.loss\n",
    "            \n",
    "            accelerator.backward(loss)\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        # evaluation\n",
    "        model.eval()\n",
    "        all_predictions = []\n",
    "        all_labels = []\n",
    "        for step, batch in enumerate(downsampled_eval_dataloader):\n",
    "            with torch.no_grad():\n",
    "                outputs = model(**batch)\n",
    "            predictions = outputs.logits.argmax(dim=-1)\n",
    "\n",
    "            # We gather predictions and labels from the 2 GPUs to combine them all\n",
    "            all_predictions.append(accelerator.gather(predictions))\n",
    "            all_labels.append(accelerator.gather(batch[\"labels\"]))\n",
    "\n",
    "        # Concatenate all predictions and labels\n",
    "        # The last thing we need to do is to truncate the predictions and labels we concatenated\n",
    "        # together as the prepared evaluation dataloader has a little bit more elements to make\n",
    "        # batches of the same size on each process.\n",
    "        all_predictions = torch.cat(all_predictions)[:len(downsampled_dataset[\"test\"])]\n",
    "        all_labels = torch.cat(all_labels)[:len(downsampled_dataset[\"test\"])]\n",
    "\n",
    "        mem_status_distributed() # show us the mem status of each GPU at the end of each epoch\n",
    "        metric = exact_match_metric.compute(predictions=all_predictions, references=all_labels)\n",
    "        accelerator.print(f\"epoch {epoch}:\", metric)\n",
    "\n",
    "    # be sure to save our trained model to a given path\n",
    "    # first wait for all processes to reach the same stage\n",
    "    accelerator.wait_for_everyone()\n",
    "    # unwraps the model from accelerate.prepare() to reintroduce the save_pretrained() fn for saving\n",
    "    unwrapped_model = accelerator.unwrap_model(model)\n",
    "    # accelerator.save() instead of torch.save()\n",
    "    unwrapped_model.save_pretrained(output_dir, save_function=accelerator.save)\n",
    "    if accelerator.is_main_process:\n",
    "        tokenizer.save_pretrained(output_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6126c3a0-869e-4379-bec8-4010b3a1548f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import torch.multiprocessing as mp\n",
    "\n",
    "# Set the multiprocessing start method to 'spawn'\n",
    "mp.set_start_method('spawn', force=True) # essential for spawning the multiprocessing properly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5d1d3744-1210-4563-9989-a77b7f2995ae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching training on 2 GPUs.\nGPU 1: | \n  Total memory: 79.15 GB |\n  Allocated memory: 5.98 GB |\n  Reserved memory: 8.48 GB |GPU 0: | \n\n  Available memory: 70.67 GB |  Total memory: 79.15 GB |\n\n  Allocated memory: 5.98 GB |\n  Reserved memory: 8.73 GB |\n  Available memory: 70.42 GB |\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bec665c49e064dee9e6f383cda51841c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 0:   0%|          | 0/1563 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03d1e57ef5284caeb7939d125e199ff7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 0:   0%|          | 0/1563 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/local_disk0/.ephemeral_nfs/envs/pythonEnv-9ebbc1fb-e5db-48bf-9e7f-0655ffbd0e07/lib/python3.11/site-packages/transformers/data/data_collator.py:657: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:274.)\n  batch[\"labels\"] = torch.tensor(batch[\"labels\"], dtype=torch.int64)\n/local_disk0/.ephemeral_nfs/envs/pythonEnv-9ebbc1fb-e5db-48bf-9e7f-0655ffbd0e07/lib/python3.11/site-packages/transformers/data/data_collator.py:657: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:274.)\n  batch[\"labels\"] = torch.tensor(batch[\"labels\"], dtype=torch.int64)\nPassing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU 1: | GPU 0: | \n\n  Total memory: 79.15 GB |  Total memory: 79.15 GB |\n\n  Allocated memory: 12.08 GB |  Allocated memory: 12.08 GB |\n\n  Reserved memory: 18.50 GB |  Reserved memory: 18.50 GB |\n\n  Available memory: 60.65 GB |  Available memory: 60.65 GB |\n\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8317893869514012b50caf1683515318",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1:   0%|          | 0/1563 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0: {'exact_match': 32.21}\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d826b9782f0466db89a469b5c32116b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1:   0%|          | 0/1563 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU 1: | GPU 0: | \n\n  Total memory: 79.15 GB |  Total memory: 79.15 GB |\n\n  Allocated memory: 12.08 GB |  Allocated memory: 12.08 GB |\n\n  Reserved memory: 18.50 GB |  Reserved memory: 18.50 GB |\n\n  Available memory: 60.65 GB |  Available memory: 60.65 GB |\n\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "617b874ba1204a95b95dd2f3a4a3d9ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2:   0%|          | 0/1563 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1: {'exact_match': 39.69}\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "993c9936ae4844fdad254714e2bc5900",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2:   0%|          | 0/1563 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU 0: | GPU 1: | \n\n  Total memory: 79.15 GB |  Total memory: 79.15 GB |\n\n  Allocated memory: 12.08 GB |  Allocated memory: 12.08 GB |\n\n  Reserved memory: 18.50 GB |  Reserved memory: 18.50 GB |\n\n  Available memory: 60.65 GB |  Available memory: 60.65 GB |\n\nepoch 2: {'exact_match': 44.99}\n[2024-12-20 18:31:34,441] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n[2024-12-20 18:31:34,448] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/bin/ld: cannot find -laio: No such file or directory\n/usr/bin/ld: cannot find -laio: No such file or directory\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[93m [WARNING] \u001B[0m async_io requires the dev libaio .so object and headers but these were not found.\n\u001B[93m [WARNING] \u001B[0m async_io requires the dev libaio .so object and headers but these were not found.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "collect2: error: ld returned 1 exit status\ncollect2: error: ld returned 1 exit status\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[93m [WARNING] \u001B[0m async_io: please install the libaio-dev package with apt\u001B[93m [WARNING] \u001B[0m async_io: please install the libaio-dev package with apt\n\n\u001B[93m [WARNING] \u001B[0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.\u001B[93m [WARNING] \u001B[0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.\n\n\u001B[93m [WARNING] \u001B[0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH\u001B[93m [WARNING] \u001B[0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH\n\n\u001B[93m [WARNING] \u001B[0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3\n\u001B[93m [WARNING] \u001B[0m using untested triton version (2.3.1), only 1.0.0 is known to be compatible\n\u001B[93m [WARNING] \u001B[0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3\n\u001B[93m [WARNING] \u001B[0m using untested triton version (2.3.1), only 1.0.0 is known to be compatible\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in /Volumes/workspace_dogfood/jgr/hugging_face_cache/CyberSolve-DeepMind-LinAlg-1D-downsampled/config.json\nConfiguration saved in /Volumes/workspace_dogfood/jgr/hugging_face_cache/CyberSolve-DeepMind-LinAlg-1D-downsampled/generation_config.json\nW1220 18:31:40.813138 140155197497344 torch/multiprocessing/spawn.py:145] Terminating process 63869 via signal SIGTERM\nE1220 18:31:43.530642 140155197497344 torch/distributed/elastic/multiprocessing/api.py:695] failed (exitcode: 1) local_rank: 0 (pid: 63866) of fn: accelerated_training_function (start_method: fork)\nE1220 18:31:43.530642 140155197497344 torch/distributed/elastic/multiprocessing/api.py:695] Traceback (most recent call last):\nE1220 18:31:43.530642 140155197497344 torch/distributed/elastic/multiprocessing/api.py:695]   File \"/databricks/python/lib/python3.11/site-packages/torch/distributed/elastic/multiprocessing/api.py\", line 656, in _poll\nE1220 18:31:43.530642 140155197497344 torch/distributed/elastic/multiprocessing/api.py:695]     self._pc.join(-1)\nE1220 18:31:43.530642 140155197497344 torch/distributed/elastic/multiprocessing/api.py:695]   File \"/databricks/python/lib/python3.11/site-packages/torch/multiprocessing/spawn.py\", line 188, in join\nE1220 18:31:43.530642 140155197497344 torch/distributed/elastic/multiprocessing/api.py:695]     raise ProcessRaisedException(msg, error_index, failed_process.pid)\nE1220 18:31:43.530642 140155197497344 torch/distributed/elastic/multiprocessing/api.py:695] torch.multiprocessing.spawn.ProcessRaisedException: \nE1220 18:31:43.530642 140155197497344 torch/distributed/elastic/multiprocessing/api.py:695] \nE1220 18:31:43.530642 140155197497344 torch/distributed/elastic/multiprocessing/api.py:695] -- Process 0 terminated with the following error:\nE1220 18:31:43.530642 140155197497344 torch/distributed/elastic/multiprocessing/api.py:695] Traceback (most recent call last):\nE1220 18:31:43.530642 140155197497344 torch/distributed/elastic/multiprocessing/api.py:695]   File \"/databricks/python/lib/python3.11/site-packages/torch/multiprocessing/spawn.py\", line 75, in _wrap\nE1220 18:31:43.530642 140155197497344 torch/distributed/elastic/multiprocessing/api.py:695]     fn(i, *args)\nE1220 18:31:43.530642 140155197497344 torch/distributed/elastic/multiprocessing/api.py:695]   File \"/databricks/python/lib/python3.11/site-packages/torch/distributed/elastic/multiprocessing/api.py\", line 580, in _wrap\nE1220 18:31:43.530642 140155197497344 torch/distributed/elastic/multiprocessing/api.py:695]     ret = record(fn)(*args_)\nE1220 18:31:43.530642 140155197497344 torch/distributed/elastic/multiprocessing/api.py:695]           ^^^^^^^^^^^^^^^^^^\nE1220 18:31:43.530642 140155197497344 torch/distributed/elastic/multiprocessing/api.py:695]   File \"/databricks/python/lib/python3.11/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py\", line 347, in wrapper\nE1220 18:31:43.530642 140155197497344 torch/distributed/elastic/multiprocessing/api.py:695]     return f(*args, **kwargs)\nE1220 18:31:43.530642 140155197497344 torch/distributed/elastic/multiprocessing/api.py:695]            ^^^^^^^^^^^^^^^^^^\nE1220 18:31:43.530642 140155197497344 torch/distributed/elastic/multiprocessing/api.py:695]   File \"/root/.ipykernel/5591/command-1975419433870057-3083509344\", line 104, in accelerated_training_function\nE1220 18:31:43.530642 140155197497344 torch/distributed/elastic/multiprocessing/api.py:695]     unwrapped_model.save_pretrained(output_dir, save_function=accelerator.save)\nE1220 18:31:43.530642 140155197497344 torch/distributed/elastic/multiprocessing/api.py:695]   File \"/local_disk0/.ephemeral_nfs/envs/pythonEnv-9ebbc1fb-e5db-48bf-9e7f-0655ffbd0e07/lib/python3.11/site-packages/transformers/modeling_utils.py\", line 3034, in save_pretrained\nE1220 18:31:43.530642 140155197497344 torch/distributed/elastic/multiprocessing/api.py:695]     safe_save_file(shard, os.path.join(save_directory, shard_file), metadata={\"format\": \"pt\"})\nE1220 18:31:43.530642 140155197497344 torch/distributed/elastic/multiprocessing/api.py:695]   File \"/databricks/python/lib/python3.11/site-packages/safetensors/torch.py\", line 281, in save_file\nE1220 18:31:43.530642 140155197497344 torch/distributed/elastic/multiprocessing/api.py:695]     serialize_file(_flatten(tensors), filename, metadata=metadata)\nE1220 18:31:43.530642 140155197497344 torch/distributed/elastic/multiprocessing/api.py:695] safetensors_rust.SafetensorError: Error while serializing: IoError(Os { code: 22, kind: InvalidInput, message: \"Invalid argument\" })\nE1220 18:31:43.530642 140155197497344 torch/distributed/elastic/multiprocessing/api.py:695] \n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mChildFailedError\u001B[0m                          Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-955088565215259>, line 3\u001B[0m\n",
       "\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01maccelerate\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m notebook_launcher\n",
       "\u001B[0;32m----> 3\u001B[0m notebook_launcher(accelerated_training_function, (model, downsampled_dataset, tokenizer, exact_match_metric, hyperparameters), num_processes\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m2\u001B[39m, mixed_precision\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbf16\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/accelerate/launchers.py:239\u001B[0m, in \u001B[0;36mnotebook_launcher\u001B[0;34m(function, args, num_processes, mixed_precision, use_port, master_addr, node_rank, num_nodes, rdzv_backend, rdzv_endpoint, rdzv_conf, rdzv_id, max_restarts, monitor_interval)\u001B[0m\n",
       "\u001B[1;32m    225\u001B[0m             rdzv_endpoint \u001B[38;5;241m=\u001B[39m \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mmaster_addr\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m:\u001B[39m\u001B[38;5;132;01m{\u001B[39;00muse_port\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n",
       "\u001B[1;32m    226\u001B[0m     launch_config \u001B[38;5;241m=\u001B[39m LaunchConfig(\n",
       "\u001B[1;32m    227\u001B[0m         min_nodes\u001B[38;5;241m=\u001B[39mnum_nodes,\n",
       "\u001B[1;32m    228\u001B[0m         max_nodes\u001B[38;5;241m=\u001B[39mnum_nodes,\n",
       "\u001B[0;32m   (...)\u001B[0m\n",
       "\u001B[1;32m    237\u001B[0m         log_line_prefix_template\u001B[38;5;241m=\u001B[39mos\u001B[38;5;241m.\u001B[39menviron\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTORCHELASTIC_LOG_LINE_PREFIX_TEMPLATE\u001B[39m\u001B[38;5;124m\"\u001B[39m),\n",
       "\u001B[1;32m    238\u001B[0m     )\n",
       "\u001B[0;32m--> 239\u001B[0m     elastic_launch(config\u001B[38;5;241m=\u001B[39mlaunch_config, entrypoint\u001B[38;5;241m=\u001B[39mfunction)(\u001B[38;5;241m*\u001B[39margs)\n",
       "\u001B[1;32m    240\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m ProcessRaisedException \u001B[38;5;28;01mas\u001B[39;00m e:\n",
       "\u001B[1;32m    241\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCannot re-initialize CUDA in forked subprocess\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m e\u001B[38;5;241m.\u001B[39margs[\u001B[38;5;241m0\u001B[39m]:\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/torch/distributed/launcher/api.py:132\u001B[0m, in \u001B[0;36melastic_launch.__call__\u001B[0;34m(self, *args)\u001B[0m\n",
       "\u001B[1;32m    131\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs):\n",
       "\u001B[0;32m--> 132\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m launch_agent(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_config, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_entrypoint, \u001B[38;5;28mlist\u001B[39m(args))\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/torch/distributed/launcher/api.py:263\u001B[0m, in \u001B[0;36mlaunch_agent\u001B[0;34m(config, entrypoint, args)\u001B[0m\n",
       "\u001B[1;32m    256\u001B[0m     events\u001B[38;5;241m.\u001B[39mrecord(agent\u001B[38;5;241m.\u001B[39mget_event_succeeded())\n",
       "\u001B[1;32m    258\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m result\u001B[38;5;241m.\u001B[39mis_failed():\n",
       "\u001B[1;32m    259\u001B[0m         \u001B[38;5;66;03m# ChildFailedError is treated specially by @record\u001B[39;00m\n",
       "\u001B[1;32m    260\u001B[0m         \u001B[38;5;66;03m# if the error files for the failed children exist\u001B[39;00m\n",
       "\u001B[1;32m    261\u001B[0m         \u001B[38;5;66;03m# @record will copy the first error (root cause)\u001B[39;00m\n",
       "\u001B[1;32m    262\u001B[0m         \u001B[38;5;66;03m# to the error file of the launcher process.\u001B[39;00m\n",
       "\u001B[0;32m--> 263\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m ChildFailedError(\n",
       "\u001B[1;32m    264\u001B[0m             name\u001B[38;5;241m=\u001B[39mentrypoint_name,\n",
       "\u001B[1;32m    265\u001B[0m             failures\u001B[38;5;241m=\u001B[39mresult\u001B[38;5;241m.\u001B[39mfailures,\n",
       "\u001B[1;32m    266\u001B[0m         )\n",
       "\u001B[1;32m    268\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m result\u001B[38;5;241m.\u001B[39mreturn_values\n",
       "\u001B[1;32m    269\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m ChildFailedError:\n",
       "\n",
       "\u001B[0;31mChildFailedError\u001B[0m: \n",
       "============================================================\n",
       "accelerated_training_function FAILED\n",
       "------------------------------------------------------------\n",
       "Failures:\n",
       "  <NO_OTHER_FAILURES>\n",
       "------------------------------------------------------------\n",
       "Root Cause (first observed failure):\n",
       "[0]:\n",
       "  time      : 2024-12-20_18:31:40\n",
       "  host      : 1219-211731-tpe1z4w3-172-18-0-10\n",
       "  rank      : 0 (local_rank: 0)\n",
       "  exitcode  : 1 (pid: 63866)\n",
       "  error_file: /tmp/torchelastic_9dmrivxw/none_s2vb8y0x/attempt_0/0/error.json\n",
       "  traceback : Traceback (most recent call last):\n",
       "    File \"/databricks/python/lib/python3.11/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py\", line 347, in wrapper\n",
       "      return f(*args, **kwargs)\n",
       "             ^^^^^^^^^^^^^^^^^^\n",
       "    File \"/root/.ipykernel/5591/command-1975419433870057-3083509344\", line 104, in accelerated_training_function\n",
       "      unwrapped_model.save_pretrained(output_dir, save_function=accelerator.save)\n",
       "    File \"/local_disk0/.ephemeral_nfs/envs/pythonEnv-9ebbc1fb-e5db-48bf-9e7f-0655ffbd0e07/lib/python3.11/site-packages/transformers/modeling_utils.py\", line 3034, in save_pretrained\n",
       "      safe_save_file(shard, os.path.join(save_directory, shard_file), metadata={\"format\": \"pt\"})\n",
       "    File \"/databricks/python/lib/python3.11/site-packages/safetensors/torch.py\", line 281, in save_file\n",
       "      serialize_file(_flatten(tensors), filename, metadata=metadata)\n",
       "  safetensors_rust.SafetensorError: Error while serializing: IoError(Os { code: 22, kind: InvalidInput, message: \"Invalid argument\" })\n",
       "  \n",
       "============================================================"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": {
        "ename": "ChildFailedError",
        "evalue": "\n============================================================\naccelerated_training_function FAILED\n------------------------------------------------------------\nFailures:\n  <NO_OTHER_FAILURES>\n------------------------------------------------------------\nRoot Cause (first observed failure):\n[0]:\n  time      : 2024-12-20_18:31:40\n  host      : 1219-211731-tpe1z4w3-172-18-0-10\n  rank      : 0 (local_rank: 0)\n  exitcode  : 1 (pid: 63866)\n  error_file: /tmp/torchelastic_9dmrivxw/none_s2vb8y0x/attempt_0/0/error.json\n  traceback : Traceback (most recent call last):\n    File \"/databricks/python/lib/python3.11/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py\", line 347, in wrapper\n      return f(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^\n    File \"/root/.ipykernel/5591/command-1975419433870057-3083509344\", line 104, in accelerated_training_function\n      unwrapped_model.save_pretrained(output_dir, save_function=accelerator.save)\n    File \"/local_disk0/.ephemeral_nfs/envs/pythonEnv-9ebbc1fb-e5db-48bf-9e7f-0655ffbd0e07/lib/python3.11/site-packages/transformers/modeling_utils.py\", line 3034, in save_pretrained\n      safe_save_file(shard, os.path.join(save_directory, shard_file), metadata={\"format\": \"pt\"})\n    File \"/databricks/python/lib/python3.11/site-packages/safetensors/torch.py\", line 281, in save_file\n      serialize_file(_flatten(tensors), filename, metadata=metadata)\n  safetensors_rust.SafetensorError: Error while serializing: IoError(Os { code: 22, kind: InvalidInput, message: \"Invalid argument\" })\n  \n============================================================"
       },
       "metadata": {
        "errorSummary": "<span class='ansi-red-fg'>ChildFailedError</span>: \n============================================================\naccelerated_training_function FAILED\n------------------------------------------------------------\nFailures:\n  <NO_OTHER_FAILURES>\n------------------------------------------------------------\nRoot Cause (first observed failure):\n[0]:\n  time      : 2024-12-20_18:31:40\n  host      : 1219-211731-tpe1z4w3-172-18-0-10\n  rank      : 0 (local_rank: 0)\n  exitcode  : 1 (pid: 63866)\n  error_file: /tmp/torchelastic_9dmrivxw/none_s2vb8y0x/attempt_0/0/error.json\n  traceback : Traceback (most recent call last):\n    File \"/databricks/python/lib/python3.11/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py\", line 347, in wrapper\n      return f(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^\n    File \"/root/.ipykernel/5591/command-1975419433870057-3083509344\", line 104, in accelerated_training_function\n      unwrapped_model.save_pretrained(output_dir, save_function=accelerator.save)\n    File \"/local_disk0/.ephemeral_nfs/envs/pythonEnv-9ebbc1fb-e5db-48bf-9e7f-0655ffbd0e07/lib/python3.11/site-packages/transformers/modeling_utils.py\", line 3034, in save_pretrained\n      safe_save_file(shard, os.path.join(save_directory, shard_file), metadata={\"format\": \"pt\"})\n    File \"/databricks/python/lib/python3.11/site-packages/safetensors/torch.py\", line 281, in save_file\n      serialize_file(_flatten(tensors), filename, metadata=metadata)\n  safetensors_rust.SafetensorError: Error while serializing: IoError(Os { code: 22, kind: InvalidInput, message: \"Invalid argument\" })\n  \n============================================================"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [
        "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
        "\u001B[0;31mChildFailedError\u001B[0m                          Traceback (most recent call last)",
        "File \u001B[0;32m<command-955088565215259>, line 3\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01maccelerate\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m notebook_launcher\n\u001B[0;32m----> 3\u001B[0m notebook_launcher(accelerated_training_function, (model, downsampled_dataset, tokenizer, exact_match_metric, hyperparameters), num_processes\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m2\u001B[39m, mixed_precision\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbf16\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
        "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/accelerate/launchers.py:239\u001B[0m, in \u001B[0;36mnotebook_launcher\u001B[0;34m(function, args, num_processes, mixed_precision, use_port, master_addr, node_rank, num_nodes, rdzv_backend, rdzv_endpoint, rdzv_conf, rdzv_id, max_restarts, monitor_interval)\u001B[0m\n\u001B[1;32m    225\u001B[0m             rdzv_endpoint \u001B[38;5;241m=\u001B[39m \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mmaster_addr\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m:\u001B[39m\u001B[38;5;132;01m{\u001B[39;00muse_port\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    226\u001B[0m     launch_config \u001B[38;5;241m=\u001B[39m LaunchConfig(\n\u001B[1;32m    227\u001B[0m         min_nodes\u001B[38;5;241m=\u001B[39mnum_nodes,\n\u001B[1;32m    228\u001B[0m         max_nodes\u001B[38;5;241m=\u001B[39mnum_nodes,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    237\u001B[0m         log_line_prefix_template\u001B[38;5;241m=\u001B[39mos\u001B[38;5;241m.\u001B[39menviron\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTORCHELASTIC_LOG_LINE_PREFIX_TEMPLATE\u001B[39m\u001B[38;5;124m\"\u001B[39m),\n\u001B[1;32m    238\u001B[0m     )\n\u001B[0;32m--> 239\u001B[0m     elastic_launch(config\u001B[38;5;241m=\u001B[39mlaunch_config, entrypoint\u001B[38;5;241m=\u001B[39mfunction)(\u001B[38;5;241m*\u001B[39margs)\n\u001B[1;32m    240\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m ProcessRaisedException \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    241\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCannot re-initialize CUDA in forked subprocess\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m e\u001B[38;5;241m.\u001B[39margs[\u001B[38;5;241m0\u001B[39m]:\n",
        "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/torch/distributed/launcher/api.py:132\u001B[0m, in \u001B[0;36melastic_launch.__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m    131\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs):\n\u001B[0;32m--> 132\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m launch_agent(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_config, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_entrypoint, \u001B[38;5;28mlist\u001B[39m(args))\n",
        "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/torch/distributed/launcher/api.py:263\u001B[0m, in \u001B[0;36mlaunch_agent\u001B[0;34m(config, entrypoint, args)\u001B[0m\n\u001B[1;32m    256\u001B[0m     events\u001B[38;5;241m.\u001B[39mrecord(agent\u001B[38;5;241m.\u001B[39mget_event_succeeded())\n\u001B[1;32m    258\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m result\u001B[38;5;241m.\u001B[39mis_failed():\n\u001B[1;32m    259\u001B[0m         \u001B[38;5;66;03m# ChildFailedError is treated specially by @record\u001B[39;00m\n\u001B[1;32m    260\u001B[0m         \u001B[38;5;66;03m# if the error files for the failed children exist\u001B[39;00m\n\u001B[1;32m    261\u001B[0m         \u001B[38;5;66;03m# @record will copy the first error (root cause)\u001B[39;00m\n\u001B[1;32m    262\u001B[0m         \u001B[38;5;66;03m# to the error file of the launcher process.\u001B[39;00m\n\u001B[0;32m--> 263\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m ChildFailedError(\n\u001B[1;32m    264\u001B[0m             name\u001B[38;5;241m=\u001B[39mentrypoint_name,\n\u001B[1;32m    265\u001B[0m             failures\u001B[38;5;241m=\u001B[39mresult\u001B[38;5;241m.\u001B[39mfailures,\n\u001B[1;32m    266\u001B[0m         )\n\u001B[1;32m    268\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m result\u001B[38;5;241m.\u001B[39mreturn_values\n\u001B[1;32m    269\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m ChildFailedError:\n",
        "\u001B[0;31mChildFailedError\u001B[0m: \n============================================================\naccelerated_training_function FAILED\n------------------------------------------------------------\nFailures:\n  <NO_OTHER_FAILURES>\n------------------------------------------------------------\nRoot Cause (first observed failure):\n[0]:\n  time      : 2024-12-20_18:31:40\n  host      : 1219-211731-tpe1z4w3-172-18-0-10\n  rank      : 0 (local_rank: 0)\n  exitcode  : 1 (pid: 63866)\n  error_file: /tmp/torchelastic_9dmrivxw/none_s2vb8y0x/attempt_0/0/error.json\n  traceback : Traceback (most recent call last):\n    File \"/databricks/python/lib/python3.11/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py\", line 347, in wrapper\n      return f(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^\n    File \"/root/.ipykernel/5591/command-1975419433870057-3083509344\", line 104, in accelerated_training_function\n      unwrapped_model.save_pretrained(output_dir, save_function=accelerator.save)\n    File \"/local_disk0/.ephemeral_nfs/envs/pythonEnv-9ebbc1fb-e5db-48bf-9e7f-0655ffbd0e07/lib/python3.11/site-packages/transformers/modeling_utils.py\", line 3034, in save_pretrained\n      safe_save_file(shard, os.path.join(save_directory, shard_file), metadata={\"format\": \"pt\"})\n    File \"/databricks/python/lib/python3.11/site-packages/safetensors/torch.py\", line 281, in save_file\n      serialize_file(_flatten(tensors), filename, metadata=metadata)\n  safetensors_rust.SafetensorError: Error while serializing: IoError(Os { code: 22, kind: InvalidInput, message: \"Invalid argument\" })\n  \n============================================================"
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from accelerate import notebook_launcher\n",
    "\n",
    "notebook_launcher(accelerated_training_function, (model, downsampled_dataset, tokenizer, exact_match_metric, hyperparameters), num_processes=2, mixed_precision=\"bf16\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "40b8d8b3-b0c1-48c6-bbf9-b55f18684f3d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Updated Downsampled Training and Evaluation with Decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5631ce2d-5426-473c-9a87-549fdcb8f7b6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "!mkdir /tmp/machine_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e2b3b63f-e932-4aeb-9f9c-7c1620184864",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rserv\r\nRtmpf8qfoW\r\nchauffeur-daemon-params\r\nchauffeur-daemon.pid\r\nchauffeur-env.sh\r\ncustom-spark.conf\r\ndriver-daemon-params\r\ndriver-daemon.pid\r\ndriver-env.sh\r\nhsperfdata_root\r\nmachine_dir\r\npython_lsp_logs\r\nsystemd-private-b81863601b2e4c9e819cc56024f38457-systemd-logind.service-1ezHoY\r\nsystemd-private-b81863601b2e4c9e819cc56024f38457-systemd-resolved.service-ikrxlP\r\ntmp.aXcBvtVXck\r\ntmpemz2zxud\r\n"
     ]
    }
   ],
   "source": [
    "!ls /tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "15624510-f4c5-4352-ae49-c99cdebada09",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Set the NCCL_SOCKET_IFNAME environment variable\n",
    "os.environ[\"NCCL_SOCKET_IFNAME\"] = \"eth0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ae3a34f2-9ac1-45f8-8582-c00dab9d1a4d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def accelerated_training_function(model, downsampled_dataset, tokenzier, metric, hyperparameters):\n",
    "    \n",
    "    from accelerate import Accelerator\n",
    "    from apex.optimizers import FusedAdam\n",
    "    import datasets\n",
    "    import torch\n",
    "    # from torch.optim import AdamW\n",
    "    from torch.utils.data import DataLoader\n",
    "    from tqdm.notebook import tqdm\n",
    "    import transformers\n",
    "    from transformers import DataCollatorForSeq2Seq, get_scheduler\n",
    "\n",
    "    # initialize our accelerator as early as possible for configuring the distributed backend\n",
    "    accelerator = Accelerator()\n",
    "\n",
    "    # To have only one message (and not 2) per logs of Transformers or Datasets, we set the logging verbosity to INFO for the main process only.\n",
    "    if accelerator.is_main_process:\n",
    "        datasets.utils.logging.set_verbosity_warning()\n",
    "        transformers.utils.logging.set_verbosity_info()\n",
    "    else:\n",
    "        datasets.utils.logging.set_verbosity_error()\n",
    "        transformers.utils.logging.set_verbosity_error()\n",
    "\n",
    "    # on machine dir\n",
    "    tmp_dir = \"/tmp/machine_dir_2\"\n",
    "    # define the output dir\n",
    "    output_dir = \"/Volumes/workspace_dogfood/jgr/hugging_face_cache/CyberSolve-DeepMind-LinAlg-1D-downsampled-v3\"\n",
    "\n",
    "    # Collate our datasets\n",
    "    data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, label_pad_token_id=tokenizer.pad_token_id, pad_to_multiple_of=2)\n",
    "\n",
    "    downsampled_train_dataloader = DataLoader(\n",
    "        downsampled_dataset[\"train\"],\n",
    "        shuffle=True, # add shuffling\n",
    "        batch_size=hyperparameters[\"train_batch_size\"],\n",
    "        collate_fn=data_collator\n",
    "    )\n",
    "    downsampled_eval_dataloader = DataLoader(\n",
    "        downsampled_dataset[\"test\"], \n",
    "        batch_size=hyperparameters[\"eval_batch_size\"], \n",
    "        collate_fn=data_collator\n",
    "    )\n",
    "\n",
    "    # use the apex optimized version of AdamW with a fused kernel\n",
    "    # NOTE T5 was pretrained with the AdaFactor optimizer - perhaps we should compare this optimizer in a separate training\n",
    "    optimizer = FusedAdam(model.parameters(), lr=hyperparameters[\"learning_rate\"], adam_w_mode=True)\n",
    "    # optimizer = AdamW(model.parameters(), lr=hyperparameters[\"learning_rate\"])\n",
    "\n",
    "    model.to(accelerator.device)\n",
    "\n",
    "    downsampled_train_dataloader, downsampled_eval_dataloader, model, optimizer = accelerator.prepare(downsampled_train_dataloader, downsampled_eval_dataloader, model, optimizer)\n",
    "\n",
    "    num_epochs = 3\n",
    "    num_training_steps = num_epochs * len(downsampled_train_dataloader)\n",
    "    lr_scheduler = get_scheduler(\n",
    "        \"linear\",\n",
    "        optimizer=optimizer,\n",
    "        num_warmup_steps=0,\n",
    "        num_training_steps=num_training_steps\n",
    "    )\n",
    "\n",
    "    mem_status_distributed()\n",
    "    for epoch in range(num_epochs):\n",
    "        # training\n",
    "        model.train()\n",
    "        for batch in tqdm(downsampled_train_dataloader, desc=f\"Epoch {epoch}\", position=0, leave=True):\n",
    "            batch = {k: v.to(accelerator.device) for k, v in batch.items()}\n",
    "            outputs = model(**batch)\n",
    "            loss = outputs.loss\n",
    "            \n",
    "            accelerator.backward(loss)\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        # evaluation\n",
    "        model.eval()\n",
    "        # all_predictions = []\n",
    "        # all_labels = []\n",
    "        for step, batch in enumerate(downsampled_eval_dataloader):\n",
    "            with torch.no_grad():\n",
    "                outputs = model(**batch)\n",
    "            predictions = outputs.logits.argmax(dim=-1)\n",
    "\n",
    "            # We gather predictions and labels from the 2 GPUs to combine them all\n",
    "            # all_predictions.append(accelerator.gather(predictions))\n",
    "            # all_labels.append(accelerator.gather(batch[\"labels\"]))\n",
    "            gathered_predictions = accelerator.gather_for_metrics(predictions)\n",
    "            gathered_labels = accelerator.gather_for_metrics(batch[\"labels\"])\n",
    "\n",
    "            for pred, label in zip(gathered_predictions, gathered_labels):\n",
    "                exact_match_metric.add(predictions=tokenizer.decode(pred, skip_special_tokens=True), references=tokenizer.decode(label, skip_special_tokens=True))\n",
    "\n",
    "        mem_status_distributed() # show us the mem status of each GPU at the end of each epoch\n",
    "        # metric = exact_match_metric.compute(predictions=all_predictions, references=all_labels)\n",
    "        metric = exact_match_metric.compute()\n",
    "        accelerator.print(f\"epoch {epoch}:\", metric)\n",
    "\n",
    "    # be sure to save our trained model to a given path\n",
    "    # first wait for all processes to reach the same stage\n",
    "    accelerator.wait_for_everyone()\n",
    "    # unwraps the model from accelerate.prepare() to reintroduce the save_pretrained() fn for saving\n",
    "    unwrapped_model = accelerator.unwrap_model(model)\n",
    "    # accelerator.save() instead of torch.save()\n",
    "    unwrapped_model.save_pretrained(tmp_dir, save_function=accelerator.save)\n",
    "    # why do we need this at all?\n",
    "    if accelerator.is_main_process:\n",
    "        tokenizer.save_pretrained(tmp_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a973a365-6218-450a-9d92-131a00e3feb1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import torch.multiprocessing as mp\n",
    "\n",
    "# Set the multiprocessing start method to 'spawn'\n",
    "mp.set_start_method('spawn', force=True) # essential for spawning the multiprocessing properly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "56250d78-d0bd-43c0-988a-bcc8d3d48480",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching training on 2 GPUs.\nGPU 0: | \n  Total memory: 79.15 GB |\n  Allocated memory: 5.98 GB |\n  Reserved memory: 8.48 GB |\n  Available memory: 70.67 GB |\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48a5913991ba44b3a88ace89ad4f1b29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 0:   0%|          | 0/196 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU 1: | \n  Total memory: 79.15 GB |\n  Allocated memory: 5.98 GB |\n  Reserved memory: 8.73 GB |\n  Available memory: 70.42 GB |\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1147676220f3443288f7ec888a7d332b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 0:   0%|          | 0/196 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/local_disk0/.ephemeral_nfs/envs/pythonEnv-4b26884f-680b-4a70-a491-f35e6dad2f39/lib/python3.11/site-packages/transformers/data/data_collator.py:657: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:274.)\n  batch[\"labels\"] = torch.tensor(batch[\"labels\"], dtype=torch.int64)\n/local_disk0/.ephemeral_nfs/envs/pythonEnv-4b26884f-680b-4a70-a491-f35e6dad2f39/lib/python3.11/site-packages/transformers/data/data_collator.py:657: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:274.)\n  batch[\"labels\"] = torch.tensor(batch[\"labels\"], dtype=torch.int64)\nPassing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU 0: | \n  Total memory: 79.15 GB |GPU 1: | \n\n  Allocated memory: 14.11 GB |  Total memory: 79.15 GB |\n\n  Reserved memory: 55.60 GB |  Allocated memory: 14.11 GB |\n\n  Available memory: 23.55 GB |  Reserved memory: 51.62 GB |\n\n  Available memory: 27.53 GB |\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72f97bdcb9064d74bd10b9c92adea85a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1:   0%|          | 0/196 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0: {'exact_match': 25.019999999999996}\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d5388f91e99447095c5f09b8c3ad16c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1:   0%|          | 0/196 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU 1: | \n  Total memory: 79.15 GB |GPU 0: | \n\n  Allocated memory: 14.11 GB |  Total memory: 79.15 GB |\n\n  Reserved memory: 51.62 GB |  Allocated memory: 14.11 GB |\n\n  Available memory: 27.53 GB |  Reserved memory: 55.60 GB |\n\n  Available memory: 23.55 GB |\nepoch 1: {'exact_match': 28.38}\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1635b17fc6ae498f8296d1d048bc5261",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2:   0%|          | 0/196 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "127b101679434adcb8157d3a7697b2e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2:   0%|          | 0/196 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU 1: | GPU 0: | \n\n  Total memory: 79.15 GB |  Total memory: 79.15 GB |\n\n  Allocated memory: 14.11 GB |  Allocated memory: 14.11 GB |\n\n  Reserved memory: 51.62 GB |  Reserved memory: 55.60 GB |\n\n  Available memory: 27.53 GB |  Available memory: 23.55 GB |\n\nepoch 2: {'exact_match': 29.74}\n[2024-12-31 18:25:47,468] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n[2024-12-31 18:25:47,493] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n\u001B[93m [WARNING] \u001B[0m async_io requires the dev libaio .so object and headers but these were not found.\n\u001B[93m [WARNING] \u001B[0m async_io requires the dev libaio .so object and headers but these were not found.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/bin/ld: cannot find -laio: No such file or directory\n/usr/bin/ld: cannot find -laio: No such file or directory\ncollect2: error: ld returned 1 exit status\ncollect2: error: ld returned 1 exit status\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[93m [WARNING] \u001B[0m async_io: please install the libaio-dev package with apt\u001B[93m [WARNING] \u001B[0m async_io: please install the libaio-dev package with apt\n\n\u001B[93m [WARNING] \u001B[0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.\u001B[93m [WARNING] \u001B[0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.\n\n\u001B[93m [WARNING] \u001B[0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH\u001B[93m [WARNING] \u001B[0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH\n\n\u001B[93m [WARNING] \u001B[0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3\u001B[93m [WARNING] \u001B[0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3\n\n\u001B[93m [WARNING] \u001B[0m using untested triton version (2.3.1), only 1.0.0 is known to be compatible\u001B[93m [WARNING] \u001B[0m using untested triton version (2.3.1), only 1.0.0 is known to be compatible\n\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in /tmp/machine_dir_2/config.json\nConfiguration saved in /tmp/machine_dir_2/generation_config.json\nModel weights saved in /tmp/machine_dir_2/model.safetensors\ntokenizer config file saved in /tmp/machine_dir_2/tokenizer_config.json\nSpecial tokens file saved in /tmp/machine_dir_2/special_tokens_map.json\nadded tokens file saved in /tmp/machine_dir_2/added_tokens.json\nW1231 18:26:13.488908 140074288316416 torch/distributed/elastic/multiprocessing/api.py:727] Closing process 16633 via signal SIGTERM\n"
     ]
    }
   ],
   "source": [
    "from accelerate import notebook_launcher\n",
    "\n",
    "notebook_launcher(accelerated_training_function, (model, downsampled_dataset, tokenizer, exact_match_metric, hyperparameters), num_processes=2, mixed_precision=\"bf16\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2b078a94-5745-4d62-a953-57a96a432ad8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Original Downsampled Training and Eval Postprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ac769c4b-73e0-4f99-8b70-0a377c15050c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# NOTE: Original\n",
    "\n",
    "# make sure we can load our model from the saved location\n",
    "from transformers import T5ForConditionalGeneration\n",
    "\n",
    "trained_model = T5ForConditionalGeneration.from_pretrained(\"/Volumes/workspace_dogfood/jgr/hugging_face_cache/CyberSolve-DeepMind-LinAlg-1D-downsampled\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "09b10468-8d93-41dd-9ccb-97a86c0b5e30",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "T5ForConditionalGeneration(\n",
       "  (shared): Embedding(32128, 1024)\n",
       "  (encoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32128, 1024)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 16)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=1024, out_features=2816, bias=False)\n",
       "              (wi_1): Linear(in_features=1024, out_features=2816, bias=False)\n",
       "              (wo): Linear(in_features=2816, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-23): 23 x T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=1024, out_features=2816, bias=False)\n",
       "              (wi_1): Linear(in_features=1024, out_features=2816, bias=False)\n",
       "              (wo): Linear(in_features=2816, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (decoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32128, 1024)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 16)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=1024, out_features=2816, bias=False)\n",
       "              (wi_1): Linear(in_features=1024, out_features=2816, bias=False)\n",
       "              (wo): Linear(in_features=2816, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-23): 23 x T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=1024, out_features=2816, bias=False)\n",
       "              (wi_1): Linear(in_features=1024, out_features=2816, bias=False)\n",
       "              (wo): Linear(in_features=2816, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1024, out_features=32128, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trained_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "006b0111-e6b2-4946-b62b-61e68ff2dada",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trained_model.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "739f45e2-1c3c-42c4-993c-89eee02af6e8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 10000\n",
       "})"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenized_eval_dataset = tokenized_eval_dataset[\"test\"]\n",
    "tokenized_eval_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9756a009-de4d-4565-9a36-d5a9c7f00ce9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# collate our true eval set \n",
    "from transformers import DataCollatorForSeq2Seq\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, label_pad_token_id=tokenizer.pad_token_id, pad_to_multiple_of=2)\n",
    "\n",
    "eval_dataloader = DataLoader(\n",
    "    tokenized_eval_dataset, \n",
    "    batch_size=hyperparameters[\"eval_batch_size\"], \n",
    "    collate_fn=data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8609564d-b544-4261-821c-d425f51b1836",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "T5ForConditionalGeneration(\n",
       "  (shared): Embedding(32128, 1024)\n",
       "  (encoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32128, 1024)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 16)\n",
       "            )\n",
       "            (layer_norm): FusedRMSNorm(torch.Size([1024]), eps=1e-06, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=1024, out_features=2816, bias=False)\n",
       "              (wi_1): Linear(in_features=1024, out_features=2816, bias=False)\n",
       "              (wo): Linear(in_features=2816, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): FusedRMSNorm(torch.Size([1024]), eps=1e-06, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-23): 23 x T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): FusedRMSNorm(torch.Size([1024]), eps=1e-06, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=1024, out_features=2816, bias=False)\n",
       "              (wi_1): Linear(in_features=1024, out_features=2816, bias=False)\n",
       "              (wo): Linear(in_features=2816, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): FusedRMSNorm(torch.Size([1024]), eps=1e-06, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): FusedRMSNorm(torch.Size([1024]), eps=1e-06, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (decoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32128, 1024)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 16)\n",
       "            )\n",
       "            (layer_norm): FusedRMSNorm(torch.Size([1024]), eps=1e-06, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): FusedRMSNorm(torch.Size([1024]), eps=1e-06, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=1024, out_features=2816, bias=False)\n",
       "              (wi_1): Linear(in_features=1024, out_features=2816, bias=False)\n",
       "              (wo): Linear(in_features=2816, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): FusedRMSNorm(torch.Size([1024]), eps=1e-06, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-23): 23 x T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): FusedRMSNorm(torch.Size([1024]), eps=1e-06, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): FusedRMSNorm(torch.Size([1024]), eps=1e-06, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=1024, out_features=2816, bias=False)\n",
       "              (wi_1): Linear(in_features=1024, out_features=2816, bias=False)\n",
       "              (wo): Linear(in_features=2816, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): FusedRMSNorm(torch.Size([1024]), eps=1e-06, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): FusedRMSNorm(torch.Size([1024]), eps=1e-06, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1024, out_features=32128, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# put the model on the GPU\n",
    "trained_model.to(torch.device(\"cuda\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "272494e5-a94e-43f6-948d-5b0bdbd386b6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from evaluate import load\n",
    "\n",
    "exact_match_metric = load(\"exact_match\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1b150fb7-3f20-4aa5-a53a-a141e02719ed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3287d8d660ca49d9a85d0433fefb8476",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/1250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'exact_match': 0.2121}\n"
     ]
    }
   ],
   "source": [
    "# evaluation\n",
    "from tqdm.notebook import tqdm\n",
    "from datasets import Dataset\n",
    "\n",
    "partials = {\"predicted_tokens\": [], \"label_tokens\": [], \"decoded_prediction\": [], \"decoded_label\": []}\n",
    "\n",
    "trained_model.eval()\n",
    "for batch in tqdm(eval_dataloader, desc=\"Evaluating\", position=0, leave=True):\n",
    "    batch = {k: v.to(torch.device(\"cuda\")) for k, v in batch.items()}\n",
    "    with torch.no_grad():\n",
    "        outputs = trained_model(**batch)\n",
    "    \n",
    "    for pred, label in zip(outputs.logits.argmax(dim=-1), batch[\"labels\"]):\n",
    "        # add decoded predictions and labels to the metric object\n",
    "        exact_match_metric.add(predictions=tokenizer.decode(pred, skip_special_tokens=True), references=tokenizer.decode(label, skip_special_tokens=True))\n",
    "        # populate the partial correctness dict for detailed, individual eval\n",
    "        partials[\"predicted_tokens\"].append(pred)\n",
    "        partials[\"label_tokens\"].append(label)\n",
    "        partials[\"decoded_prediction\"].append(tokenizer.decode(pred, skip_special_tokens=True))\n",
    "        partials[\"decoded_label\"].append(tokenizer.decode(label, skip_special_tokens=True))\n",
    "\n",
    "print(exact_match_metric.compute())\n",
    "partial_correctness_dataset = Dataset.from_dict(partials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "28d444d9-4223-40ca-b80e-ec0aa2933a2a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['predicted_tokens', 'label_tokens', 'decoded_prediction', 'decoded_label'],\n",
       "    num_rows: 10000\n",
       "})"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "partial_correctness_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b138bcc9-549c-4e72-ba5c-50570acc7447",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# lets push the partial downsampled finetuned model and the evaluation dataset to the hub for saving\n",
    "dbutils.widgets.text(\"hf_token\", \"\", \"hf_token\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d0856a7f-a94d-4231-89d8-d99ea2408943",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: huggingface-cli <command> [<args>] login [-h] [--token TOKEN]\r\n                                                [--add-to-git-credential]\r\nhuggingface-cli <command> [<args>] login: error: argument --token: expected one argument\r\n"
     ]
    }
   ],
   "source": [
    "hf_token = dbutils.widgets.get(\"hf_token\")\n",
    "!huggingface-cli login --token $hf_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c38d84dc-bebf-4eaa-9548-6641aa450e33",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3bc8b21f05d14e18a4dc0ebc40d0df3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/5.17k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/local_disk0/.ephemeral_nfs/envs/pythonEnv-9ebbc1fb-e5db-48bf-9e7f-0655ffbd0e07/lib/python3.11/site-packages/huggingface_hub/file_download.py:140: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in /Volumes/workspace_dogfood/jgr/hugging_face_cache/hub/models--MarioBarbeque--CyberSolve-DeepMind-LinAlg-1D-downsample. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n  warnings.warn(message)\nNo files have been modified since last commit. Skipping to prevent empty commit.\nWARNING:huggingface_hub.hf_api:No files have been modified since last commit. Skipping to prevent empty commit.\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/MarioBarbeque/CyberSolve-DeepMind-LinAlg-1D-downsample/commit/a8f4d7d9c07c8dca81a71fc03d05e85e83bdb8e7', commit_message='A initial finetuing of the flan-T5-large model on a downsampled version of the DeepMind LingAlg 1D Dataset. We call this CyberSolve', commit_description='', oid='a8f4d7d9c07c8dca81a71fc03d05e85e83bdb8e7', pr_url=None, repo_url=RepoUrl('https://huggingface.co/MarioBarbeque/CyberSolve-DeepMind-LinAlg-1D-downsample', endpoint='https://huggingface.co', repo_type='model', repo_id='MarioBarbeque/CyberSolve-DeepMind-LinAlg-1D-downsample'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trained_model.push_to_hub(\"CyberSolve-DeepMind-LinAlg-1D-downsample\", commit_message=\"A initial finetuing of the flan-T5-large model on a downsampled version of the DeepMind LingAlg 1D Dataset. We call this CyberSolve\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "91ba58cf-afaa-4e3e-b747-bb09ca80b8ec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40ee13b8418447169ae1ab3acb9ce11a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7bbbc8d1cfaf4ce196d0250ba675f53b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/10 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fabb4497e0e34326b5ebe1c59aa5a2c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/417 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/local_disk0/.ephemeral_nfs/envs/pythonEnv-9ebbc1fb-e5db-48bf-9e7f-0655ffbd0e07/lib/python3.11/site-packages/huggingface_hub/file_download.py:140: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in /Volumes/workspace_dogfood/jgr/hugging_face_cache/hub/datasets--MarioBarbeque--CyberSolve-DeepMind-LinAlg-1D-downsample-benchmark. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n  warnings.warn(message)\nNo files have been modified since last commit. Skipping to prevent empty commit.\nWARNING:huggingface_hub.hf_api:No files have been modified since last commit. Skipping to prevent empty commit.\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/MarioBarbeque/CyberSolve-DeepMind-LinAlg-1D-downsample-benchmark/commit/5665466e11700bb36670c7b28018aea96b8f3749', commit_message='a dataset for evaluating the partial correctness of the initial finetuning of the flan-T5-large model on a downsampled version of the DeepMind LingAlg 1D Dataset (which we subesequently call CyberSolve)', commit_description='', oid='5665466e11700bb36670c7b28018aea96b8f3749', pr_url=None, repo_url=RepoUrl('https://huggingface.co/datasets/MarioBarbeque/CyberSolve-DeepMind-LinAlg-1D-downsample-benchmark', endpoint='https://huggingface.co', repo_type='dataset', repo_id='MarioBarbeque/CyberSolve-DeepMind-LinAlg-1D-downsample-benchmark'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "partial_correctness_dataset.push_to_hub(\"CyberSolve-DeepMind-LinAlg-1D-downsample-benchmark\", commit_message=\"a dataset for evaluating the partial correctness of the initial finetuning of the flan-T5-large model on a downsampled version of the DeepMind LingAlg 1D Dataset (which we subesequently call CyberSolve)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e7f3e8d0-4e56-4e6c-a574-343f6df84300",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Updated Downsampled Training and Eval Postprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "abbff4d6-7680-4b9c-b59e-9cdac7aec65e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# NOTE: Updated\n",
    "\n",
    "# make sure we can load our model from the saved location\n",
    "from transformers import T5ForConditionalGeneration\n",
    "\n",
    "v2_trained_model = T5ForConditionalGeneration.from_pretrained(\"/Volumes/workspace_dogfood/jgr/hugging_face_cache/CyberSolve-DeepMind-LinAlg-1D-downsampled-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2c120352-4b8d-4bb6-9b3e-1c93077e25fa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "T5ForConditionalGeneration(\n",
       "  (shared): Embedding(32128, 1024)\n",
       "  (encoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32128, 1024)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 16)\n",
       "            )\n",
       "            (layer_norm): FusedRMSNorm(torch.Size([1024]), eps=1e-06, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=1024, out_features=2816, bias=False)\n",
       "              (wi_1): Linear(in_features=1024, out_features=2816, bias=False)\n",
       "              (wo): Linear(in_features=2816, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): FusedRMSNorm(torch.Size([1024]), eps=1e-06, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-23): 23 x T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): FusedRMSNorm(torch.Size([1024]), eps=1e-06, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=1024, out_features=2816, bias=False)\n",
       "              (wi_1): Linear(in_features=1024, out_features=2816, bias=False)\n",
       "              (wo): Linear(in_features=2816, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): FusedRMSNorm(torch.Size([1024]), eps=1e-06, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): FusedRMSNorm(torch.Size([1024]), eps=1e-06, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (decoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32128, 1024)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 16)\n",
       "            )\n",
       "            (layer_norm): FusedRMSNorm(torch.Size([1024]), eps=1e-06, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): FusedRMSNorm(torch.Size([1024]), eps=1e-06, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=1024, out_features=2816, bias=False)\n",
       "              (wi_1): Linear(in_features=1024, out_features=2816, bias=False)\n",
       "              (wo): Linear(in_features=2816, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): FusedRMSNorm(torch.Size([1024]), eps=1e-06, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-23): 23 x T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): FusedRMSNorm(torch.Size([1024]), eps=1e-06, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): FusedRMSNorm(torch.Size([1024]), eps=1e-06, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=1024, out_features=2816, bias=False)\n",
       "              (wi_1): Linear(in_features=1024, out_features=2816, bias=False)\n",
       "              (wo): Linear(in_features=2816, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): FusedRMSNorm(torch.Size([1024]), eps=1e-06, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): FusedRMSNorm(torch.Size([1024]), eps=1e-06, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1024, out_features=32128, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v2_trained_model # this should be the model we just adjusted the weights of"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ed760761-f530-42cc-add3-7db90963f105",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 10000\n",
       "})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_eval_dataset = tokenized_eval_dataset[\"test\"]\n",
    "tokenized_eval_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b254e68d-ff5c-4cdf-8d47-bd815b90ffcf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# collate our true eval set \n",
    "from transformers import DataCollatorForSeq2Seq\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, label_pad_token_id=tokenizer.pad_token_id, pad_to_multiple_of=2)\n",
    "\n",
    "eval_dataloader = DataLoader(\n",
    "    tokenized_eval_dataset, \n",
    "    batch_size=hyperparameters[\"eval_batch_size\"], \n",
    "    collate_fn=data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "14268d7c-51ad-43df-bbc9-776738cd662b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "T5ForConditionalGeneration(\n",
       "  (shared): Embedding(32128, 1024)\n",
       "  (encoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32128, 1024)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 16)\n",
       "            )\n",
       "            (layer_norm): FusedRMSNorm(torch.Size([1024]), eps=1e-06, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=1024, out_features=2816, bias=False)\n",
       "              (wi_1): Linear(in_features=1024, out_features=2816, bias=False)\n",
       "              (wo): Linear(in_features=2816, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): FusedRMSNorm(torch.Size([1024]), eps=1e-06, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-23): 23 x T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): FusedRMSNorm(torch.Size([1024]), eps=1e-06, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=1024, out_features=2816, bias=False)\n",
       "              (wi_1): Linear(in_features=1024, out_features=2816, bias=False)\n",
       "              (wo): Linear(in_features=2816, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): FusedRMSNorm(torch.Size([1024]), eps=1e-06, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): FusedRMSNorm(torch.Size([1024]), eps=1e-06, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (decoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32128, 1024)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 16)\n",
       "            )\n",
       "            (layer_norm): FusedRMSNorm(torch.Size([1024]), eps=1e-06, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): FusedRMSNorm(torch.Size([1024]), eps=1e-06, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=1024, out_features=2816, bias=False)\n",
       "              (wi_1): Linear(in_features=1024, out_features=2816, bias=False)\n",
       "              (wo): Linear(in_features=2816, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): FusedRMSNorm(torch.Size([1024]), eps=1e-06, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-23): 23 x T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): FusedRMSNorm(torch.Size([1024]), eps=1e-06, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): FusedRMSNorm(torch.Size([1024]), eps=1e-06, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=1024, out_features=2816, bias=False)\n",
       "              (wi_1): Linear(in_features=1024, out_features=2816, bias=False)\n",
       "              (wo): Linear(in_features=2816, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): FusedRMSNorm(torch.Size([1024]), eps=1e-06, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): FusedRMSNorm(torch.Size([1024]), eps=1e-06, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1024, out_features=32128, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# put the model on the GPU\n",
    "v2_trained_model.to(torch.device(\"cuda\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eab3dcea-1e7d-444f-90a0-3880983437e3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from evaluate import load\n",
    "\n",
    "exact_match_metric = load(\"exact_match\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a9c8a2a9-e68c-4d7c-8b67-c8cd47467539",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "828a7463ae454be996706ebc58045a62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'exact_match': 0.0692}\n"
     ]
    }
   ],
   "source": [
    "# evaluation\n",
    "from tqdm.notebook import tqdm\n",
    "from datasets import Dataset\n",
    "\n",
    "partials = {\"predicted_tokens\": [], \"label_tokens\": [], \"decoded_prediction\": [], \"decoded_label\": []}\n",
    "\n",
    "v2_trained_model.eval()\n",
    "for batch in tqdm(eval_dataloader, desc=\"Evaluating\", position=0, leave=True):\n",
    "    batch = {k: v.to(torch.device(\"cuda\")) for k, v in batch.items()}\n",
    "    with torch.no_grad():\n",
    "        # outputs = trained_model(**batch)\n",
    "        outputs = v2_trained_model(**batch)\n",
    "    \n",
    "    for pred, label in zip(outputs.logits.argmax(dim=-1), batch[\"labels\"]):\n",
    "        # add decoded predictions and labels to the metric object\n",
    "        exact_match_metric.add(predictions=tokenizer.decode(pred, skip_special_tokens=True), references=tokenizer.decode(label, skip_special_tokens=True))\n",
    "        # populate the partial correctness dict for detailed, individual eval\n",
    "        partials[\"predicted_tokens\"].append(pred)\n",
    "        partials[\"label_tokens\"].append(label)\n",
    "        partials[\"decoded_prediction\"].append(tokenizer.decode(pred, skip_special_tokens=True))\n",
    "        partials[\"decoded_label\"].append(tokenizer.decode(label, skip_special_tokens=True))\n",
    "\n",
    "print(exact_match_metric.compute())\n",
    "partial_correctness_dataset = Dataset.from_dict(partials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d50b8bc2-bc56-47ee-9bab-572f2f67a3dc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['predicted_tokens', 'label_tokens', 'decoded_prediction', 'decoded_label'],\n",
       "    num_rows: 10000\n",
       "})"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "partial_correctness_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7439f424-ab1c-4f5f-885d-58cdf5241b00",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "v2_trained_model.push_to_hub(\"CyberSolve-DeepMind-LinAlg-1D-downsample-v2\", commit_message=\"A second finetuing of the flan-T5-large model on the downsampled DeepMind LingAlg 1D dataset, this time with a GPU batch size of 256 as opposed to 32 used before\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b3e198f6-2b91-4aa2-8e0e-96b768ae85a8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c73e0027c13f4965b795731cd72c00c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "385531cdaf9e4cd2a13a75361fd329d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/10 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/MarioBarbeque/CyberSolve-DeepMind-LinAlg-1D-downsample-benchmark-v2/commit/c0462b69888b93ee06cb892683ca3f64dc4cffe4', commit_message='a second dataset for evaluating the partial correctness of the second finetuning of the flan-T5-large model on a downsampled version of the DeepMind LingAlg 1D dataset', commit_description='', oid='c0462b69888b93ee06cb892683ca3f64dc4cffe4', pr_url=None, repo_url=RepoUrl('https://huggingface.co/datasets/MarioBarbeque/CyberSolve-DeepMind-LinAlg-1D-downsample-benchmark-v2', endpoint='https://huggingface.co', repo_type='dataset', repo_id='MarioBarbeque/CyberSolve-DeepMind-LinAlg-1D-downsample-benchmark-v2'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "partial_correctness_dataset.push_to_hub(\"CyberSolve-DeepMind-LinAlg-1D-downsample-benchmark-v2\", commit_message=\"a second dataset for evaluating the partial correctness of the second finetuning of the flan-T5-large model on a downsampled version of the DeepMind LingAlg 1D dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3d8afd68-f8cf-4145-9e90-cd97640b989e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "1"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 1152595035260126,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Downsampled Training",
   "widgets": {
    "hf_token": {
     "currentValue": "",
     "nuid": "4172fcf7-fb23-43ba-b50a-d56a8de2c667",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": "hf_token",
      "name": "hf_token",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": "hf_token",
      "name": "hf_token",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}