{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "55392bdb-39e5-45a9-ad9d-d8f6df0b83fb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Benchmarking\n",
    "\n",
    "Here we load our preprocessed evaluation dataset `MarioBarbeque/DeepMind-LinAlg-1D-eval` into a DataLoader with a specific collate function in order to benchmark the `flan-t5-large` model's performance on solving linear equations of a single variable before finetuning it on the large DeepMind training dataset. \n",
    "\n",
    "For the benchmarking in this notebook, we use a single node, single GPU (Nivida T4) compute instance. For improved computation, we install the Nvidia `apex` python package for use in computing the normalization layers of the T5 model. In regards to `apex` the \uD83E\uDD17 T5 documentation states:\n",
    "\n",
    "\"*[after installation] the model will automatically use `apex.normalization.FusedRMSNorm` instead of `T5LayerNorm`. The former uses an optimized fused kernel which is several times faster than the latter.*\"\n",
    "\n",
    " The `apex` package and its `optimizers` module will also be useful when we actually train our model. We can construct an improved `FusedAdam` (Adam or AdamW) optimizer in favor of the standard `torch.optim.AdamW` optimizer. \n",
    " \n",
    " For this benchmarking notebook, we first update and install all relevant software to our compute instance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c4daa83b-9c33-40ae-bd94-968a3fdce789",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.1+cu121\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "175e3c08-5afd-4a33-89eb-9d0a183a613e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /databricks/python3/lib/python3.11/site-packages (4.41.2)\nCollecting transformers\n  Obtaining dependency information for transformers from https://files.pythonhosted.org/packages/f2/3a/8bdab26e09c5a242182b7ba9152e216d5ab4ae2d78c4298eb4872549cd35/transformers-4.47.1-py3-none-any.whl.metadata\n  Downloading transformers-4.47.1-py3-none-any.whl.metadata (44 kB)\n\u001B[?25l     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/44.1 kB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\n\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m44.1/44.1 kB\u001B[0m \u001B[31m1.7 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hRequirement already satisfied: filelock in /databricks/python3/lib/python3.11/site-packages (from transformers) (3.13.4)\nCollecting huggingface-hub<1.0,>=0.24.0 (from transformers)\n  Obtaining dependency information for huggingface-hub<1.0,>=0.24.0 from https://files.pythonhosted.org/packages/61/8c/fbdc0a88a622d9fa54e132d7bf3ee03ec602758658a2db5b339a65be2cfe/huggingface_hub-0.27.0-py3-none-any.whl.metadata\n  Downloading huggingface_hub-0.27.0-py3-none-any.whl.metadata (13 kB)\nRequirement already satisfied: numpy>=1.17 in /databricks/python3/lib/python3.11/site-packages (from transformers) (1.23.5)\nRequirement already satisfied: packaging>=20.0 in /databricks/python3/lib/python3.11/site-packages (from transformers) (23.2)\nRequirement already satisfied: pyyaml>=5.1 in /databricks/python3/lib/python3.11/site-packages (from transformers) (6.0)\nRequirement already satisfied: regex!=2019.12.17 in /databricks/python3/lib/python3.11/site-packages (from transformers) (2022.7.9)\nRequirement already satisfied: requests in /databricks/python3/lib/python3.11/site-packages (from transformers) (2.31.0)\nCollecting tokenizers<0.22,>=0.21 (from transformers)\n  Obtaining dependency information for tokenizers<0.22,>=0.21 from https://files.pythonhosted.org/packages/22/06/69d7ce374747edaf1695a4f61b83570d91cc8bbfc51ccfecf76f56ab4aac/tokenizers-0.21.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n  Downloading tokenizers-0.21.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\nRequirement already satisfied: safetensors>=0.4.1 in /databricks/python3/lib/python3.11/site-packages (from transformers) (0.4.2)\nRequirement already satisfied: tqdm>=4.27 in /databricks/python3/lib/python3.11/site-packages (from transformers) (4.65.0)\nRequirement already satisfied: fsspec>=2023.5.0 in /databricks/python3/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (2023.5.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /databricks/python3/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (4.10.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /databricks/python3/lib/python3.11/site-packages (from requests->transformers) (2.0.4)\nRequirement already satisfied: idna<4,>=2.5 in /databricks/python3/lib/python3.11/site-packages (from requests->transformers) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.11/site-packages (from requests->transformers) (2.0.7)\nRequirement already satisfied: certifi>=2017.4.17 in /databricks/python3/lib/python3.11/site-packages (from requests->transformers) (2023.7.22)\nDownloading transformers-4.47.1-py3-none-any.whl (10.1 MB)\n\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/10.1 MB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\n\u001B[2K   \u001B[91m━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m1.8/10.1 MB\u001B[0m \u001B[31m52.8 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\n\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━━\u001B[0m \u001B[32m7.6/10.1 MB\u001B[0m \u001B[31m111.0 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\n\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m \u001B[32m10.1/10.1 MB\u001B[0m \u001B[31m121.1 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\n\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m \u001B[32m10.1/10.1 MB\u001B[0m \u001B[31m121.1 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\n\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m10.1/10.1 MB\u001B[0m \u001B[31m67.2 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hDownloading huggingface_hub-0.27.0-py3-none-any.whl (450 kB)\n\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/450.5 kB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\n\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m450.5/450.5 kB\u001B[0m \u001B[31m26.2 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hDownloading tokenizers-0.21.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/3.0 MB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\n\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m \u001B[32m3.0/3.0 MB\u001B[0m \u001B[31m150.6 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\n\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m3.0/3.0 MB\u001B[0m \u001B[31m70.2 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hInstalling collected packages: huggingface-hub, tokenizers, transformers\n  Attempting uninstall: huggingface-hub\n    Found existing installation: huggingface-hub 0.23.4\n    Not uninstalling huggingface-hub at /databricks/python3/lib/python3.11/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-c6dc5ff9-367d-4006-8e30-1875d7d29fb6\n    Can't uninstall 'huggingface-hub'. No files were found to uninstall.\n  Attempting uninstall: tokenizers\n    Found existing installation: tokenizers 0.19.0\n    Not uninstalling tokenizers at /databricks/python3/lib/python3.11/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-c6dc5ff9-367d-4006-8e30-1875d7d29fb6\n    Can't uninstall 'tokenizers'. No files were found to uninstall.\n  Attempting uninstall: transformers\n    Found existing installation: transformers 4.41.2\n    Not uninstalling transformers at /databricks/python3/lib/python3.11/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-c6dc5ff9-367d-4006-8e30-1875d7d29fb6\n    Can't uninstall 'transformers'. No files were found to uninstall.\nSuccessfully installed huggingface-hub-0.27.0 tokenizers-0.21.0 transformers-4.47.1\n\u001B[43mNote: you may need to restart the kernel using %restart_python or dbutils.library.restartPython() to use updated packages.\u001B[0m\nRequirement already satisfied: ninja in /databricks/python3/lib/python3.11/site-packages (1.11.1.1)\n\u001B[43mNote: you may need to restart the kernel using %restart_python or dbutils.library.restartPython() to use updated packages.\u001B[0m\nCollecting git+https://github.com/NVIDIA/apex.git\n  Cloning https://github.com/NVIDIA/apex.git to /tmp/pip-req-build-v44t3fb0\n  Running command git clone --filter=blob:none --quiet https://github.com/NVIDIA/apex.git /tmp/pip-req-build-v44t3fb0\n  Resolved https://github.com/NVIDIA/apex.git to commit 73375b3bbcb59a5d6ff43f2fafd00b9ecdbe0417\n  Running command git submodule update --init --recursive -q\n  Preparing metadata (pyproject.toml): started\n  Preparing metadata (pyproject.toml): finished with status 'done'\nRequirement already satisfied: packaging>20.6 in /databricks/python3/lib/python3.11/site-packages (from apex==0.1) (23.2)\nBuilding wheels for collected packages: apex\n  Building wheel for apex (pyproject.toml): started\n  Building wheel for apex (pyproject.toml): still running...\n  Building wheel for apex (pyproject.toml): still running...\n  Building wheel for apex (pyproject.toml): still running...\n  Building wheel for apex (pyproject.toml): still running...\n  Building wheel for apex (pyproject.toml): still running...\n  Building wheel for apex (pyproject.toml): still running...\n  Building wheel for apex (pyproject.toml): still running...\n  Building wheel for apex (pyproject.toml): still running...\n  Building wheel for apex (pyproject.toml): still running...\n  Building wheel for apex (pyproject.toml): still running...\n  Building wheel for apex (pyproject.toml): finished with status 'done'\n  Created wheel for apex: filename=apex-0.1-cp311-cp311-linux_x86_64.whl size=35430970 sha256=7b5ee0e6da2639cd0319082beba3cb4deb9e943e116abe40ebe1079e061e8f9b\n  Stored in directory: /tmp/pip-ephem-wheel-cache-kaghaais/wheels/79/b8/83/5235f93f5bca64242106bf00bd06a198b5c54b8df578ca2f99\nSuccessfully built apex\n\u001B[33mWARNING: Error parsing requirements for tiktoken: [Errno 2] No such file or directory: '/local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.11/site-packages/tiktoken-0.8.0.dist-info/METADATA'\u001B[0m\u001B[33m\n\u001B[0mInstalling collected packages: apex\nSuccessfully installed apex-0.1\n\u001B[43mNote: you may need to restart the kernel using %restart_python or dbutils.library.restartPython() to use updated packages.\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "# ensure we have the most recent version of transformers\n",
    "!pip install -U transformers\n",
    "# ensure we have ninja installed to speed up Nvidia apex source compilation\n",
    "!pip install ninja\n",
    "# install Nvidia Apex for optimized computation in the T5 normalization layers\n",
    "!pip install git+https://github.com/NVIDIA/apex.git --no-cache-dir --no-build-isolation --config-settings \"--build-option=--cpp_ext\" --config-settings \"--build-option=--cuda_ext\"\n",
    "\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dbdbe1b8-1cd2-4436-8203-6738a71ac890",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'12.1'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.version.cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "99a1bde6-329e-4ab3-a109-96f688f84c0c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "apex @ git+https://github.com/NVIDIA/apex.git@73375b3bbcb59a5d6ff43f2fafd00b9ecdbe0417\n"
     ]
    }
   ],
   "source": [
    "!pip freeze | grep apex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c2fa3846-bbc5-4c2e-92bb-c3c309e8f164",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# confirm the apex library is available\n",
    "from apex import normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3dd5f52d-aa09-44cd-8a15-7c9eae4d29b5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/python_shell/dbruntime/huggingface_patches/datasets.py:14: UserWarning: During large dataset downloads, there could be multiple progress bar widgets that can cause performance issues for your notebook or browser. To avoid these issues, use `datasets.utils.logging.disable_progress_bar()` to turn off the progress bars.\n  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# load our eval dataset\n",
    "from datasets import load_dataset\n",
    "\n",
    "eval_dataset = load_dataset(\"MarioBarbeque/DeepMind-LinAlg-1D-eval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "762f6c75-1f68-4696-ada8-8cf0340740f7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 10000\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# grab the only relevant Dataset object within the DatasetDict object and peek it\n",
    "eval_dataset = eval_dataset[\"test\"]\n",
    "eval_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5016859f-485b-42ef-953d-f9a6e7194cbb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'input_ids': [[5175,\n",
       "   162,\n",
       "   3,\n",
       "   4949,\n",
       "   4613,\n",
       "   1935,\n",
       "   26,\n",
       "   1768,\n",
       "   668,\n",
       "   3166,\n",
       "   3,\n",
       "   18,\n",
       "   3,\n",
       "   27640,\n",
       "   3274,\n",
       "   3,\n",
       "   5947,\n",
       "   2773,\n",
       "   21,\n",
       "   3,\n",
       "   26,\n",
       "   5,\n",
       "   1,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0],\n",
       "  [5175,\n",
       "   162,\n",
       "   9526,\n",
       "   1935,\n",
       "   40,\n",
       "   1768,\n",
       "   3479,\n",
       "   1935,\n",
       "   40,\n",
       "   3,\n",
       "   18,\n",
       "   3,\n",
       "   10124,\n",
       "   3,\n",
       "   18,\n",
       "   3,\n",
       "   3891,\n",
       "   3274,\n",
       "   3,\n",
       "   632,\n",
       "   21,\n",
       "   3,\n",
       "   40,\n",
       "   5,\n",
       "   1,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0],\n",
       "  [5175,\n",
       "   162,\n",
       "   3,\n",
       "   18,\n",
       "   4389,\n",
       "   1935,\n",
       "   17,\n",
       "   1768,\n",
       "   1179,\n",
       "   4225,\n",
       "   3,\n",
       "   18,\n",
       "   505,\n",
       "   3707,\n",
       "   1768,\n",
       "   668,\n",
       "   4201,\n",
       "   3274,\n",
       "   3,\n",
       "   632,\n",
       "   21,\n",
       "   3,\n",
       "   17,\n",
       "   5,\n",
       "   1,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0],\n",
       "  [5175,\n",
       "   162,\n",
       "   6374,\n",
       "   1935,\n",
       "   122,\n",
       "   3274,\n",
       "   3,\n",
       "   19978,\n",
       "   1935,\n",
       "   122,\n",
       "   3,\n",
       "   18,\n",
       "   3,\n",
       "   4450,\n",
       "   1935,\n",
       "   122,\n",
       "   3,\n",
       "   18,\n",
       "   3,\n",
       "   4440,\n",
       "   1935,\n",
       "   122,\n",
       "   3,\n",
       "   18,\n",
       "   3,\n",
       "   26755,\n",
       "   21,\n",
       "   3,\n",
       "   122,\n",
       "   5,\n",
       "   1,\n",
       "   0,\n",
       "   0,\n",
       "   0],\n",
       "  [5175,\n",
       "   162,\n",
       "   3,\n",
       "   18,\n",
       "   3710,\n",
       "   1935,\n",
       "   208,\n",
       "   1768,\n",
       "   3,\n",
       "   23188,\n",
       "   1935,\n",
       "   208,\n",
       "   1768,\n",
       "   305,\n",
       "   2773,\n",
       "   5553,\n",
       "   3274,\n",
       "   314,\n",
       "   3914,\n",
       "   4433,\n",
       "   21,\n",
       "   3,\n",
       "   208,\n",
       "   5,\n",
       "   1,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0]],\n",
       " 'attention_mask': [[1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0],\n",
       "  [1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0],\n",
       "  [1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0],\n",
       "  [1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   0,\n",
       "   0,\n",
       "   0],\n",
       "  [1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0]],\n",
       " 'labels': [[489, 1, 0, 0],\n",
       "  [204, 1, 0, 0],\n",
       "  [1902, 1, 0, 0],\n",
       "  [3, 6039, 1, 0],\n",
       "  [3, 10794, 1, 0]]}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the preprocessed, tokenized dataset is loaded as expected\n",
    "eval_dataset[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "51ac33f0-d8b4-4fd8-8a06-734d41a0594c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-18 04:36:08.426196: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\nTo enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\nYou are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    }
   ],
   "source": [
    "# reinstantiate our tokenizer and model in bfloat16\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-large\")\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-large\", device_map=\"auto\", torch_dtype=torch.bfloat16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8d93c300-06c1-4a08-a35f-6d1c31a57c88",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.bfloat16"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# double check the precision of our tensors\n",
    "model.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "83605608-91f8-42c3-bf9a-d4fe252f0346",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory status: \nGPU 0:\n  Total memory: 15.57 GB\n  Allocated memory: 1.50 GB\n  Reserved memory: 1.53 GB\n  Available memory: 14.04 GB\n"
     ]
    }
   ],
   "source": [
    "# use the custom mem_status fn to check the amount of memory used on the T4 GPU\n",
    "def mem_status(): \n",
    "    if torch.cuda.is_available():\n",
    "        gpus = torch.cuda.device_count()\n",
    "        print(\"Memory status: \")\n",
    "        for i in range(gpus):\n",
    "            properties = torch.cuda.get_device_properties(i)\n",
    "            total_memory = properties.total_memory / (1024 ** 3)  # Convert to GB\n",
    "            allocated_memory = torch.cuda.memory_allocated(i) / (1024 ** 3)  # Convert to GB\n",
    "            reserved_memory = torch.cuda.memory_reserved(i) / (1024 ** 3)  # Convert to GB\n",
    "            available_memory = total_memory - reserved_memory\n",
    "            print(f\"GPU {i}:\")\n",
    "            print(f\"  Total memory: {total_memory:.2f} GB\")\n",
    "            print(f\"  Allocated memory: {allocated_memory:.2f} GB\")\n",
    "            print(f\"  Reserved memory: {reserved_memory:.2f} GB\")\n",
    "            print(f\"  Available memory: {available_memory:.2f} GB\")\n",
    "    else:\n",
    "        print(\"No GPU available.\")\n",
    "\n",
    "mem_status()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2769051a-38ba-4ef6-bfd2-8d18e3964770",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# preliminarily, we convert our tokenized datasets' data format to numpy\n",
    "# this will ultimately be required under the hood by the DataCollatorForSeq2Seq class for padding the labels to the same length in each of our batches \n",
    "eval_dataset.set_format(\"numpy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fecacc10-b571-4b58-bc1a-f9fba97cd47a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# time to configure our dataloaders with a unique collating function\n",
    "\n",
    "from transformers import DataCollatorForSeq2Seq\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "batch_size = 32 # multiple of 8 to optimize computation on Nvidia tensor cores\n",
    "\n",
    "# we make use of the standard seq2seq collator and pass the powerful `pad_to_multiple_of` argument as discussed in the preprocessing notebook\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, label_pad_token_id=tokenizer.pad_token_id, pad_to_multiple_of=2)\n",
    "\n",
    "eval_dataloader = DataLoader(\n",
    "    eval_dataset, \n",
    "    batch_size=batch_size, \n",
    "    collate_fn=data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9deb56a5-8096-43b0-89c5-7a18d5820703",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/local_disk0/.ephemeral_nfs/envs/pythonEnv-c6dc5ff9-367d-4006-8e30-1875d7d29fb6/lib/python3.11/site-packages/transformers/data/data_collator.py:657: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:274.)\n  batch[\"labels\"] = torch.tensor(batch[\"labels\"], dtype=torch.int64)\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'input_ids': torch.Size([32, 34]),\n",
       " 'attention_mask': torch.Size([32, 34]),\n",
       " 'labels': torch.Size([32, 4])}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# grab a batch from our eval dataloader\n",
    "for batch in eval_dataloader:\n",
    "    break\n",
    "{k: v.shape for k, v in batch.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "df98d6a2-2f8c-4e95-9b9f-39331d3749d5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# put all elements of the batch on the GPU\n",
    "device = torch.device(\"cuda\")\n",
    "batch = {k: v.to(device) for k, v in batch.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "96df3560-5d4f-4e3c-bd2f-2f22a8bf5a8a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(tensor(15.1875, device='cuda:0', dtype=torch.bfloat16,\n",
       "        grad_fn=<NllLossBackward0>),\n",
       " torch.Size([32, 4, 32128]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check how the model responds to a single batch before starting the whole benchmarking eval\n",
    "outputs = model(**batch)\n",
    "outputs.loss, outputs.logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "812c56af-8f22-4392-a9d2-4d9db7778023",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(tensor([[ 314,    1,    1,  489],\n",
       "         [ 668,    1,    1,  431],\n",
       "         [   3,    1,    1, 1401],\n",
       "         [   3, 5783,    1,    1],\n",
       "         [   3,   18,    1,    1],\n",
       "         [   3,   18, 2469,    1],\n",
       "         [   3, 6039,    1,    1],\n",
       "         [   3,    1,    1,    3],\n",
       "         [   3,    1,    1,    3],\n",
       "         [   3,   18, 3420,    1],\n",
       "         [ 944,    1,    1,  489],\n",
       "         [   3,   18,    1,    1],\n",
       "         [ 668,    1,    1,  489],\n",
       "         [   3,   18,    1,    1],\n",
       "         [   3,   18, 2773,    1],\n",
       "         [ 204,    1,    1,  204],\n",
       "         [   3,   18, 2688,    1],\n",
       "         [   3,    1,    1,    3],\n",
       "         [   3,   18, 3420,    1],\n",
       "         [   3,   18, 3420,    1],\n",
       "         [ 489,    1,    1,  305],\n",
       "         [   3,   18, 2469,    1],\n",
       "         [   3,    1,    1,    3],\n",
       "         [   3,   18, 2773,    1],\n",
       "         [   3,    1,    1,    3],\n",
       "         [   3, 4536,    1,    1],\n",
       "         [ 314,    1,    1,  489],\n",
       "         [   3,   18, 3341,    1],\n",
       "         [   3,   18, 2773,    1],\n",
       "         [   3,    1,    1,  507],\n",
       "         [   3,    1,    1,    3],\n",
       "         [   3, 6039,    1,    1]], device='cuda:0'),\n",
       " tensor([[  489,     1,     0,     0],\n",
       "         [  204,     1,     0,     0],\n",
       "         [ 1902,     1,     0,     0],\n",
       "         [    3,  6039,     1,     0],\n",
       "         [    3, 10794,     1,     0],\n",
       "         [    3,    18,  2577,     1],\n",
       "         [    3,  5947,     1,     0],\n",
       "         [ 2307,     1,     0,     0],\n",
       "         [ 6426,     1,     0,     0],\n",
       "         [    3,    18,  3747,     1],\n",
       "         [  489,     1,     0,     0],\n",
       "         [    3,  5947,     1,     0],\n",
       "         [  968,     1,     0,     0],\n",
       "         [    3,  9169,     1,     0],\n",
       "         [    3,    18,  3747,     1],\n",
       "         [  204,     1,     0,     0],\n",
       "         [    3,    18,  3707,     1],\n",
       "         [ 9526,     1,     0,     0],\n",
       "         [    3,    18,  3166,     1],\n",
       "         [    3,    18,  3628,     1],\n",
       "         [  305,     1,     0,     0],\n",
       "         [    3,    18,  2128,     1],\n",
       "         [ 2838,     1,     0,     0],\n",
       "         [    3,    18,  4165,     1],\n",
       "         [  335,     1,     0,     0],\n",
       "         [    3,  6832,     1,     0],\n",
       "         [  489,     1,     0,     0],\n",
       "         [    3,    18,  2128,     1],\n",
       "         [    3,    18,  4118,     1],\n",
       "         [  957,     1,     0,     0],\n",
       "         [ 9526,     1,     0,     0],\n",
       "         [    3,  7141,     1,     0]], device='cuda:0'))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.logits.argmax(dim=-1), batch[\"labels\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7f6bb24c-a253-461b-9c5b-c38998677176",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory status: \nGPU 0:\n  Total memory: 15.57 GB\n  Allocated memory: 3.40 GB\n  Reserved memory: 3.47 GB\n  Available memory: 12.11 GB\n"
     ]
    }
   ],
   "source": [
    "# check the updated mem status after passing the test batch into the model for inference\n",
    "mem_status()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8e2e3c69-dc12-4e5e-bad3-5ec26241108f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction: 4 7, label: 7\nprediction: 9 6, label: 2\nprediction: 21, label: 23\nprediction: -6, label: -8\nprediction: -, label: -17\nprediction: -35, label: -28\nprediction: -8, label: -12\nprediction: , label: 27\nprediction: , label: 42\nprediction: -36, label: -38\nprediction: 25 7, label: 7\nprediction: -, label: -12\nprediction: 9 7, label: 14\nprediction: -, label: -11\nprediction: -23, label: -38\nprediction: 2 2, label: 2\nprediction: -26, label: -48\nprediction: , label: 49\nprediction: -36, label: -29\nprediction: -36, label: -44\nprediction: 7 5, label: 5\nprediction: -35, label: -45\nprediction: , label: 29\nprediction: -23, label: -42\nprediction: , label: 10\nprediction: -10, label: -7\nprediction: 4 7, label: 7\nprediction: -31, label: -45\nprediction: -23, label: -37\nprediction: 18, label: 19\nprediction: , label: 49\nprediction: -8, label: -9\n"
     ]
    }
   ],
   "source": [
    "# compare predictions to labels in the test batch\n",
    "for pred, label in zip([tokenizer.decode(pred, skip_special_tokens=True) for pred in outputs.logits.argmax(dim=-1)], [tokenizer.decode(label, skip_special_tokens=True) for label in batch[\"labels\"]]):\n",
    "    print(f\"prediction: {pred}, label: {label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0c5485da-4a92-4ac0-bcdb-11c37bdd66ca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# create exact match benchmark metric to be used for evaluation\n",
    "from evaluate import load\n",
    "\n",
    "exact_match_test_benchmark = load(\"exact_match\")\n",
    "# f1_benchmark = load(\"f1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ce7022d7-4151-4af3-b33b-668276cb8270",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> The exact_match score of this batch is: {'exact_match': 0.0}\n"
     ]
    }
   ],
   "source": [
    "# TODO remove since we are adding items individually\n",
    "\n",
    "# compute metric for first batch - how many of the predictions are correct? \n",
    "exact_match_test_benchmark.add_batch(predictions=[tokenizer.decode(pred, skip_special_tokens=True) for pred in outputs.logits.argmax(dim=-1)], references=[tokenizer.decode(label, skip_special_tokens=True) for label in batch[\"labels\"]])\n",
    "\n",
    "# f1_benchmark.add_batch(predictions=[pred for pred in outputs.logits.argmax(dim=-1).tolist()], references=[label for label in batch[\"labels\"].tolist()])\n",
    "\n",
    "print(\">>> The exact_match score of this batch is: \" + str(exact_match_test_benchmark.compute()))\n",
    "# print(\">>> The f1 score of this batch is: \" + str(f1_benchmark.compute()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "360c2174-0a6e-44af-9853-94cb9285a7a8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO - do we need this? \n",
    "\n",
    "# comparing the output tokens directly - double checking the above\n",
    "num_correct = 0\n",
    "for i in batch[\"labels\"]:\n",
    "    for j in outputs.logits.argmax(dim=-1):\n",
    "        if i.cpu().numpy().tolist() == j.cpu().numpy().tolist():\n",
    "            num_correct            \n",
    "num_correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0408079c-27ca-46f0-9253-1c7e5e6c2fb6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> The exact_match score of this batch is: {'exact_match': 0.0}\n"
     ]
    }
   ],
   "source": [
    "# get the exact match score and create a dataset for individual partial correctness in the same loop\n",
    "batch_partials = {\"predicted_tokens\": [], \"label_tokens\": [], \"decoded_prediction\": [], \"decoded_label\": []}\n",
    "\n",
    "for pred, label in zip(outputs.logits.argmax(dim=-1), batch[\"labels\"]):\n",
    "    \n",
    "    exact_match_test_benchmark.add(predictions=tokenizer.decode(pred, skip_special_tokens=True), references=tokenizer.decode(label, skip_special_tokens=True))\n",
    "    \n",
    "    batch_partials[\"predicted_tokens\"].append(pred)\n",
    "    batch_partials[\"label_tokens\"].append(label)\n",
    "    batch_partials[\"decoded_prediction\"].append(tokenizer.decode(pred, skip_special_tokens=True))\n",
    "    batch_partials[\"decoded_label\"].append(tokenizer.decode(label, skip_special_tokens=True))\n",
    "\n",
    "print(\">>> The exact_match score of this batch is: \" + str(exact_match_test_benchmark.compute()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "02892fa6-0b81-4603-aa00-0e35526642f5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'predicted_tokens': [tensor([314,   1,   1, 489], device='cuda:0'),\n",
       "  tensor([668,   1,   1, 431], device='cuda:0'),\n",
       "  tensor([   3,    1,    1, 1401], device='cuda:0'),\n",
       "  tensor([   3, 5783,    1,    1], device='cuda:0'),\n",
       "  tensor([ 3, 18,  1,  1], device='cuda:0'),\n",
       "  tensor([   3,   18, 2469,    1], device='cuda:0'),\n",
       "  tensor([   3, 6039,    1,    1], device='cuda:0'),\n",
       "  tensor([3, 1, 1, 3], device='cuda:0'),\n",
       "  tensor([3, 1, 1, 3], device='cuda:0'),\n",
       "  tensor([   3,   18, 3420,    1], device='cuda:0'),\n",
       "  tensor([944,   1,   1, 489], device='cuda:0'),\n",
       "  tensor([ 3, 18,  1,  1], device='cuda:0'),\n",
       "  tensor([668,   1,   1, 489], device='cuda:0'),\n",
       "  tensor([ 3, 18,  1,  1], device='cuda:0'),\n",
       "  tensor([   3,   18, 2773,    1], device='cuda:0'),\n",
       "  tensor([204,   1,   1, 204], device='cuda:0'),\n",
       "  tensor([   3,   18, 2688,    1], device='cuda:0'),\n",
       "  tensor([3, 1, 1, 3], device='cuda:0'),\n",
       "  tensor([   3,   18, 3420,    1], device='cuda:0'),\n",
       "  tensor([   3,   18, 3420,    1], device='cuda:0'),\n",
       "  tensor([489,   1,   1, 305], device='cuda:0'),\n",
       "  tensor([   3,   18, 2469,    1], device='cuda:0'),\n",
       "  tensor([3, 1, 1, 3], device='cuda:0'),\n",
       "  tensor([   3,   18, 2773,    1], device='cuda:0'),\n",
       "  tensor([3, 1, 1, 3], device='cuda:0'),\n",
       "  tensor([   3, 4536,    1,    1], device='cuda:0'),\n",
       "  tensor([314,   1,   1, 489], device='cuda:0'),\n",
       "  tensor([   3,   18, 3341,    1], device='cuda:0'),\n",
       "  tensor([   3,   18, 2773,    1], device='cuda:0'),\n",
       "  tensor([  3,   1,   1, 507], device='cuda:0'),\n",
       "  tensor([3, 1, 1, 3], device='cuda:0'),\n",
       "  tensor([   3, 6039,    1,    1], device='cuda:0')],\n",
       " 'label_tokens': [tensor([489,   1,   0,   0], device='cuda:0'),\n",
       "  tensor([204,   1,   0,   0], device='cuda:0'),\n",
       "  tensor([1902,    1,    0,    0], device='cuda:0'),\n",
       "  tensor([   3, 6039,    1,    0], device='cuda:0'),\n",
       "  tensor([    3, 10794,     1,     0], device='cuda:0'),\n",
       "  tensor([   3,   18, 2577,    1], device='cuda:0'),\n",
       "  tensor([   3, 5947,    1,    0], device='cuda:0'),\n",
       "  tensor([2307,    1,    0,    0], device='cuda:0'),\n",
       "  tensor([6426,    1,    0,    0], device='cuda:0'),\n",
       "  tensor([   3,   18, 3747,    1], device='cuda:0'),\n",
       "  tensor([489,   1,   0,   0], device='cuda:0'),\n",
       "  tensor([   3, 5947,    1,    0], device='cuda:0'),\n",
       "  tensor([968,   1,   0,   0], device='cuda:0'),\n",
       "  tensor([   3, 9169,    1,    0], device='cuda:0'),\n",
       "  tensor([   3,   18, 3747,    1], device='cuda:0'),\n",
       "  tensor([204,   1,   0,   0], device='cuda:0'),\n",
       "  tensor([   3,   18, 3707,    1], device='cuda:0'),\n",
       "  tensor([9526,    1,    0,    0], device='cuda:0'),\n",
       "  tensor([   3,   18, 3166,    1], device='cuda:0'),\n",
       "  tensor([   3,   18, 3628,    1], device='cuda:0'),\n",
       "  tensor([305,   1,   0,   0], device='cuda:0'),\n",
       "  tensor([   3,   18, 2128,    1], device='cuda:0'),\n",
       "  tensor([2838,    1,    0,    0], device='cuda:0'),\n",
       "  tensor([   3,   18, 4165,    1], device='cuda:0'),\n",
       "  tensor([335,   1,   0,   0], device='cuda:0'),\n",
       "  tensor([   3, 6832,    1,    0], device='cuda:0'),\n",
       "  tensor([489,   1,   0,   0], device='cuda:0'),\n",
       "  tensor([   3,   18, 2128,    1], device='cuda:0'),\n",
       "  tensor([   3,   18, 4118,    1], device='cuda:0'),\n",
       "  tensor([957,   1,   0,   0], device='cuda:0'),\n",
       "  tensor([9526,    1,    0,    0], device='cuda:0'),\n",
       "  tensor([   3, 7141,    1,    0], device='cuda:0')],\n",
       " 'decoded_prediction': ['4 7',\n",
       "  '9 6',\n",
       "  '21',\n",
       "  '-6',\n",
       "  '-',\n",
       "  '-35',\n",
       "  '-8',\n",
       "  '',\n",
       "  '',\n",
       "  '-36',\n",
       "  '25 7',\n",
       "  '-',\n",
       "  '9 7',\n",
       "  '-',\n",
       "  '-23',\n",
       "  '2 2',\n",
       "  '-26',\n",
       "  '',\n",
       "  '-36',\n",
       "  '-36',\n",
       "  '7 5',\n",
       "  '-35',\n",
       "  '',\n",
       "  '-23',\n",
       "  '',\n",
       "  '-10',\n",
       "  '4 7',\n",
       "  '-31',\n",
       "  '-23',\n",
       "  '18',\n",
       "  '',\n",
       "  '-8'],\n",
       " 'decoded_label': ['7',\n",
       "  '2',\n",
       "  '23',\n",
       "  '-8',\n",
       "  '-17',\n",
       "  '-28',\n",
       "  '-12',\n",
       "  '27',\n",
       "  '42',\n",
       "  '-38',\n",
       "  '7',\n",
       "  '-12',\n",
       "  '14',\n",
       "  '-11',\n",
       "  '-38',\n",
       "  '2',\n",
       "  '-48',\n",
       "  '49',\n",
       "  '-29',\n",
       "  '-44',\n",
       "  '5',\n",
       "  '-45',\n",
       "  '29',\n",
       "  '-42',\n",
       "  '10',\n",
       "  '-7',\n",
       "  '7',\n",
       "  '-45',\n",
       "  '-37',\n",
       "  '19',\n",
       "  '49',\n",
       "  '-9']}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# peek dict\n",
    "batch_partials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "92a1c347-dc71-4523-a998-c5d840567aee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# confirm we can construct a \uD83E\uDD17 datasets Dataset object from this dict\n",
    "from datasets import Dataset\n",
    "\n",
    "ds = Dataset.from_dict(batch_partials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c8e0f91b-5eb7-40f7-b0e7-f54ad9cd79d0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(Dataset({\n",
       "     features: ['predicted_tokens', 'label_tokens', 'decoded_prediction', 'decoded_label'],\n",
       "     num_rows: 32\n",
       " }),\n",
       " [[314, 1, 1, 489],\n",
       "  [668, 1, 1, 431],\n",
       "  [3, 1, 1, 1401],\n",
       "  [3, 5783, 1, 1],\n",
       "  [3, 18, 1, 1],\n",
       "  [3, 18, 2469, 1],\n",
       "  [3, 6039, 1, 1],\n",
       "  [3, 1, 1, 3],\n",
       "  [3, 1, 1, 3],\n",
       "  [3, 18, 3420, 1],\n",
       "  [944, 1, 1, 489],\n",
       "  [3, 18, 1, 1],\n",
       "  [668, 1, 1, 489],\n",
       "  [3, 18, 1, 1],\n",
       "  [3, 18, 2773, 1],\n",
       "  [204, 1, 1, 204],\n",
       "  [3, 18, 2688, 1],\n",
       "  [3, 1, 1, 3],\n",
       "  [3, 18, 3420, 1],\n",
       "  [3, 18, 3420, 1],\n",
       "  [489, 1, 1, 305],\n",
       "  [3, 18, 2469, 1],\n",
       "  [3, 1, 1, 3],\n",
       "  [3, 18, 2773, 1],\n",
       "  [3, 1, 1, 3],\n",
       "  [3, 4536, 1, 1],\n",
       "  [314, 1, 1, 489],\n",
       "  [3, 18, 3341, 1],\n",
       "  [3, 18, 2773, 1],\n",
       "  [3, 1, 1, 507],\n",
       "  [3, 1, 1, 3],\n",
       "  [3, 6039, 1, 1]],\n",
       " ['7',\n",
       "  '2',\n",
       "  '23',\n",
       "  '-8',\n",
       "  '-17',\n",
       "  '-28',\n",
       "  '-12',\n",
       "  '27',\n",
       "  '42',\n",
       "  '-38',\n",
       "  '7',\n",
       "  '-12',\n",
       "  '14',\n",
       "  '-11',\n",
       "  '-38',\n",
       "  '2',\n",
       "  '-48',\n",
       "  '49',\n",
       "  '-29',\n",
       "  '-44',\n",
       "  '5',\n",
       "  '-45',\n",
       "  '29',\n",
       "  '-42',\n",
       "  '10',\n",
       "  '-7',\n",
       "  '7',\n",
       "  '-45',\n",
       "  '-37',\n",
       "  '19',\n",
       "  '49',\n",
       "  '-9'])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# peek \uD83E\uDD17 Dataset format\n",
    "ds, ds[\"predicted_tokens\"], ds[\"decoded_label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0570e6e0-af98-4b78-aaa0-8c3eea20b946",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "313"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the number of eval steps for the entire benchmarking process\n",
    "num_eval_steps = len(eval_dataloader)\n",
    "num_eval_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "847f96ee-5845-43d0-993a-5ac385ea0747",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ec2b3e9233c481fbe885d0a35a57e27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory status: \nGPU 0:\n  Total memory: 15.57 GB\n  Allocated memory: 3.53 GB\n  Reserved memory: 3.77 GB\n  Available memory: 11.81 GB\nMemory status: \nGPU 0:\n  Total memory: 15.57 GB\n  Allocated memory: 3.54 GB\n  Reserved memory: 3.81 GB\n  Available memory: 11.77 GB\nMemory status: \nGPU 0:\n  Total memory: 15.57 GB\n  Allocated memory: 3.54 GB\n  Reserved memory: 3.81 GB\n  Available memory: 11.77 GB\n{'exact_match': 0.0956}\n"
     ]
    }
   ],
   "source": [
    "# now we run a full benchmarking on our eval set\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "progress_bar = tqdm(range(num_eval_steps))\n",
    "exact_match_benchmark = load(\"exact_match\")\n",
    "# create an empty dict to populate with the results for partial correctness evaluation\n",
    "partials = {\"predicted_tokens\": [], \"label_tokens\": [], \"decoded_prediction\": [], \"decoded_label\": []}\n",
    "\n",
    "model.eval()\n",
    "for batch in eval_dataloader:\n",
    "    batch = {k: v.to(device) for k, v in batch.items()}\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**batch)\n",
    "\n",
    "    # logits = outputs.logits\n",
    "    # predictions = torch.argmax(logits, dim=-1)\n",
    "    for pred, label in zip(outputs.logits.argmax(dim=-1), batch[\"labels\"]):\n",
    "        # add decoded predictions and labels to the metric object\n",
    "        exact_match_test_benchmark.add(predictions=tokenizer.decode(pred, skip_special_tokens=True), references=tokenizer.decode(label, skip_special_tokens=True))\n",
    "        # populate the partial correctness dict for detailed, individual eval\n",
    "        partials[\"predicted_tokens\"].append(pred)\n",
    "        partials[\"label_tokens\"].append(label)\n",
    "        partials[\"decoded_prediction\"].append(tokenizer.decode(pred, skip_special_tokens=True))\n",
    "        partials[\"decoded_label\"].append(tokenizer.decode(label, skip_special_tokens=True))\n",
    "\n",
    "    # exact_match_benchmark.add_batch(\n",
    "    #     predictions=[tokenizer.decode(pred, skip_special_tokens=True) for pred in outputs.logits.argmax(dim=-1)], \n",
    "    #     references=[tokenizer.decode(label, skip_special_tokens=True) for label in batch[\"labels\"]]\n",
    "    # )\n",
    "    # # here add all the predictions and labels to their own datasets\n",
    "    # for pred in outputs.logits.argmax(dim=-1).tolist():\n",
    "\n",
    "    # f1_benchmark.add_batch(predictions=[pred for pred in outputs.logits.argmax(dim=-1)], references=[label for label in batch[\"labels\"]])\n",
    "    progress_bar.update(1)\n",
    "    if progress_bar.n % 100 == 0:\n",
    "        mem_status()\n",
    "\n",
    "print(exact_match_test_benchmark.compute())\n",
    "partial_correctness_dataset = Dataset.from_dict(partials)\n",
    "# print(f1_benchmark.compute())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "58ea679a-fa8b-437c-8b11-b4d60d39b02a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['predicted_tokens', 'label_tokens', 'decoded_prediction', 'decoded_label'],\n",
       "    num_rows: 10000\n",
       "})"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# peek the created dataset\n",
    "partial_correctness_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b9316bea-3f5b-456a-b676-e3bbf6b759a3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# lets push the partial evaluation dataset to the hub for saving\n",
    "dbutils.widgets.text(\"hf_token\", \"\", \"hf_token\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b7aa4557-89f6-49db-8834-805b65cfe7a4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: huggingface-cli <command> [<args>] login [-h] [--token TOKEN]\r\n                                                [--add-to-git-credential]\r\nhuggingface-cli <command> [<args>] login: error: argument --token: expected one argument\r\n"
     ]
    }
   ],
   "source": [
    "hf_token = dbutils.widgets.get(\"hf_token\")\n",
    "!huggingface-cli login --token $hf_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b0bfc5f1-3b33-4cb8-9c35-0ef962209523",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "574de89c3f3e4b6fba2e2e143bc56426",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c10f9ef3ff14970a7b16e474565cc18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/10 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/MarioBarbeque/FLAN-T5-DeepMind-LinAlg-1D-benchmark/commit/6c76ae43e827c8037a55a790c57a88f0d03addc5', commit_message=\"dataset constructed for benchmarking the partial correctness of the pretrained FLAN T5 large model's ability to solve 1D linear equations\", commit_description='', oid='6c76ae43e827c8037a55a790c57a88f0d03addc5', pr_url=None, repo_url=RepoUrl('https://huggingface.co/datasets/MarioBarbeque/FLAN-T5-DeepMind-LinAlg-1D-benchmark', endpoint='https://huggingface.co', repo_type='dataset', repo_id='MarioBarbeque/FLAN-T5-DeepMind-LinAlg-1D-benchmark'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "partial_correctness_dataset.push_to_hub(\"FLAN-T5-DeepMind-LinAlg-1D-benchmark\", commit_message=\"dataset constructed for benchmarking the partial correctness of the pretrained FLAN T5 large model's ability to solve 1D linear equations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d9b705c7-bf0e-40c5-b3fb-2b8a56b2d584",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/python_shell/dbruntime/huggingface_patches/datasets.py:14: UserWarning: During large dataset downloads, there could be multiple progress bar widgets that can cause performance issues for your notebook or browser. To avoid these issues, use `datasets.utils.logging.disable_progress_bar()` to turn off the progress bars.\n  warnings.warn(\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d19a50afe6d74d868c745140cf5f26f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/415 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/python_shell/dbruntime/huggingface_patches/datasets.py:127: UserWarning: The dataset would be saved to both local disk and PersistentStorageType.VOLUMES for better performance.\n  warnings.warn(\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03c4b0548f9342dd81466eab8b2a8881",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/103k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7db0de673334818a879feda1bc2b5b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating eval split:   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "partial_correctness_ds = load_dataset(\"MarioBarbeque/FLAN-T5-DeepMind-LinAlg-1D-benchmark\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e8e81517-5044-4728-98ee-a258e603362d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# rename the split to eval \n",
    "data = partial_correctness_ds.pop(\"train\")\n",
    "partial_correctness_ds[\"eval\"] = data\n",
    "partial_correctness_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eb77e51d-2f5b-4f7d-8f7f-6c3c9cb2d205",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1acc79b507d54e2eb866c8cea809bd87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0158c4cda6146c781dfdb52184b428e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/10 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b2f1854234f4a249fc11564e2f53f4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/418 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/python/lib/python3.11/site-packages/huggingface_hub/file_download.py:157: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in /Volumes/workspace_dogfood/jgr/hugging_face_cache/hub/datasets--MarioBarbeque--FLAN-T5-DeepMind-LinAlg-1D-benchmark. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n  warnings.warn(message)\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/MarioBarbeque/FLAN-T5-DeepMind-LinAlg-1D-benchmark/commit/4cbda527ccc0f11917280f9a0fd37ac0cac072b2', commit_message=\"update the dataset's split name\", commit_description='', oid='4cbda527ccc0f11917280f9a0fd37ac0cac072b2', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "partial_correctness_ds.push_to_hub(\"FLAN-T5-DeepMind-LinAlg-1D-benchmark\", commit_message=\"update the dataset's split name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7c539ea4-3b11-456c-afd4-bf0a7209b022",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['predicted_tokens', 'label_tokens', 'decoded_prediction', 'decoded_label'],\n",
       "    num_rows: 10000\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "partial_correctness_ds = partial_correctness_ds[\"eval\"]\n",
    "partial_correctness_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4b103acc-8835-48a1-81df-30c43beb8aba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'predicted_tokens': [[314, 1, 1, 489],\n",
       "  [668, 1, 1, 431],\n",
       "  [3, 1, 1, 1401],\n",
       "  [3, 5783, 1, 1],\n",
       "  [3, 18, 1, 1],\n",
       "  [3, 18, 2469, 1],\n",
       "  [3, 6039, 1, 1],\n",
       "  [3, 1, 1, 3],\n",
       "  [3, 1, 1, 3],\n",
       "  [3, 18, 3420, 1]],\n",
       " 'label_tokens': [[489, 1, 0, 0],\n",
       "  [204, 1, 0, 0],\n",
       "  [1902, 1, 0, 0],\n",
       "  [3, 6039, 1, 0],\n",
       "  [3, 10794, 1, 0],\n",
       "  [3, 18, 2577, 1],\n",
       "  [3, 5947, 1, 0],\n",
       "  [2307, 1, 0, 0],\n",
       "  [6426, 1, 0, 0],\n",
       "  [3, 18, 3747, 1]],\n",
       " 'decoded_prediction': ['4 7',\n",
       "  '9 6',\n",
       "  '21',\n",
       "  '-6',\n",
       "  '-',\n",
       "  '-35',\n",
       "  '-8',\n",
       "  '',\n",
       "  '',\n",
       "  '-36'],\n",
       " 'decoded_label': ['7',\n",
       "  '2',\n",
       "  '23',\n",
       "  '-8',\n",
       "  '-17',\n",
       "  '-28',\n",
       "  '-12',\n",
       "  '27',\n",
       "  '42',\n",
       "  '-38']}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lets analyze this data\n",
    "partial_correctness_ds[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aaba8b2c-b6e2-4390-81c7-5cf6fd4ad9ee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# first, add some interesting metrics to each of our records\n",
    "# lets score the f1 metric, precision, and recall scores for each predicition, inlcusive of the special tokens, while also adding a boolean flag for exact matches\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "\n",
    "def compute_individual_metrics(example):\n",
    "    example[\"f1__w_special\"] = f1_score(y_true=example[\"label_tokens\"], y_pred=example[\"predicted_tokens\"], average='micro')\n",
    "    example[\"precision__w_special\"] = precision_score(y_true=example[\"label_tokens\"], y_pred=example[\"predicted_tokens\"], average='micro')\n",
    "    example[\"recall__w_special\"] = recall_score(y_true=example[\"label_tokens\"], y_pred=example[\"predicted_tokens\"], average='micro')\n",
    "    example[\"is_exact_match\"] = example[\"decoded_prediction\"] == example[\"decoded_label\"]\n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b251eec0-688f-4b6a-9e18-178845eb867b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'predicted_tokens': [314, 1, 1, 489],\n",
       " 'label_tokens': [489, 1, 0, 0],\n",
       " 'decoded_prediction': '4 7',\n",
       " 'decoded_label': '7',\n",
       " 'f1__w_special': 0.25,\n",
       " 'precision__w_special': 0.25,\n",
       " 'recall__w_special': 0.25,\n",
       " 'is_exact_match': False}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test metric computation one single record\n",
    "test_record = compute_individual_metrics(partial_correctness_ds[0])\n",
    "test_record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ac057607-e7f1-4c27-abc5-fbeb0498df0b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0ccecb095284d5dae0c32c6a17023d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "individual_metrics_ds = partial_correctness_ds.map(compute_individual_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cf77bba8-173c-46ee-9f6b-b74615581a7b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'predicted_tokens': [[314, 1, 1, 489],\n",
       "  [668, 1, 1, 431],\n",
       "  [3, 1, 1, 1401],\n",
       "  [3, 5783, 1, 1],\n",
       "  [3, 18, 1, 1]],\n",
       " 'label_tokens': [[489, 1, 0, 0],\n",
       "  [204, 1, 0, 0],\n",
       "  [1902, 1, 0, 0],\n",
       "  [3, 6039, 1, 0],\n",
       "  [3, 10794, 1, 0]],\n",
       " 'decoded_prediction': ['4 7', '9 6', '21', '-6', '-'],\n",
       " 'decoded_label': ['7', '2', '23', '-8', '-17'],\n",
       " 'f1__w_special': [0.25, 0.25, 0.25, 0.5, 0.5],\n",
       " 'precision__w_special': [0.25, 0.25, 0.25, 0.5, 0.5],\n",
       " 'recall__w_special': [0.25, 0.25, 0.25, 0.5, 0.5],\n",
       " 'is_exact_match': [False, False, False, False, False]}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "individual_metrics_ds[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3bdf8274-4474-405b-b1a4-dee380c84cdd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "339774b46c48482f89c55d76d1754edc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# we know the exact match score is .0956\n",
    "exact_matches = individual_metrics_ds.filter(lambda x: x['is_exact_match'] == True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1484e700-d06d-41bd-820e-1505ff44c661",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "956"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(exact_matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "14188c3e-e43c-4c6a-85b8-b6e2aa3f46bd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "126"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_perfect_tokens = 0\n",
    "for match in exact_matches:\n",
    "    if match['f1__w_special'] == 1.0:\n",
    "        num_perfect_tokens += 1\n",
    "num_perfect_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ede158b3-ff28-4792-b788-2077641185bf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Of the correct decoded matches, only 0.1318 have predicted and label tokens that are identical.\n"
     ]
    }
   ],
   "source": [
    "# that is, only __% of the exactly matched decoded predicitions are equivalent in terms of tokenization\n",
    "print(f\">>> Of the correct decoded matches, only {num_perfect_tokens / len(exact_matches):.4f} have predicted and label tokens that are identical.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e28c0f09-6665-4fc7-a605-828f73a58175",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'predicted_tokens': [[3, 4525, 1, 1],\n",
       "  [3, 18, 2773, 1],\n",
       "  [3, 4949, 1, 1],\n",
       "  [3, 1, 1, 505],\n",
       "  [3, 1, 1, 944],\n",
       "  [3, 1, 1, 668],\n",
       "  [3, 18, 2469, 1],\n",
       "  [3, 1, 1, 507],\n",
       "  [3, 6039, 1, 1],\n",
       "  [3, 1, 1, 604],\n",
       "  [3, 1, 1, 943],\n",
       "  [3, 1, 1, 305],\n",
       "  [3, 1, 1, 209],\n",
       "  [3, 1, 1, 6654],\n",
       "  [3, 18, 3647, 1],\n",
       "  [3, 1, 1, 898],\n",
       "  [3, 1, 1, 335],\n",
       "  [3, 1, 1, 1630],\n",
       "  [3, 1, 1, 335],\n",
       "  [3, 2292, 1, 1],\n",
       "  [3, 18, 2773, 1],\n",
       "  [3, 632, 1, 1],\n",
       "  [3, 4949, 1, 1],\n",
       "  [3, 1, 1, 204],\n",
       "  [3, 1, 1, 505]],\n",
       " 'label_tokens': [[3, 4525, 1, 0],\n",
       "  [3, 18, 2773, 1],\n",
       "  [3, 4949, 1, 0],\n",
       "  [505, 1, 0, 0],\n",
       "  [944, 1, 0, 0],\n",
       "  [668, 1, 0, 0],\n",
       "  [3, 18, 2469, 1],\n",
       "  [507, 1, 0, 0],\n",
       "  [3, 6039, 1, 0],\n",
       "  [604, 1, 0, 0],\n",
       "  [943, 1, 0, 0],\n",
       "  [305, 1, 0, 0],\n",
       "  [209, 1, 0, 0],\n",
       "  [6654, 1, 0, 0],\n",
       "  [3, 18, 3647, 1],\n",
       "  [898, 1, 0, 0],\n",
       "  [335, 1, 0, 0],\n",
       "  [1630, 1, 0, 0],\n",
       "  [335, 1, 0, 0],\n",
       "  [3, 2292, 1, 0],\n",
       "  [3, 18, 2773, 1],\n",
       "  [3, 632, 1, 0],\n",
       "  [3, 4949, 1, 0],\n",
       "  [204, 1, 0, 0],\n",
       "  [505, 1, 0, 0]],\n",
       " 'decoded_prediction': ['-5',\n",
       "  '-23',\n",
       "  '-2',\n",
       "  '8',\n",
       "  '25',\n",
       "  '9',\n",
       "  '-35',\n",
       "  '18',\n",
       "  '-8',\n",
       "  '30',\n",
       "  '50',\n",
       "  '5',\n",
       "  '1',\n",
       "  '38',\n",
       "  '-49',\n",
       "  '16',\n",
       "  '10',\n",
       "  '22',\n",
       "  '10',\n",
       "  '-1',\n",
       "  '-23',\n",
       "  '0',\n",
       "  '-2',\n",
       "  '2',\n",
       "  '8'],\n",
       " 'decoded_label': ['-5',\n",
       "  '-23',\n",
       "  '-2',\n",
       "  '8',\n",
       "  '25',\n",
       "  '9',\n",
       "  '-35',\n",
       "  '18',\n",
       "  '-8',\n",
       "  '30',\n",
       "  '50',\n",
       "  '5',\n",
       "  '1',\n",
       "  '38',\n",
       "  '-49',\n",
       "  '16',\n",
       "  '10',\n",
       "  '22',\n",
       "  '10',\n",
       "  '-1',\n",
       "  '-23',\n",
       "  '0',\n",
       "  '-2',\n",
       "  '2',\n",
       "  '8'],\n",
       " 'f1__w_special': [0.75,\n",
       "  1.0,\n",
       "  0.75,\n",
       "  0.25,\n",
       "  0.25,\n",
       "  0.25,\n",
       "  1.0,\n",
       "  0.25,\n",
       "  0.75,\n",
       "  0.25,\n",
       "  0.25,\n",
       "  0.25,\n",
       "  0.25,\n",
       "  0.25,\n",
       "  1.0,\n",
       "  0.25,\n",
       "  0.25,\n",
       "  0.25,\n",
       "  0.25,\n",
       "  0.75,\n",
       "  1.0,\n",
       "  0.75,\n",
       "  0.75,\n",
       "  0.25,\n",
       "  0.25],\n",
       " 'precision__w_special': [0.75,\n",
       "  1.0,\n",
       "  0.75,\n",
       "  0.25,\n",
       "  0.25,\n",
       "  0.25,\n",
       "  1.0,\n",
       "  0.25,\n",
       "  0.75,\n",
       "  0.25,\n",
       "  0.25,\n",
       "  0.25,\n",
       "  0.25,\n",
       "  0.25,\n",
       "  1.0,\n",
       "  0.25,\n",
       "  0.25,\n",
       "  0.25,\n",
       "  0.25,\n",
       "  0.75,\n",
       "  1.0,\n",
       "  0.75,\n",
       "  0.75,\n",
       "  0.25,\n",
       "  0.25],\n",
       " 'recall__w_special': [0.75,\n",
       "  1.0,\n",
       "  0.75,\n",
       "  0.25,\n",
       "  0.25,\n",
       "  0.25,\n",
       "  1.0,\n",
       "  0.25,\n",
       "  0.75,\n",
       "  0.25,\n",
       "  0.25,\n",
       "  0.25,\n",
       "  0.25,\n",
       "  0.25,\n",
       "  1.0,\n",
       "  0.25,\n",
       "  0.25,\n",
       "  0.25,\n",
       "  0.25,\n",
       "  0.75,\n",
       "  1.0,\n",
       "  0.75,\n",
       "  0.75,\n",
       "  0.25,\n",
       "  0.25],\n",
       " 'is_exact_match': [True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True]}"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exact_matches[:25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2ed75c03-45b7-4efd-9f62-39fb85e2ff67",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "246f18de770f41999e12d855e6a7be66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wrong_matches = individual_metrics_ds.filter(lambda x: x['is_exact_match'] == False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "71a7f91a-cc14-4715-9826-831a22f5725c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "4861"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# but most of these may or may not be largely because of matches on special tokens\n",
    "num = 0\n",
    "for m in wrong_matches:\n",
    "    if m[\"f1__w_special\"] >= 0.5:\n",
    "        num += 1\n",
    "num"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0a18aee4-6065-49ca-9029-fb35e89fff27",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "The poor performance of the model overall, notably its `0.0956` exact match score, coupled with the saturation of common special tokens makes the desired task of interpreting the partial correctness of predictions rather difficult. It may be wise, rather, to go ahead and train our model. From here, we can review its (most important) exact match metric for full correctness of predicitions. Subsequently, we may investigate the structure of partial correctness of those (hopefully) much improved predictions. At which time, we might learn something interesting in the trends of incorrect answers before ultimately returning here and applying a similar method of analysis on this messier, pretrained benchmarking."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "1"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": -1,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Benchmarking",
   "widgets": {
    "hf_token": {
     "currentValue": "",
     "nuid": "f185f53e-b1ab-4f94-80b0-530efaf95128",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": "hf_token",
      "name": "hf_token",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": "hf_token",
      "name": "hf_token",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}