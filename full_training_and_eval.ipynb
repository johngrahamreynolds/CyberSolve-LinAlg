{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3b2460a7-1b6a-46aa-b384-ba15c0168a80",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Full Training and Evaluation - DeepMind LingAlg 1D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "38b77755-d43b-46c7-835b-5ea5ce6ff93d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Now that we have conducted a preliminary downsampled training, we can improve our model by feeding it more tokens. For this smaller, initial fine-tuning, we split our dataset of 2M records down to only 100K records (5% of the original data). This showed us that the T5 model can indeed improve when fed the properly preprocessed data, but the results of such a downsampled training were still unsatisfactory. We hope to attain results similar to (or potentially better than) those found in the original [DeepMind Mathematics Dataset paper](https://arxiv.org/abs/1904.01557).\n",
    "\n",
    "We make use of the same architecture and optimization found in the downsampled training. We utilize Nvidia `Apex` for improved computation and memory utilization across the `flan-T5-large` model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e8c08bfa-16bd-423d-b3ea-77d8f7744d2b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /databricks/python3/lib/python3.11/site-packages (4.41.2)\nCollecting transformers\n  Obtaining dependency information for transformers from https://files.pythonhosted.org/packages/f2/3a/8bdab26e09c5a242182b7ba9152e216d5ab4ae2d78c4298eb4872549cd35/transformers-4.47.1-py3-none-any.whl.metadata\n  Downloading transformers-4.47.1-py3-none-any.whl.metadata (44 kB)\n\u001B[?25l     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/44.1 kB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\n\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m44.1/44.1 kB\u001B[0m \u001B[31m1.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hRequirement already satisfied: filelock in /databricks/python3/lib/python3.11/site-packages (from transformers) (3.13.4)\nCollecting huggingface-hub<1.0,>=0.24.0 (from transformers)\n  Obtaining dependency information for huggingface-hub<1.0,>=0.24.0 from https://files.pythonhosted.org/packages/6c/3f/50f6b25fafdcfb1c089187a328c95081abf882309afd86f4053951507cd1/huggingface_hub-0.27.1-py3-none-any.whl.metadata\n  Downloading huggingface_hub-0.27.1-py3-none-any.whl.metadata (13 kB)\nRequirement already satisfied: numpy>=1.17 in /databricks/python3/lib/python3.11/site-packages (from transformers) (1.23.5)\nRequirement already satisfied: packaging>=20.0 in /databricks/python3/lib/python3.11/site-packages (from transformers) (23.2)\nRequirement already satisfied: pyyaml>=5.1 in /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.11/site-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /databricks/python3/lib/python3.11/site-packages (from transformers) (2022.7.9)\nRequirement already satisfied: requests in /databricks/python3/lib/python3.11/site-packages (from transformers) (2.31.0)\nCollecting tokenizers<0.22,>=0.21 (from transformers)\n  Obtaining dependency information for tokenizers<0.22,>=0.21 from https://files.pythonhosted.org/packages/22/06/69d7ce374747edaf1695a4f61b83570d91cc8bbfc51ccfecf76f56ab4aac/tokenizers-0.21.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n  Downloading tokenizers-0.21.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\nRequirement already satisfied: safetensors>=0.4.1 in /databricks/python3/lib/python3.11/site-packages (from transformers) (0.4.2)\nRequirement already satisfied: tqdm>=4.27 in /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.11/site-packages (from transformers) (4.67.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /databricks/python3/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (2023.5.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /databricks/python3/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (4.10.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /databricks/python3/lib/python3.11/site-packages (from requests->transformers) (2.0.4)\nRequirement already satisfied: idna<4,>=2.5 in /databricks/python3/lib/python3.11/site-packages (from requests->transformers) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.11/site-packages (from requests->transformers) (2.0.7)\nRequirement already satisfied: certifi>=2017.4.17 in /databricks/python3/lib/python3.11/site-packages (from requests->transformers) (2023.7.22)\nDownloading transformers-4.47.1-py3-none-any.whl (10.1 MB)\n\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/10.1 MB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\n\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m4.8/10.1 MB\u001B[0m \u001B[31m144.8 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\n\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m \u001B[32m10.1/10.1 MB\u001B[0m \u001B[31m160.9 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\n\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m \u001B[32m10.1/10.1 MB\u001B[0m \u001B[31m160.9 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\n\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m10.1/10.1 MB\u001B[0m \u001B[31m81.2 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hDownloading huggingface_hub-0.27.1-py3-none-any.whl (450 kB)\n\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/450.7 kB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\n\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m450.7/450.7 kB\u001B[0m \u001B[31m36.9 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hDownloading tokenizers-0.21.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/3.0 MB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\n\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m \u001B[32m3.0/3.0 MB\u001B[0m \u001B[31m185.5 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\n\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m3.0/3.0 MB\u001B[0m \u001B[31m83.7 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hInstalling collected packages: huggingface-hub, tokenizers, transformers\n  Attempting uninstall: huggingface-hub\n    Found existing installation: huggingface-hub 0.23.4\n    Not uninstalling huggingface-hub at /databricks/python3/lib/python3.11/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-40b574f2-133d-4a3e-96ff-9b9d0d7041ed\n    Can't uninstall 'huggingface-hub'. No files were found to uninstall.\n  Attempting uninstall: tokenizers\n    Found existing installation: tokenizers 0.19.0\n    Not uninstalling tokenizers at /databricks/python3/lib/python3.11/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-40b574f2-133d-4a3e-96ff-9b9d0d7041ed\n    Can't uninstall 'tokenizers'. No files were found to uninstall.\n  Attempting uninstall: transformers\n    Found existing installation: transformers 4.41.2\n    Not uninstalling transformers at /databricks/python3/lib/python3.11/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-40b574f2-133d-4a3e-96ff-9b9d0d7041ed\n    Can't uninstall 'transformers'. No files were found to uninstall.\nSuccessfully installed huggingface-hub-0.27.1 tokenizers-0.21.0 transformers-4.47.1\n\u001B[43mNote: you may need to restart the kernel using %restart_python or dbutils.library.restartPython() to use updated packages.\u001B[0m\nProcessing /Volumes/workspace_dogfood/jgr/wheels/apex-0.1-cp311-cp311-linux_x86_64.whl\nRequirement already satisfied: packaging>20.6 in /databricks/python3/lib/python3.11/site-packages (from apex==0.1) (23.2)\nInstalling collected packages: apex\nSuccessfully installed apex-0.1\n\u001B[43mNote: you may need to restart the kernel using %restart_python or dbutils.library.restartPython() to use updated packages.\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "# install / upgrade transformers and install apex\n",
    "!pip install -U transformers\n",
    "!pip install /Volumes/workspace_dogfood/jgr/wheels/apex-0.1-cp311-cp311-linux_x86_64.whl\n",
    "\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bf75ee5b-9d7d-446a-a4a2-aa4f0ac2e92f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# confirm the apex library is available\n",
    "from apex import normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "825ce423-728f-4476-9f6b-c9ce04492a8f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/python_shell/dbruntime/huggingface_patches/datasets.py:14: UserWarning: During large dataset downloads, there could be multiple progress bar widgets that can cause performance issues for your notebook or browser. To avoid these issues, use `datasets.utils.logging.disable_progress_bar()` to turn off the progress bars.\n  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# load the full preprocessed training and evaluation datasets\n",
    "from datasets import load_dataset\n",
    "\n",
    "tokenized_train_dataset = load_dataset(\"MarioBarbeque/DeepMind-LinAlg-1D-train\")\n",
    "tokenized_eval_dataset = load_dataset(\"MarioBarbeque/DeepMind-LinAlg-1D-eval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c33a3acd-7f88-4314-892b-3d8e6c821188",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "tokenized_train_dataset = tokenized_train_dataset[\"train\"]\n",
    "tokenized_eval_dataset = tokenized_eval_dataset[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "92a611ff-6b7a-4827-bfa6-96dd2ed9890a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# set the downsampled format to numpy in order to pass it to the seq2seq datacollator\n",
    "# the DataCollatorForSeq2Seq uses numpy arrays to pad the labels\n",
    "tokenized_train_dataset.set_format(\"numpy\")\n",
    "tokenized_eval_dataset.set_format(\"numpy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9ac65ec0-4753-4349-85a4-aa44088d90fd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "({'input_ids': array([[ 5175,   162,   997,  3274,   898,  4542,  1935,    75,     3,\n",
       "             18,   898,  3076,  1935,    75,    21,     3,    75,     5,\n",
       "              1,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0],\n",
       "         [ 5175,   162,   431,  3436,  3274,     3,  4949,  1755,  1935,\n",
       "             17,  1768,   335,  3840,  1935,    17,  1768,  1630,  1458,\n",
       "            940,    21,     3,    17,     5,     1,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0],\n",
       "         [ 5175,   162,     3,  9169,  1935,    63,     3,    18,   204,\n",
       "           3891,  1935,    63,  1768,  2664,  4056,  3274,     3,    18,\n",
       "           4060,  1935,    63,    21,     3,    63,     5,     1,     0,\n",
       "              0,     0,     0,     0,     0,     0],\n",
       "         [ 5175,   162,     3,   632,  3274,     3,  9169,  1935,   115,\n",
       "              3,    18,   314, 24748,  1768,   314, 20489,    21,     3,\n",
       "            115,     5,     1,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0],\n",
       "         [ 5175,   162,  7123,  1935,    40,     3,    18,   220,  4241,\n",
       "           1768,   505,  4959,  3274,     3,   632,    21,     3,    40,\n",
       "              5,     1,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0]]),\n",
       "  'attention_mask': array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]),\n",
       "  'labels': array([[    3,  5783,     1,     0],\n",
       "         [    3, 14855,     1,     0],\n",
       "         [ 1003,     1,     0,     0],\n",
       "         [  489,     1,     0,     0],\n",
       "         [    3,  6039,     1,     0]])},\n",
       " {'input_ids': array([[ 5175,   162,     3,  4949,  4613,  1935,    26,  1768,   668,\n",
       "           3166,     3,    18,     3, 27640,  3274,     3,  5947,  2773,\n",
       "             21,     3,    26,     5,     1,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0],\n",
       "         [ 5175,   162,  9526,  1935,    40,  1768,  3479,  1935,    40,\n",
       "              3,    18,     3, 10124,     3,    18,     3,  3891,  3274,\n",
       "              3,   632,    21,     3,    40,     5,     1,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0],\n",
       "         [ 5175,   162,     3,    18,  4389,  1935,    17,  1768,  1179,\n",
       "           4225,     3,    18,   505,  3707,  1768,   668,  4201,  3274,\n",
       "              3,   632,    21,     3,    17,     5,     1,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0],\n",
       "         [ 5175,   162,  6374,  1935,   122,  3274,     3, 19978,  1935,\n",
       "            122,     3,    18,     3,  4450,  1935,   122,     3,    18,\n",
       "              3,  4440,  1935,   122,     3,    18,     3, 26755,    21,\n",
       "              3,   122,     5,     1,     0,     0,     0],\n",
       "         [ 5175,   162,     3,    18,  3710,  1935,   208,  1768,     3,\n",
       "          23188,  1935,   208,  1768,   305,  2773,  5553,  3274,   314,\n",
       "           3914,  4433,    21,     3,   208,     5,     1,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0]]),\n",
       "  'attention_mask': array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]]),\n",
       "  'labels': array([[  489,     1,     0,     0],\n",
       "         [  204,     1,     0,     0],\n",
       "         [ 1902,     1,     0,     0],\n",
       "         [    3,  6039,     1,     0],\n",
       "         [    3, 10794,     1,     0]])})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# peek some records\n",
    "tokenized_train_dataset[:5], tokenized_eval_dataset[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eb471e52-4b4d-45f6-8ddb-9fe500a4dfc1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-31 20:00:40.140896: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\nTo enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\nYou are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    }
   ],
   "source": [
    "# reinstantiate our tokenizer and model on the CPU - let the accelerator handle device placement\n",
    "import torch\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "checkpoint = \"google/flan-t5-large\"\n",
    "tokenizer = T5Tokenizer.from_pretrained(checkpoint)\n",
    "# load the model onto the CPU and let \uD83E\uDD17 accelerate take care of device placement in our training loop\n",
    "# model = T5ForConditionalGeneration.from_pretrained(checkpoint, torch_dtype=torch.bfloat16)\n",
    "model = T5ForConditionalGeneration.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e278fbec-1bb3-41d4-bbca-7d7e9fb36050",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/.ipykernel/3417/command-1152595035260058-2491816732:4: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library \uD83E\uDD17 Evaluate: https://huggingface.co/docs/evaluate\n  exact_match_metric = load_metric(\"exact_match\")\n/databricks/python/lib/python3.11/site-packages/datasets/load.py:759: FutureWarning: The repository for exact_match contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.19.1/metrics/exact_match/exact_match.py\nYou can avoid this message in future by passing the argument `trust_remote_code=True`.\nPassing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# grab our exact match metric\n",
    "from datasets import load_metric\n",
    "\n",
    "exact_match_metric = load_metric(\"exact_match\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "160bafdc-2ccc-4e9a-98f8-5d23321c6be9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# choose an intermediate batch size of 64 for our training loop\n",
    "# we saw best results at batch size 32, and overfitting at batch size 256\n",
    "# also bump the learning rate to 3e-4 from 1e-4 to see if we can get better results\n",
    "\n",
    "hyperparameters = {\n",
    "    \"learning_rate\": 3e-4, # see the T5 documentation on finetuning learning rate for AdamW\n",
    "    \"num_epochs\": 3,\n",
    "    \"train_batch_size\": 64, # Actual batch size will this x num gpus\n",
    "    \"eval_batch_size\": 256, # Actual batch size will this x num gpus\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d277a663-4c4c-4a25-9980-c00689ae827d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def mem_status_distributed():\n",
    "    rank = torch.cuda.current_device()\n",
    "    properties = torch.cuda.get_device_properties(rank)\n",
    "    total_memory = properties.total_memory / (1024 ** 3)  # Convert to GB\n",
    "    allocated_memory = torch.cuda.memory_allocated(rank) / (1024 ** 3)\n",
    "    reserved_memory = torch.cuda.memory_reserved(rank) / (1024 ** 3)\n",
    "    available_memory = total_memory - reserved_memory\n",
    "    print(f\"GPU {rank}: | \")\n",
    "    print(f\"Total memory: {total_memory:.2f} GB |\")\n",
    "    print(f\"Allocated memory: {allocated_memory:.2f} GB |\")\n",
    "    print(f\"Reserved memory: {reserved_memory:.2f} GB |\")\n",
    "    print(f\"Available memory: {available_memory:.2f} GB |\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3cb94f2d-9490-4c83-aa41-595f8a65bd33",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Initial 3 epoch checkpoint training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b4f4c225-82a7-463c-b07e-5b3af220bce5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rserv\r\nRtmppEQUgN\r\nchauffeur-daemon-params\r\nchauffeur-daemon.pid\r\nchauffeur-env.sh\r\ncustom-spark.conf\r\ndriver-daemon-params\r\ndriver-daemon.pid\r\ndriver-env.sh\r\nhsperfdata_root\r\nmachine_dir\r\npython_lsp_logs\r\nsystemd-private-260b01789a5045ea8496a775359e3934-systemd-logind.service-CtBx3v\r\nsystemd-private-260b01789a5045ea8496a775359e3934-systemd-resolved.service-d0WEGP\r\ntmp.yIJ4letUmp\r\ntmpfabskme2\r\n"
     ]
    }
   ],
   "source": [
    "# create a dir on the local machine to save our model after distributed training\n",
    "!mkdir /tmp/machine_dir\n",
    "!ls /tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8c818b7c-d75f-4796-845a-425ccd392bde",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def accelerated_training_function(model, tokenized_train_dataset, tokenized_eval_dataset, tokenzier, exact_match_metric, hyperparameters):\n",
    "    \n",
    "    from accelerate import Accelerator\n",
    "    from apex.optimizers import FusedAdam\n",
    "    import datasets\n",
    "    import torch\n",
    "    from torch.utils.data import DataLoader\n",
    "    from tqdm.notebook import tqdm\n",
    "    import transformers\n",
    "    from transformers import DataCollatorForSeq2Seq, get_scheduler\n",
    "\n",
    "    # initialize our accelerator as early as possible for configuring the distributed backend\n",
    "    accelerator = Accelerator()\n",
    "\n",
    "    # To have only one message (and not 2) per logs of Transformers or Datasets, we set the logging verbosity to INFO for the main process only.\n",
    "    if accelerator.is_main_process:\n",
    "        datasets.utils.logging.set_verbosity_warning()\n",
    "        transformers.utils.logging.set_verbosity_info()\n",
    "    else:\n",
    "        datasets.utils.logging.set_verbosity_error()\n",
    "        transformers.utils.logging.set_verbosity_error()\n",
    "\n",
    "    # on machine dir where we will save the model\n",
    "    tmp_dir = \"/tmp/machine_dir\"\n",
    "\n",
    "    # Collate our datasets\n",
    "    data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, label_pad_token_id=tokenizer.pad_token_id, pad_to_multiple_of=2)\n",
    "\n",
    "    train_dataloader = DataLoader(\n",
    "        tokenized_train_dataset,\n",
    "        shuffle=True, # add shuffling\n",
    "        batch_size=hyperparameters[\"train_batch_size\"],\n",
    "        collate_fn=data_collator\n",
    "    )\n",
    "    eval_dataloader = DataLoader(\n",
    "        tokenized_eval_dataset, \n",
    "        batch_size=hyperparameters[\"eval_batch_size\"], \n",
    "        collate_fn=data_collator\n",
    "    )\n",
    "\n",
    "    # use the apex optimized version of AdamW with a fused kernel\n",
    "    # NOTE T5 was pretrained with the AdaFactor optimizer - perhaps we should compare this optimizer in a separate training\n",
    "    optimizer = FusedAdam(model.parameters(), lr=hyperparameters[\"learning_rate\"], adam_w_mode=True)\n",
    "    # optimizer = AdamW(model.parameters(), lr=hyperparameters[\"learning_rate\"])\n",
    "\n",
    "    model.to(accelerator.device)\n",
    "\n",
    "    train_dataloader, eval_dataloader, model, optimizer = accelerator.prepare(train_dataloader, eval_dataloader, model, optimizer)\n",
    "\n",
    "    num_epochs = hyperparameters[\"num_epochs\"]\n",
    "    num_training_steps = num_epochs * len(train_dataloader)\n",
    "    lr_scheduler = get_scheduler(\n",
    "        \"linear\",\n",
    "        optimizer=optimizer,\n",
    "        num_warmup_steps=0,\n",
    "        num_training_steps=num_training_steps\n",
    "    )\n",
    "\n",
    "    mem_status_distributed()\n",
    "    for epoch in range(num_epochs):\n",
    "        # training\n",
    "        model.train()\n",
    "        for batch in tqdm(train_dataloader, desc=f\"Epoch {epoch}\", position=0, leave=True):\n",
    "            batch = {k: v.to(accelerator.device) for k, v in batch.items()}\n",
    "            outputs = model(**batch)\n",
    "            loss = outputs.loss\n",
    "            \n",
    "            accelerator.backward(loss)\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        # evaluation\n",
    "        model.eval()\n",
    "        for step, batch in enumerate(eval_dataloader):\n",
    "            with torch.no_grad():\n",
    "                outputs = model(**batch)\n",
    "            predictions = outputs.logits.argmax(dim=-1)\n",
    "\n",
    "            # We gather predictions and labels from the 2 GPUs to combine them all\n",
    "            gathered_predictions = accelerator.gather_for_metrics(predictions)\n",
    "            gathered_labels = accelerator.gather_for_metrics(batch[\"labels\"])\n",
    "\n",
    "            for pred, label in zip(gathered_predictions, gathered_labels):\n",
    "                exact_match_metric.add(predictions=tokenizer.decode(pred, skip_special_tokens=True), references=tokenizer.decode(label, skip_special_tokens=True))\n",
    "\n",
    "        mem_status_distributed() # show us the mem status of each GPU at the end of each epoch\n",
    "        metric = exact_match_metric.compute()\n",
    "        accelerator.print(f\"epoch {epoch}:\", metric)\n",
    "\n",
    "    # be sure to save our trained model to a given path\n",
    "    # first wait for all processes to reach the same stage\n",
    "    accelerator.wait_for_everyone()\n",
    "    unwrapped_model = accelerator.unwrap_model(model)\n",
    "    unwrapped_model.save_pretrained(tmp_dir, save_function=accelerator.save)\n",
    "    if accelerator.is_main_process:\n",
    "        tokenizer.save_pretrained(tmp_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ec99b0a7-f33d-4415-be3b-d62a510094dc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.databricks.v1+torch_distributed_hint": "",
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch.multiprocessing as mp\n",
    "\n",
    "# Set the multiprocessing start method to 'spawn'\n",
    "mp.set_start_method('spawn', force=True) # essential for spawning the multiprocessing properly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f788e052-1f3b-4b75-be75-068b047acd46",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching training on 2 GPUs.\nGPU 1: | \nTotal memory: 79.15 GB |\nGPU 0: | Allocated memory: 5.98 GB |\n\nReserved memory: 8.48 GB |Total memory: 79.15 GB |\n\nAvailable memory: 70.67 GB |Allocated memory: 5.98 GB |\n\nReserved memory: 8.48 GB |\nAvailable memory: 70.67 GB |\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dede8405c55e491992f723fc41bad094",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 0:   0%|          | 0/15625 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ed9540352824bada8c9ebd59638d63d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 0:   0%|          | 0/15625 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/local_disk0/.ephemeral_nfs/envs/pythonEnv-d33fcd92-7883-468f-b6a0-0afc3e40709b/lib/python3.11/site-packages/transformers/data/data_collator.py:657: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:274.)\n  batch[\"labels\"] = torch.tensor(batch[\"labels\"], dtype=torch.int64)\n/local_disk0/.ephemeral_nfs/envs/pythonEnv-d33fcd92-7883-468f-b6a0-0afc3e40709b/lib/python3.11/site-packages/transformers/data/data_collator.py:657: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:274.)\n  batch[\"labels\"] = torch.tensor(batch[\"labels\"], dtype=torch.int64)\nPassing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU 1: | GPU 0: | \n\nTotal memory: 79.15 GB |Total memory: 79.15 GB |\n\nAllocated memory: 13.89 GB |Allocated memory: 13.90 GB |\n\nReserved memory: 28.15 GB |Reserved memory: 29.94 GB |\n\nAvailable memory: 51.00 GB |Available memory: 49.21 GB |\n\nepoch 0: {'exact_match': 55.35}\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67e483ef0e094929972ee4ea41ae1d81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1:   0%|          | 0/15625 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6a4411c0e5e4ccdbd0c62ca949c432e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1:   0%|          | 0/15625 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:No handler for dede8405c55e491992f723fc41bad094\nWARNING:root:No handler for 5ed9540352824bada8c9ebd59638d63d\nWARNING:root:No handler for 67e483ef0e094929972ee4ea41ae1d81\nWARNING:root:No handler for b6a4411c0e5e4ccdbd0c62ca949c432e\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU 0: | \nTotal memory: 79.15 GB |GPU 1: | \n\nAllocated memory: 13.90 GB |Total memory: 79.15 GB |\n\nReserved memory: 29.94 GB |Allocated memory: 13.89 GB |\n\nAvailable memory: 49.21 GB |Reserved memory: 28.15 GB |\n\nAvailable memory: 51.00 GB |\nepoch 1: {'exact_match': 73.8}\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "409eff46998a4e0a9055b17f92e4a45e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2:   0%|          | 0/15625 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e5d05e88c5d47328e5e301abdc83ea2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2:   0%|          | 0/15625 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU 0: | GPU 1: | \n\nTotal memory: 79.15 GB |Total memory: 79.15 GB |\n\nAllocated memory: 13.90 GB |Allocated memory: 13.89 GB |\n\nReserved memory: 29.94 GB |Reserved memory: 28.15 GB |\n\nAvailable memory: 49.21 GB |Available memory: 51.00 GB |\n\nepoch 2: {'exact_match': 86.56}\n[2025-01-01 02:30:00,050] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n[2025-01-01 02:30:00,059] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "df: /root/.triton/autotune: No such file or directory\ndf: /root/.triton/autotune: No such file or directory\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[93m [WARNING] \u001B[0m async_io requires the dev libaio .so object and headers but these were not found.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/bin/ld: cannot find -laio: No such file or directory\ncollect2: error: ld returned 1 exit status\n/usr/bin/ld: cannot find -laio: No such file or directory\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[93m [WARNING] \u001B[0m async_io requires the dev libaio .so object and headers but these were not found.\n\u001B[93m [WARNING] \u001B[0m async_io: please install the libaio-dev package with apt\n\u001B[93m [WARNING] \u001B[0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.\n\u001B[93m [WARNING] \u001B[0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH\n\u001B[93m [WARNING] \u001B[0m async_io: please install the libaio-dev package with apt\n\u001B[93m [WARNING] \u001B[0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.\n\u001B[93m [WARNING] \u001B[0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "collect2: error: ld returned 1 exit status\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[93m [WARNING] \u001B[0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3\n\u001B[93m [WARNING] \u001B[0m using untested triton version (2.3.1), only 1.0.0 is known to be compatible\n\u001B[93m [WARNING] \u001B[0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3\n\u001B[93m [WARNING] \u001B[0m using untested triton version (2.3.1), only 1.0.0 is known to be compatible\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in /tmp/machine_dir/config.json\nConfiguration saved in /tmp/machine_dir/generation_config.json\nModel weights saved in /tmp/machine_dir/model.safetensors\ntokenizer config file saved in /tmp/machine_dir/tokenizer_config.json\nSpecial tokens file saved in /tmp/machine_dir/special_tokens_map.json\nadded tokens file saved in /tmp/machine_dir/added_tokens.json\nW0101 02:30:30.890990 140480460886016 torch/distributed/elastic/multiprocessing/api.py:727] Closing process 3806 via signal SIGTERM\n"
     ]
    }
   ],
   "source": [
    "from accelerate import notebook_launcher\n",
    "\n",
    "notebook_launcher(accelerated_training_function, (model, tokenized_train_dataset, tokenized_eval_dataset, tokenizer, exact_match_metric, hyperparameters), num_processes=2, mixed_precision=\"bf16\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ecbd5a57-52c4-4ae7-9641-3ccf37599235",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "added_tokens.json\tmodel.safetensors\t tokenizer_config.json\r\nconfig.json\t\tspecial_tokens_map.json\r\ngeneration_config.json\tspiece.model\r\n"
     ]
    }
   ],
   "source": [
    "# peek the saved files on the local machine\n",
    "!ls /tmp/machine_dir/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9944aa4a-8b0a-4750-91c1-471a41eb870d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Copy the contents of the temp directory to the permanent volume\n",
    "dbutils.fs.cp(\"file:/tmp/machine_dir/\", \"dbfs:/Volumes/workspace_dogfood/jgr/hugging_face_cache/CyberSolve-DeepMind-LinAlg-1D\", recurse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f674a4b9-5dcf-4735-8337-8049f6273044",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Further fine-tuning our model checkpoint\n",
    "\n",
    "We've reached an `exact_match` score of 86.6%, lets see if we can bump this past 95% with added epochs. We will train an additional 2 epochs from our previous checkpoint that hit the 86.6% benchmark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c9daa234-5a2b-48f3-9e76-4608c81f78cd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-01 16:35:09.974783: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\nTo enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# reinstantiate our tokenizer and model on the CPU - let the accelerator handle device placement\n",
    "import torch\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "checkpoint = \"/Volumes/workspace_dogfood/jgr/hugging_face_cache/CyberSolve-DeepMind-LinAlg-1D\"\n",
    "tokenizer = T5Tokenizer.from_pretrained(checkpoint)\n",
    "# load the model onto the CPU and let \uD83E\uDD17 accelerate take care of device placement in our training loop\n",
    "# model = T5ForConditionalGeneration.from_pretrained(checkpoint, torch_dtype=torch.bfloat16)\n",
    "checkpoint_model = T5ForConditionalGeneration.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1a3e9655-7c1f-44e0-9d70-f0eb47c2f181",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# update our hyperparameters to train for an additional 2 epochs with a slightly lower initial learning rate\n",
    "\n",
    "hyperparameters = {\n",
    "    \"learning_rate\": 1e-4, # see the T5 documentation on finetuning learning rate for AdamW\n",
    "    \"num_epochs\": 2,\n",
    "    \"train_batch_size\": 64, # Actual batch size will this x num gpus\n",
    "    \"eval_batch_size\": 256, # Actual batch size will this x num gpus\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ffd606c1-5561-40e4-9fa8-3504873109e1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rserv\r\nRtmptL6LDu\r\nchauffeur-daemon-params\r\nchauffeur-daemon.pid\r\nchauffeur-env.sh\r\ncustom-spark.conf\r\ndriver-daemon-params\r\ndriver-daemon.pid\r\ndriver-env.sh\r\nhsperfdata_root\r\nmachine_dir_1\r\nmachine_dir_2\r\npython_lsp_logs\r\nsystemd-private-41be431891994dfb8161431ee8b41975-systemd-logind.service-1BQg7x\r\nsystemd-private-41be431891994dfb8161431ee8b41975-systemd-resolved.service-fm378o\r\ntmp.XNysxuc1Bg\r\ntmp5zp6ec06\r\ntmppi6h52cn\r\n"
     ]
    }
   ],
   "source": [
    "# recreate a dir on the local machine to save our model since were spinning up a new compute instance\n",
    "!mkdir /tmp/machine_dir_1\n",
    "!mkdir /tmp/machine_dir_2\n",
    "!ls /tmp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "87d49968-a1b2-43af-8ee1-4c9434eaca7c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Update our training loop to write out the weights at the end of each epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6112ece9-1512-4295-b098-b79be957994e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def second_accelerated_training_function(model, tokenized_train_dataset, tokenized_eval_dataset, tokenzier, exact_match_metric, hyperparameters):\n",
    "    \n",
    "    from accelerate import Accelerator\n",
    "    from apex.optimizers import FusedAdam\n",
    "    import datasets\n",
    "    import torch\n",
    "    from torch.utils.data import DataLoader\n",
    "    from tqdm.notebook import tqdm\n",
    "    import transformers\n",
    "    from transformers import DataCollatorForSeq2Seq, get_scheduler\n",
    "\n",
    "    # initialize our accelerator as early as possible for configuring the distributed backend\n",
    "    accelerator = Accelerator()\n",
    "\n",
    "    # To have only one message (and not 2) per logs of Transformers or Datasets, we set the logging verbosity to INFO for the main process only.\n",
    "    if accelerator.is_main_process:\n",
    "        datasets.utils.logging.set_verbosity_warning()\n",
    "        transformers.utils.logging.set_verbosity_info()\n",
    "    else:\n",
    "        datasets.utils.logging.set_verbosity_error()\n",
    "        transformers.utils.logging.set_verbosity_error()\n",
    "\n",
    "    # on machine dir where we will save the model\n",
    "    tmp_dir_1 = \"/tmp/machine_dir_1\"\n",
    "    tmp_dir_2 = \"/tmp/machine_dir_2\"\n",
    "\n",
    "    # Collate our datasets\n",
    "    data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, label_pad_token_id=tokenizer.pad_token_id, pad_to_multiple_of=2)\n",
    "\n",
    "    train_dataloader = DataLoader(\n",
    "        tokenized_train_dataset,\n",
    "        shuffle=True, # add shuffling\n",
    "        batch_size=hyperparameters[\"train_batch_size\"],\n",
    "        collate_fn=data_collator\n",
    "    )\n",
    "    eval_dataloader = DataLoader(\n",
    "        tokenized_eval_dataset, \n",
    "        batch_size=hyperparameters[\"eval_batch_size\"], \n",
    "        collate_fn=data_collator\n",
    "    )\n",
    "\n",
    "    # use the apex optimized version of AdamW with a fused kernel\n",
    "    # NOTE T5 was pretrained with the AdaFactor optimizer - perhaps we should compare this optimizer in a separate training\n",
    "    optimizer = FusedAdam(model.parameters(), lr=hyperparameters[\"learning_rate\"], adam_w_mode=True)\n",
    "\n",
    "    model.to(accelerator.device)\n",
    "\n",
    "    train_dataloader, eval_dataloader, model, optimizer = accelerator.prepare(train_dataloader, eval_dataloader, model, optimizer)\n",
    "\n",
    "    num_epochs = hyperparameters[\"num_epochs\"]\n",
    "    num_training_steps = num_epochs * len(train_dataloader)\n",
    "    lr_scheduler = get_scheduler(\n",
    "        \"linear\",\n",
    "        optimizer=optimizer,\n",
    "        num_warmup_steps=0,\n",
    "        num_training_steps=num_training_steps\n",
    "    )\n",
    "\n",
    "    mem_status_distributed()\n",
    "    for epoch in range(num_epochs):\n",
    "        # training\n",
    "        model.train()\n",
    "        for batch in tqdm(train_dataloader, desc=f\"Epoch {epoch}\", position=0, leave=True):\n",
    "            batch = {k: v.to(accelerator.device) for k, v in batch.items()}\n",
    "            outputs = model(**batch)\n",
    "            loss = outputs.loss\n",
    "            \n",
    "            accelerator.backward(loss)\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        # evaluation\n",
    "        model.eval()\n",
    "        for step, batch in enumerate(eval_dataloader):\n",
    "            with torch.no_grad():\n",
    "                outputs = model(**batch)\n",
    "            predictions = outputs.logits.argmax(dim=-1)\n",
    "\n",
    "            # We gather predictions and labels from the 2 GPUs to combine them all\n",
    "            gathered_predictions = accelerator.gather_for_metrics(predictions)\n",
    "            gathered_labels = accelerator.gather_for_metrics(batch[\"labels\"])\n",
    "\n",
    "            for pred, label in zip(gathered_predictions, gathered_labels):\n",
    "                exact_match_metric.add(predictions=tokenizer.decode(pred, skip_special_tokens=True), references=tokenizer.decode(label, skip_special_tokens=True))\n",
    "\n",
    "        mem_status_distributed() # show us the mem status of each GPU at the end of each epoch\n",
    "        metric = exact_match_metric.compute()\n",
    "        accelerator.print(f\"epoch {epoch}:\", metric)\n",
    "\n",
    "        # be sure to save our trained model to a given path\n",
    "        # first wait for all processes to reach the same stage\n",
    "        accelerator.wait_for_everyone()\n",
    "        unwrapped_model = accelerator.unwrap_model(model)\n",
    "        if epoch == 0:\n",
    "            output_dir = tmp_dir_1\n",
    "        else:\n",
    "            output_dir = tmp_dir_2\n",
    "        unwrapped_model.save_pretrained(output_dir, save_function=accelerator.save)\n",
    "        if accelerator.is_main_process:\n",
    "            tokenizer.save_pretrained(output_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "387b9c4a-b6b5-4d1f-b7d8-034db146993b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.databricks.v1+torch_distributed_hint": "",
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch.multiprocessing as mp\n",
    "\n",
    "# Set the multiprocessing start method to 'spawn'\n",
    "mp.set_start_method('spawn', force=True) # essential for spawning the multiprocessing properly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e4859e34-4a6c-4cc8-a264-6b6874d89c5d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching training on 2 GPUs.\nGPU 1: | \nTotal memory: 79.15 GB |\nAllocated memory: 5.98 GB |\nReserved memory: 8.48 GB |\nAvailable memory: 70.67 GB |\nGPU 0: | \nTotal memory: 79.15 GB |\nAllocated memory: 5.98 GB |\nReserved memory: 8.48 GB |\nAvailable memory: 70.67 GB |\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd6dad4988f74dd9bf6ba70de295771f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 0:   0%|          | 0/15625 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc0e67f519174151ab427a08ed173841",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 0:   0%|          | 0/15625 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/local_disk0/.ephemeral_nfs/envs/pythonEnv-b53c864d-c586-45b6-af85-3284079226b3/lib/python3.11/site-packages/transformers/data/data_collator.py:657: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:274.)\n  batch[\"labels\"] = torch.tensor(batch[\"labels\"], dtype=torch.int64)\n/local_disk0/.ephemeral_nfs/envs/pythonEnv-b53c864d-c586-45b6-af85-3284079226b3/lib/python3.11/site-packages/transformers/data/data_collator.py:657: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:274.)\n  batch[\"labels\"] = torch.tensor(batch[\"labels\"], dtype=torch.int64)\nPassing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\nWARNING:root:No handler for bd6dad4988f74dd9bf6ba70de295771f\nWARNING:root:No handler for cc0e67f519174151ab427a08ed173841\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU 0: | GPU 1: | \n\nTotal memory: 79.15 GB |Total memory: 79.15 GB |\n\nAllocated memory: 13.90 GB |Allocated memory: 13.89 GB |\n\nReserved memory: 29.94 GB |Reserved memory: 28.15 GB |\n\nAvailable memory: 49.21 GB |Available memory: 51.00 GB |\n\nepoch 0: {'exact_match': 83.12}\n[2025-01-01 18:46:45,823] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n[2025-01-01 18:46:45,826] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n\u001B[93m [WARNING] \u001B[0m async_io requires the dev libaio .so object and headers but these were not found.\u001B[93m [WARNING] \u001B[0m async_io requires the dev libaio .so object and headers but these were not found.\n\n\u001B[93m [WARNING] \u001B[0m async_io: please install the libaio-dev package with apt\n\u001B[93m [WARNING] \u001B[0m async_io: please install the libaio-dev package with apt\u001B[93m [WARNING] \u001B[0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.\n\n\u001B[93m [WARNING] \u001B[0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.\u001B[93m [WARNING] \u001B[0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH\n\n\u001B[93m [WARNING] \u001B[0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "df: /root/.triton/autotune: No such file or directory\ndf: /root/.triton/autotune: No such file or directory\n/usr/bin/ld: cannot find -laio: No such file or directory\n/usr/bin/ld: cannot find -laio: No such file or directory\ncollect2: error: ld returned 1 exit status\ncollect2: error: ld returned 1 exit status\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[93m [WARNING] \u001B[0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3\n\u001B[93m [WARNING] \u001B[0m using untested triton version (2.3.1), only 1.0.0 is known to be compatible\n\u001B[93m [WARNING] \u001B[0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3\n\u001B[93m [WARNING] \u001B[0m using untested triton version (2.3.1), only 1.0.0 is known to be compatible\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in /tmp/machine_dir_1/config.json\nConfiguration saved in /tmp/machine_dir_1/generation_config.json\nModel weights saved in /tmp/machine_dir_1/model.safetensors\ntokenizer config file saved in /tmp/machine_dir_1/tokenizer_config.json\nSpecial tokens file saved in /tmp/machine_dir_1/special_tokens_map.json\nadded tokens file saved in /tmp/machine_dir_1/added_tokens.json\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4a24e3735b4437a9084c82197439ef6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1:   0%|          | 0/15625 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bad9eca6f97049ce97cc792c6f1a2dd6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1:   0%|          | 0/15625 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:No handler for a4a24e3735b4437a9084c82197439ef6\nWARNING:root:No handler for bad9eca6f97049ce97cc792c6f1a2dd6\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU 1: | \nGPU 0: | Total memory: 79.15 GB |\n\nTotal memory: 79.15 GB |Allocated memory: 13.89 GB |\n\nAllocated memory: 13.90 GB |Reserved memory: 28.15 GB |\n\nReserved memory: 29.94 GB |Available memory: 51.00 GB |\n\nAvailable memory: 49.21 GB |\nepoch 1: {'exact_match': 90.75}\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in /tmp/machine_dir_2/config.json\nConfiguration saved in /tmp/machine_dir_2/generation_config.json\nModel weights saved in /tmp/machine_dir_2/model.safetensors\ntokenizer config file saved in /tmp/machine_dir_2/tokenizer_config.json\nSpecial tokens file saved in /tmp/machine_dir_2/special_tokens_map.json\nadded tokens file saved in /tmp/machine_dir_2/added_tokens.json\nW0101 20:53:46.322427 140278112034816 torch/distributed/elastic/multiprocessing/api.py:727] Closing process 4869 via signal SIGTERM\n"
     ]
    }
   ],
   "source": [
    "from accelerate import notebook_launcher\n",
    "\n",
    "notebook_launcher(second_accelerated_training_function, (checkpoint_model, tokenized_train_dataset, tokenized_eval_dataset, tokenizer, exact_match_metric, hyperparameters), num_processes=2, mixed_precision=\"bf16\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9ba12926-4db8-4918-aa84-60d735508378",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Copy the contents of the temp directories to the permanent volumes\n",
    "dbutils.fs.cp(\"file:/tmp/machine_dir_1/\", \"dbfs:/Volumes/workspace_dogfood/jgr/hugging_face_cache/CyberSolve-DeepMind-LinAlg-1D-v2\", recurse=True)\n",
    "dbutils.fs.cp(\"file:/tmp/machine_dir_2/\", \"dbfs:/Volumes/workspace_dogfood/jgr/hugging_face_cache/CyberSolve-DeepMind-LinAlg-1D-v3\", recurse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "078f5c3d-df13-4172-a775-28d393e20117",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Peek our trained models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bc5996b2-f108-4d31-a8e6-9765eeffd741",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-09 21:35:17.039721: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\nTo enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# make sure we can load our models from the saved location\n",
    "from transformers import T5ForConditionalGeneration\n",
    "\n",
    "trained_model_1 = T5ForConditionalGeneration.from_pretrained(\"/Volumes/workspace_dogfood/jgr/hugging_face_cache/CyberSolve-DeepMind-LinAlg-1D\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3841ed90-6f27-4bad-99a5-33567960f105",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "trained_model_2 = T5ForConditionalGeneration.from_pretrained(\"/Volumes/workspace_dogfood/jgr/hugging_face_cache/CyberSolve-DeepMind-LinAlg-1D-v3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ccd1484c-cf01-487b-91b9-3a62c1c12d92",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from transformers import T5Tokenizer\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"/Volumes/workspace_dogfood/jgr/hugging_face_cache/CyberSolve-DeepMind-LinAlg-1D-v3\") # tokenizer is the same for both checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ddf1abfa-9b60-41c2-8d7f-d4e72eadccb9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-6\n"
     ]
    }
   ],
   "source": [
    "# test the inference on CPU\n",
    "\n",
    "input_text = \"Solve 24 = 1601*c - 1605*c for c.\"\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids\n",
    "\n",
    "outputs = trained_model_2.generate(input_ids)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "90d1e140-b958-430f-a1e4-67141d561120",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-6\n"
     ]
    }
   ],
   "source": [
    "trained_model_2.to(\"cuda\")\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "\n",
    "outputs = trained_model_2.generate(input_ids)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "73f1a711-5405-4a76-bb5d-d709f6903944",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# lets push the fully finetuned model to the hub for saving\n",
    "dbutils.widgets.text(\"hf_token\", \"\", \"hf_token\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "95fd6da5-9308-4929-a10b-d9927ddc1aae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\r\nToken is valid (permission: fineGrained).\r\nThe token `Personal Hub Token` has been saved to /Volumes/workspace_dogfood/jgr/hugging_face_cache/stored_tokens\r\nYour token has been saved to /Volumes/workspace_dogfood/jgr/hugging_face_cache/token\r\nLogin successful.\r\nThe current active token is: `Personal Hub Token`\r\n"
     ]
    }
   ],
   "source": [
    "hf_token = dbutils.widgets.get(\"hf_token\")\n",
    "!huggingface-cli login --token $hf_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "17354a9d-a61d-41f4-ac78-11fc1b696d4b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-01-09 20:46:33,648] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "df: /root/.triton/autotune: No such file or directory\n/usr/bin/ld: cannot find -laio: No such file or directory\ncollect2: error: ld returned 1 exit status\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[93m [WARNING] \u001B[0m async_io requires the dev libaio .so object and headers but these were not found.\n\u001B[93m [WARNING] \u001B[0m async_io: please install the libaio-dev package with apt\n\u001B[93m [WARNING] \u001B[0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.\n\u001B[93m [WARNING] \u001B[0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH\n\u001B[93m [WARNING] \u001B[0m NVIDIA Inference is only supported on Ampere and newer architectures\n\u001B[93m [WARNING] \u001B[0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3\n\u001B[93m [WARNING] \u001B[0m using untested triton version (2.3.1), only 1.0.0 is known to be compatible\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d073498246b4a86a6d580d20456f472",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/3.13G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/MarioBarbeque/CyberSolve-LinAlg-1.1/commit/71a7616de8128a0665f022616c5e965bf72b4ec4', commit_message='We introduce CyberSolve, the flan-t5-large model fintuned on all 2M records of the DeepMind LinAlg 1D dataset. This is the first model checkpoint, scoring 86.56 on the eval dataset', commit_description='', oid='71a7616de8128a0665f022616c5e965bf72b4ec4', pr_url=None, repo_url=RepoUrl('https://huggingface.co/MarioBarbeque/CyberSolve-LinAlg-1.1', endpoint='https://huggingface.co', repo_type='model', repo_id='MarioBarbeque/CyberSolve-LinAlg-1.1'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trained_model_1.push_to_hub(\"CyberSolve-LinAlg-1.1\", commit_message=\"We introduce CyberSolve, the flan-t5-large model fintuned on all 2M records of the DeepMind LinAlg 1D dataset. This is the first model checkpoint, scoring 86.56 on the eval dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "21d3dd3b-7ab6-4b16-aef0-f77cb25e15ea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1da3834e5ab458f91cfc2130fd2cdc6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/3.13G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/MarioBarbeque/CyberSolve-LinAlg-1.2/commit/c1d20ad152b8a8f634c2862359d58fe11bc14076', commit_message='Second CyberSolve model checkpoint; scoring 90.75 on the eval dataset', commit_description='', oid='c1d20ad152b8a8f634c2862359d58fe11bc14076', pr_url=None, repo_url=RepoUrl('https://huggingface.co/MarioBarbeque/CyberSolve-LinAlg-1.2', endpoint='https://huggingface.co', repo_type='model', repo_id='MarioBarbeque/CyberSolve-LinAlg-1.2'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trained_model_2.push_to_hub(\"CyberSolve-LinAlg-1.2\", commit_message=\"Second CyberSolve model checkpoint; scoring 90.75 on the eval dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "42a86229-bd81-4580-b714-eea37d166bdd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Partial Correctness Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d69eb3f9-2582-440e-aa22-8be23359bbdd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "T5ForConditionalGeneration(\n",
       "  (shared): Embedding(32128, 1024)\n",
       "  (encoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32128, 1024)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 16)\n",
       "            )\n",
       "            (layer_norm): FusedRMSNorm(torch.Size([1024]), eps=1e-06, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=1024, out_features=2816, bias=False)\n",
       "              (wi_1): Linear(in_features=1024, out_features=2816, bias=False)\n",
       "              (wo): Linear(in_features=2816, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): FusedRMSNorm(torch.Size([1024]), eps=1e-06, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-23): 23 x T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): FusedRMSNorm(torch.Size([1024]), eps=1e-06, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=1024, out_features=2816, bias=False)\n",
       "              (wi_1): Linear(in_features=1024, out_features=2816, bias=False)\n",
       "              (wo): Linear(in_features=2816, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): FusedRMSNorm(torch.Size([1024]), eps=1e-06, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): FusedRMSNorm(torch.Size([1024]), eps=1e-06, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (decoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32128, 1024)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 16)\n",
       "            )\n",
       "            (layer_norm): FusedRMSNorm(torch.Size([1024]), eps=1e-06, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): FusedRMSNorm(torch.Size([1024]), eps=1e-06, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=1024, out_features=2816, bias=False)\n",
       "              (wi_1): Linear(in_features=1024, out_features=2816, bias=False)\n",
       "              (wo): Linear(in_features=2816, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): FusedRMSNorm(torch.Size([1024]), eps=1e-06, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-23): 23 x T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): FusedRMSNorm(torch.Size([1024]), eps=1e-06, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): FusedRMSNorm(torch.Size([1024]), eps=1e-06, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=1024, out_features=2816, bias=False)\n",
       "              (wi_1): Linear(in_features=1024, out_features=2816, bias=False)\n",
       "              (wo): Linear(in_features=2816, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): FusedRMSNorm(torch.Size([1024]), eps=1e-06, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): FusedRMSNorm(torch.Size([1024]), eps=1e-06, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1024, out_features=32128, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# put the models on the GPU so that we can create the partial correctness datasets in the same loop\n",
    "trained_model_1.to(\"cuda\")\n",
    "\n",
    "# *** interestingly, this also confirms that we have the FusedRMSNorm layer in place of the T5Norm layer *** use this above to show this directly on our model before training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d2bbe384-1a04-4e66-b5ef-b56d19b68d99",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "T5ForConditionalGeneration(\n",
       "  (shared): Embedding(32128, 1024)\n",
       "  (encoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32128, 1024)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 16)\n",
       "            )\n",
       "            (layer_norm): FusedRMSNorm(torch.Size([1024]), eps=1e-06, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=1024, out_features=2816, bias=False)\n",
       "              (wi_1): Linear(in_features=1024, out_features=2816, bias=False)\n",
       "              (wo): Linear(in_features=2816, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): FusedRMSNorm(torch.Size([1024]), eps=1e-06, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-23): 23 x T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): FusedRMSNorm(torch.Size([1024]), eps=1e-06, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=1024, out_features=2816, bias=False)\n",
       "              (wi_1): Linear(in_features=1024, out_features=2816, bias=False)\n",
       "              (wo): Linear(in_features=2816, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): FusedRMSNorm(torch.Size([1024]), eps=1e-06, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): FusedRMSNorm(torch.Size([1024]), eps=1e-06, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (decoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32128, 1024)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 16)\n",
       "            )\n",
       "            (layer_norm): FusedRMSNorm(torch.Size([1024]), eps=1e-06, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): FusedRMSNorm(torch.Size([1024]), eps=1e-06, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=1024, out_features=2816, bias=False)\n",
       "              (wi_1): Linear(in_features=1024, out_features=2816, bias=False)\n",
       "              (wo): Linear(in_features=2816, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): FusedRMSNorm(torch.Size([1024]), eps=1e-06, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-23): 23 x T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): FusedRMSNorm(torch.Size([1024]), eps=1e-06, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): FusedRMSNorm(torch.Size([1024]), eps=1e-06, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=1024, out_features=2816, bias=False)\n",
       "              (wi_1): Linear(in_features=1024, out_features=2816, bias=False)\n",
       "              (wo): Linear(in_features=2816, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): FusedRMSNorm(torch.Size([1024]), eps=1e-06, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): FusedRMSNorm(torch.Size([1024]), eps=1e-06, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1024, out_features=32128, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trained_model_2.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c2cc642d-d66f-4e86-b7af-a5eeb3a1a214",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# construct the partial correctness dataset in a separate evaluation loop so as to not interfere with the larger, more fragile, more expensive full training\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import DataCollatorForSeq2Seq\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, label_pad_token_id=tokenizer.pad_token_id, pad_to_multiple_of=2)\n",
    "\n",
    "eval_dataloader = DataLoader(\n",
    "    tokenized_eval_dataset, \n",
    "    batch_size=64, # multiple of 8 for tensor cores\n",
    "    collate_fn=data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "13e6c4ee-87d6-40a8-bd85-f2ff8398d36d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6d1588282b14d7d94690dabcc155c46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/157 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/.ipykernel/2070/command-1875847745248601-3560740654:8: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library \uD83E\uDD17 Evaluate: https://huggingface.co/docs/evaluate\n  exact_match_1 = load_metric(\"exact_match\")\n/databricks/python/lib/python3.11/site-packages/datasets/load.py:759: FutureWarning: The repository for exact_match contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.19.1/metrics/exact_match/exact_match.py\nYou can avoid this message in future by passing the argument `trust_remote_code=True`.\nPassing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n  warnings.warn(\n/databricks/python/lib/python3.11/site-packages/datasets/load.py:759: FutureWarning: The repository for exact_match contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.19.1/metrics/exact_match/exact_match.py\nYou can avoid this message in future by passing the argument `trust_remote_code=True`.\nPassing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n  warnings.warn(\nPassing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'exact_match': 86.56}\n{'exact_match': 90.75}\n"
     ]
    }
   ],
   "source": [
    "# now we construct the partial correctness datasets by doing inference with each of our checkpoints\n",
    "from tqdm.auto import tqdm\n",
    "import torch\n",
    "from datasets import Dataset, load_metric\n",
    "\n",
    "progress_bar = tqdm(range(len(eval_dataloader)))\n",
    "# compute the exact match of each dataset again - we expect 86.56 and 90.75, respectively\n",
    "exact_match_1 = load_metric(\"exact_match\")\n",
    "exact_match_2 = load_metric(\"exact_match\")\n",
    "\n",
    "# create empty dicts to populate with the results for partial correctness evaluation\n",
    "partials_1 = {\"predicted_tokens\": [], \"label_tokens\": [], \"decoded_prediction\": [], \"decoded_label\": []}\n",
    "partials_2 = {\"predicted_tokens\": [], \"label_tokens\": [], \"decoded_prediction\": [], \"decoded_label\": []}\n",
    "\n",
    "trained_model_1.eval()\n",
    "trained_model_2.eval()\n",
    "for batch in eval_dataloader:\n",
    "    batch = {k: v.to(\"cuda\") for k, v in batch.items()}\n",
    "    with torch.no_grad():\n",
    "        outputs_1 = trained_model_1(**batch)\n",
    "        outputs_2 = trained_model_2(**batch)\n",
    "\n",
    "    for pred, label in zip(outputs_1.logits.argmax(dim=-1), batch[\"labels\"]):\n",
    "        # compute the exact match from this checkpoint's predictions\n",
    "        exact_match_1.add(predictions=tokenizer.decode(pred, skip_special_tokens=True), references=tokenizer.decode(label, skip_special_tokens=True))\n",
    "        # populate the partial correctness dict for detailed, individual eval\n",
    "        partials_1[\"predicted_tokens\"].append(pred)\n",
    "        partials_1[\"label_tokens\"].append(label)\n",
    "        partials_1[\"decoded_prediction\"].append(tokenizer.decode(pred, skip_special_tokens=True))\n",
    "        partials_1[\"decoded_label\"].append(tokenizer.decode(label, skip_special_tokens=True))\n",
    "    \n",
    "    for pred, label in zip(outputs_2.logits.argmax(dim=-1), batch[\"labels\"]):\n",
    "        # compute the exact match from this checkpoint's predictions\n",
    "        exact_match_2.add(predictions=tokenizer.decode(pred, skip_special_tokens=True), references=tokenizer.decode(label, skip_special_tokens=True))\n",
    "        # populate the partial correctness dict for detailed, individual eval\n",
    "        partials_2[\"predicted_tokens\"].append(pred)\n",
    "        partials_2[\"label_tokens\"].append(label)\n",
    "        partials_2[\"decoded_prediction\"].append(tokenizer.decode(pred, skip_special_tokens=True))\n",
    "        partials_2[\"decoded_label\"].append(tokenizer.decode(label, skip_special_tokens=True))\n",
    "\n",
    "    progress_bar.update(1)\n",
    "\n",
    "print(exact_match_1.compute())\n",
    "print(exact_match_2.compute())\n",
    "partial_correctness_dataset_1 = Dataset.from_dict(partials_1)\n",
    "partial_correctness_dataset_2 = Dataset.from_dict(partials_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "12324399-6bcc-41f6-a330-463743567c71",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(Dataset({\n",
       "     features: ['predicted_tokens', 'label_tokens', 'decoded_prediction', 'decoded_label'],\n",
       "     num_rows: 10000\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['predicted_tokens', 'label_tokens', 'decoded_prediction', 'decoded_label'],\n",
       "     num_rows: 10000\n",
       " }))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# peek the created dataset\n",
    "partial_correctness_dataset_1, partial_correctness_dataset_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "37512f47-6d69-4de5-9a7b-14a4f02c4722",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "({'predicted_tokens': [[489, 1, 0, 0],\n",
       "   [204, 1, 0, 0],\n",
       "   [1902, 1, 0, 0],\n",
       "   [3, 6039, 1, 0],\n",
       "   [3, 6996, 1, 0]],\n",
       "  'label_tokens': [[489, 1, 0, 0],\n",
       "   [204, 1, 0, 0],\n",
       "   [1902, 1, 0, 0],\n",
       "   [3, 6039, 1, 0],\n",
       "   [3, 10794, 1, 0]],\n",
       "  'decoded_prediction': ['7', '2', '23', '-8', '-18'],\n",
       "  'decoded_label': ['7', '2', '23', '-8', '-17']},\n",
       " {'predicted_tokens': [[489, 1, 0, 0],\n",
       "   [204, 1, 0, 0],\n",
       "   [1902, 1, 0, 0],\n",
       "   [3, 6039, 1, 0],\n",
       "   [3, 10794, 1, 0]],\n",
       "  'label_tokens': [[489, 1, 0, 0],\n",
       "   [204, 1, 0, 0],\n",
       "   [1902, 1, 0, 0],\n",
       "   [3, 6039, 1, 0],\n",
       "   [3, 10794, 1, 0]],\n",
       "  'decoded_prediction': ['7', '2', '23', '-8', '-17'],\n",
       "  'decoded_label': ['7', '2', '23', '-8', '-17']})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "partial_correctness_dataset_1[:5], partial_correctness_dataset_2[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "16fa3b52-c715-4222-84b9-5cbb672620cd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53f5fdc21f1b480da3fe70a0a7a6690d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01eba80c34b54e148aca64049876aa1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/10 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/MarioBarbeque/CyberSolve-LinAlg-1.1-correctness-benchmark/commit/350b1ba2d109755f287fcf92b50dd2ee98564010', commit_message='dataset constructed for benchmarking the partial correctness of our finetuned CyberSolve-LingAlg-1.1 model', commit_description='', oid='350b1ba2d109755f287fcf92b50dd2ee98564010', pr_url=None, repo_url=RepoUrl('https://huggingface.co/datasets/MarioBarbeque/CyberSolve-LinAlg-1.1-correctness-benchmark', endpoint='https://huggingface.co', repo_type='dataset', repo_id='MarioBarbeque/CyberSolve-LinAlg-1.1-correctness-benchmark'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "partial_correctness_dataset_1.push_to_hub(\"CyberSolve-LinAlg-1.1-correctness-benchmark\", commit_message=\"dataset constructed for benchmarking the partial correctness of our finetuned CyberSolve-LingAlg-1.1 model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b67deae7-1067-4fc5-8c06-a90e010e3281",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20291e3ead844713bdb3a288ce844dae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7322e3b56374690af245c1c41b3b454",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/10 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/MarioBarbeque/CyberSolve-LinAlg-1.2-correctness-benchmark/commit/f85e943523a1c35a1834acb8ec4effdb2b9d8026', commit_message='dataset constructed for benchmarking the partial correctness of our finetuned CyberSolve-LingAlg-1.2 model', commit_description='', oid='f85e943523a1c35a1834acb8ec4effdb2b9d8026', pr_url=None, repo_url=RepoUrl('https://huggingface.co/datasets/MarioBarbeque/CyberSolve-LinAlg-1.2-correctness-benchmark', endpoint='https://huggingface.co', repo_type='dataset', repo_id='MarioBarbeque/CyberSolve-LinAlg-1.2-correctness-benchmark'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "partial_correctness_dataset_2.push_to_hub(\"CyberSolve-LinAlg-1.2-correctness-benchmark\", commit_message=\"dataset constructed for benchmarking the partial correctness of our finetuned CyberSolve-LingAlg-1.2 model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "757e820c-0737-45ab-80f8-554b5663ec8a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "1"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Full Training",
   "widgets": {
    "hf_token": {
     "currentValue": "",
     "nuid": "aa822eb9-3b39-4d8a-8fee-0ade0ff5e23e",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": "hf_token",
      "name": "hf_token",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": "hf_token",
      "name": "hf_token",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}