{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "84179bef-9e9d-4140-abe8-fbc24f873279",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Model and Dataset Inspection\n",
    "\n",
    "Before preprocessing, we examine the `flan-t5-large` model and the `DeepMind` 1-dimensional linear algebra dataset to confirm they are compatible for finetuning on a mathematical task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "128913e6-7ce3-4cc2-9359-050d00f7d503",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /databricks/python3/lib/python3.11/site-packages (4.41.2)\nCollecting transformers\n  Obtaining dependency information for transformers from https://files.pythonhosted.org/packages/d0/a7/7eedcf6a359e1e1eff3bc204ad022485aa5d88c08e1e3e0e0aee8a2e2235/transformers-4.47.0-py3-none-any.whl.metadata\n  Downloading transformers-4.47.0-py3-none-any.whl.metadata (43 kB)\n\u001B[?25l     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/43.5 kB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\n\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m43.5/43.5 kB\u001B[0m \u001B[31m1.7 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hRequirement already satisfied: filelock in /databricks/python3/lib/python3.11/site-packages (from transformers) (3.13.4)\nCollecting huggingface-hub<1.0,>=0.24.0 (from transformers)\n  Obtaining dependency information for huggingface-hub<1.0,>=0.24.0 from https://files.pythonhosted.org/packages/44/5a/dc6af87c61f89b23439eb95521e4e99862636cfd538ae12fd36be5483e5f/huggingface_hub-0.26.5-py3-none-any.whl.metadata\n  Downloading huggingface_hub-0.26.5-py3-none-any.whl.metadata (13 kB)\nRequirement already satisfied: numpy>=1.17 in /databricks/python3/lib/python3.11/site-packages (from transformers) (1.23.5)\nRequirement already satisfied: packaging>=20.0 in /databricks/python3/lib/python3.11/site-packages (from transformers) (23.2)\nRequirement already satisfied: pyyaml>=5.1 in /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.11/site-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /databricks/python3/lib/python3.11/site-packages (from transformers) (2022.7.9)\nRequirement already satisfied: requests in /databricks/python3/lib/python3.11/site-packages (from transformers) (2.31.0)\nCollecting tokenizers<0.22,>=0.21 (from transformers)\n  Obtaining dependency information for tokenizers<0.22,>=0.21 from https://files.pythonhosted.org/packages/22/06/69d7ce374747edaf1695a4f61b83570d91cc8bbfc51ccfecf76f56ab4aac/tokenizers-0.21.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n  Downloading tokenizers-0.21.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\nRequirement already satisfied: safetensors>=0.4.1 in /databricks/python3/lib/python3.11/site-packages (from transformers) (0.4.2)\nRequirement already satisfied: tqdm>=4.27 in /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.11/site-packages (from transformers) (4.67.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /databricks/python3/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (2023.5.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /databricks/python3/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (4.10.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /databricks/python3/lib/python3.11/site-packages (from requests->transformers) (2.0.4)\nRequirement already satisfied: idna<4,>=2.5 in /databricks/python3/lib/python3.11/site-packages (from requests->transformers) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.11/site-packages (from requests->transformers) (2.0.7)\nRequirement already satisfied: certifi>=2017.4.17 in /databricks/python3/lib/python3.11/site-packages (from requests->transformers) (2023.7.22)\nDownloading transformers-4.47.0-py3-none-any.whl (10.1 MB)\n\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/10.1 MB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\n\u001B[2K   \u001B[91m━━━━━━━━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m3.7/10.1 MB\u001B[0m \u001B[31m110.0 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\n\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━\u001B[0m \u001B[32m9.6/10.1 MB\u001B[0m \u001B[31m139.1 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\n\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m \u001B[32m10.1/10.1 MB\u001B[0m \u001B[31m138.4 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\n\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m \u001B[32m10.1/10.1 MB\u001B[0m \u001B[31m138.4 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\n\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m10.1/10.1 MB\u001B[0m \u001B[31m61.1 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hDownloading huggingface_hub-0.26.5-py3-none-any.whl (447 kB)\n\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/447.8 kB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\n\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m447.8/447.8 kB\u001B[0m \u001B[31m40.9 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hDownloading tokenizers-0.21.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/3.0 MB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\n\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m \u001B[32m3.0/3.0 MB\u001B[0m \u001B[31m183.3 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\n\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m3.0/3.0 MB\u001B[0m \u001B[31m83.6 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hInstalling collected packages: huggingface-hub, tokenizers, transformers\n  Attempting uninstall: huggingface-hub\n    Found existing installation: huggingface-hub 0.23.4\n    Not uninstalling huggingface-hub at /databricks/python3/lib/python3.11/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-5bce6fee-34bb-4245-a534-c7930bc7b356\n    Can't uninstall 'huggingface-hub'. No files were found to uninstall.\n  Attempting uninstall: tokenizers\n    Found existing installation: tokenizers 0.19.0\n    Not uninstalling tokenizers at /databricks/python3/lib/python3.11/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-5bce6fee-34bb-4245-a534-c7930bc7b356\n    Can't uninstall 'tokenizers'. No files were found to uninstall.\n  Attempting uninstall: transformers\n    Found existing installation: transformers 4.41.2\n    Not uninstalling transformers at /databricks/python3/lib/python3.11/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-5bce6fee-34bb-4245-a534-c7930bc7b356\n    Can't uninstall 'transformers'. No files were found to uninstall.\nSuccessfully installed huggingface-hub-0.26.5 tokenizers-0.21.0 transformers-4.47.0\n\u001B[43mNote: you may need to restart the kernel using %restart_python or dbutils.library.restartPython() to use updated packages.\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "# ensure we have the most recent version of transformers\n",
    "!pip install -U transformers\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "46472736-605a-4ff0-b7b6-6078e04b0759",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/python_shell/dbruntime/huggingface_patches/datasets.py:14: UserWarning: During large dataset downloads, there could be multiple progress bar widgets that can cause performance issues for your notebook or browser. To avoid these issues, use `datasets.utils.logging.disable_progress_bar()` to turn off the progress bars.\n  warnings.warn(\n/databricks/python/lib/python3.11/site-packages/datasets/load.py:1486: FutureWarning: The repository for deepmind/math_dataset contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/deepmind/math_dataset\nYou can avoid this message in future by passing the argument `trust_remote_code=True`.\nPassing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# grab alegbra__linear_1d dataset from DeepMind\n",
    "import datasets\n",
    "\n",
    "train_examples_1d, eval_examples_1d = datasets.load_dataset('deepmind/math_dataset', 'algebra__linear_1d', split=['train', 'test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "38094915-8655-4cb7-b034-ce1baa78f76b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(Dataset({\n",
       "     features: ['question', 'answer'],\n",
       "     num_rows: 1999998\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['question', 'answer'],\n",
       "     num_rows: 10000\n",
       " }))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check split sizes\n",
    "train_examples_1d, eval_examples_1d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a41305b1-e502-4a77-a5c6-7e5caa42470f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "({'question': [\"b'Solve 24 = 1601*c - 1605*c for c.\\\\n'\",\n",
       "   \"b'Solve 657 = -220*t + 1086*t + 22307 for t.\\\\n'\",\n",
       "   \"b'Solve -11*y - 263*y + 3162 = -88*y for y.\\\\n'\",\n",
       "   \"b'Solve 0 = -11*b - 4148 + 4225 for b.\\\\n'\",\n",
       "   \"b'Solve 65*l - 361 + 881 = 0 for l.\\\\n'\"],\n",
       "  'answer': [\"b'-6\\\\n'\", \"b'-25\\\\n'\", \"b'17\\\\n'\", \"b'7\\\\n'\", \"b'-8\\\\n'\"]},\n",
       " {'question': [\"b'Solve -282*d + 929 - 178 = -1223 for d.\\\\n'\",\n",
       "   \"b'Solve 49*l + 45*l - 125 - 63 = 0 for l.\\\\n'\",\n",
       "   \"b'Solve -64*t + 1387 - 848 + 933 = 0 for t.\\\\n'\",\n",
       "   \"b'Solve 75*g = 192*g - 71*g - 79*g - 264 for g.\\\\n'\",\n",
       "   \"b'Solve -34*v + 232*v + 52351 = 48985 for v.\\\\n'\"],\n",
       "  'answer': [\"b'7\\\\n'\", \"b'2\\\\n'\", \"b'23\\\\n'\", \"b'-8\\\\n'\", \"b'-17\\\\n'\"]})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# peek the data\n",
    "train_examples_1d[:5], eval_examples_1d[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d33b334c-e1b4-4c0a-b066-225dccdfb965",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-11 19:47:53.166943: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\nTo enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\nYou are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    }
   ],
   "source": [
    "# instantiate our flan-t5-large model with brain float mixed precision - this model appears better for comparing labels with the DeepMind mathematics dataset than the Gemma text generation model \n",
    "import torch\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-large\")\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-large\", device_map=\"auto\", torch_dtype=torch.bfloat16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "618a66bd-ff5a-4da2-9dd6-3797f4adf8cb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.bfloat16"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# double check the precision of our tensors\n",
    "model.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b86f8fcd-1faf-43c4-a824-2b4e7022f9de",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pad> 4</s>\n"
     ]
    }
   ],
   "source": [
    "# test an inference example\n",
    "input_text = \"Solve 24 = 1601*c - 1605*c for c.\"\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(input_ids)\n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7a71fcaa-8326-40bd-9b75-525a6255e806",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pad> -23</s>\n"
     ]
    }
   ],
   "source": [
    "# another inference example\n",
    "input_text = \"Solve -11*y - 263*y + 3162 = -88*y for y.\"\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(input_ids)\n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "175e0ebc-7382-44e4-87f5-66360c5456b0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "T5ForConditionalGeneration(\n",
       "  (shared): Embedding(32128, 1024)\n",
       "  (encoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32128, 1024)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 16)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=1024, out_features=2816, bias=False)\n",
       "              (wi_1): Linear(in_features=1024, out_features=2816, bias=False)\n",
       "              (wo): Linear(in_features=2816, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-23): 23 x T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=1024, out_features=2816, bias=False)\n",
       "              (wi_1): Linear(in_features=1024, out_features=2816, bias=False)\n",
       "              (wo): Linear(in_features=2816, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (decoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32128, 1024)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 16)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=1024, out_features=2816, bias=False)\n",
       "              (wi_1): Linear(in_features=1024, out_features=2816, bias=False)\n",
       "              (wo): Linear(in_features=2816, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-23): 23 x T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=1024, out_features=2816, bias=False)\n",
       "              (wi_1): Linear(in_features=1024, out_features=2816, bias=False)\n",
       "              (wo): Linear(in_features=2816, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1024, out_features=32128, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this model appears to output a single (albeit often incorrect) answer making it easier to train and compare to the DeepMind mathematics dataset than the Google Gemma text generation model\n",
    "\n",
    "# we examine the architecture as given by the transformers' T5ForConditionalGeneration class\n",
    "# we can leverage this later in detail to estimate the amount of memory required for training\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8943ff00-884d-4969-851d-514c1d2a3b83",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory status: \nGPU 0:\n  Total memory: 15.57 GB\n  Allocated memory: 1.51 GB\n  Reserved memory: 1.55 GB\n  Available memory: 14.02 GB\n"
     ]
    }
   ],
   "source": [
    "# look at the amount of GPU memory used to load the model in bfloat16\n",
    "# for this initial inspection and preprocessing we use a single T4 GPU\n",
    "# we will subsequently beef up our compute for training\n",
    "def mem_status(): \n",
    "    if torch.cuda.is_available():\n",
    "        gpus = torch.cuda.device_count()\n",
    "        print(\"Memory status: \")\n",
    "        for i in range(gpus):\n",
    "            properties = torch.cuda.get_device_properties(i)\n",
    "            total_memory = properties.total_memory / (1024 ** 3)  # Convert to GB\n",
    "            allocated_memory = torch.cuda.memory_allocated(i) / (1024 ** 3)  # Convert to GB\n",
    "            reserved_memory = torch.cuda.memory_reserved(i) / (1024 ** 3)  # Convert to GB\n",
    "            available_memory = total_memory - reserved_memory\n",
    "            print(f\"GPU {i}:\")\n",
    "            print(f\"  Total memory: {total_memory:.2f} GB\")\n",
    "            print(f\"  Allocated memory: {allocated_memory:.2f} GB\")\n",
    "            print(f\"  Reserved memory: {reserved_memory:.2f} GB\")\n",
    "            print(f\"  Available memory: {available_memory:.2f} GB\")\n",
    "    else:\n",
    "        print(\"No GPU available.\")\n",
    "\n",
    "mem_status()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "086efa48-b7e1-430f-a81f-0079fc807121",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##Preprocessing and Prep for Training\n",
    "\n",
    "The `alegbra__linear_1d` split of the DeepMind math dataset comes in a friendly raw format but still requires a fair amount of calculated preprocessing for configuring all 2M examples and labels before passing to the `flan-t5-large` seq2seq model during training.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6d2a4531-be06-48ec-8650-fa50f5c82cd8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# we need to clean up some of the dataset's formatting\n",
    "# it appears all 'questions' and 'answers' are string prefixed with  \"b'  and postfixed with  //n'\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a6c11e3d-8c8c-4833-b148-cc30d10c1f01",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# remove unneeded characters\n",
    "def clean_up_dataset(record):\n",
    "    record['question'] = record['question'][2:-3]\n",
    "    record['answer'] = record['answer'][2:-3]\n",
    "    return record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6e8ca03e-8e32-4f51-a5eb-e0bc5b744607",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00eb31a5e81f4f37add92fca5cda5bc7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1999998 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7f0d2383edb4285977a32059446a4f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# map fn to train and eval datsets\n",
    "train_examples_1d = train_examples_1d.map(clean_up_dataset)\n",
    "eval_examples_1d = eval_examples_1d.map(clean_up_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "70b50210-45c7-41cf-9f57-e73864474bf8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(Dataset({\n",
       "     features: ['question', 'answer'],\n",
       "     num_rows: 1999998\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['question', 'answer'],\n",
       "     num_rows: 10000\n",
       " }))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_examples_1d, eval_examples_1d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5de0a88c-28e1-422c-8cf1-d445b2af8598",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "({'question': ['Solve 24 = 1601*c - 1605*c for c.',\n",
       "   'Solve 657 = -220*t + 1086*t + 22307 for t.',\n",
       "   'Solve -11*y - 263*y + 3162 = -88*y for y.',\n",
       "   'Solve 0 = -11*b - 4148 + 4225 for b.',\n",
       "   'Solve 65*l - 361 + 881 = 0 for l.'],\n",
       "  'answer': ['-6', '-25', '17', '7', '-8']},\n",
       " {'question': ['Solve -282*d + 929 - 178 = -1223 for d.',\n",
       "   'Solve 49*l + 45*l - 125 - 63 = 0 for l.',\n",
       "   'Solve -64*t + 1387 - 848 + 933 = 0 for t.',\n",
       "   'Solve 75*g = 192*g - 71*g - 79*g - 264 for g.',\n",
       "   'Solve -34*v + 232*v + 52351 = 48985 for v.'],\n",
       "  'answer': ['7', '2', '23', '-8', '-17']})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_examples_1d[:5], eval_examples_1d[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bc0994d3-17ec-4308-a746-4fa13596ef80",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# tokenize function\n",
    "def preprocess_function(example):\n",
    "    return tokenizer(example[\"question\"], text_target=example[\"answer\"], return_tensors=\"pt\", padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0c9ee4a0-9425-4098-8ea1-5c74469dc29e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[5175,  162,  997, 3274,  898, 4542, 1935,   75,    3,   18,  898, 3076,\n",
       "         1935,   75,   21,    3,   75,    5,    1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'labels': tensor([[   3, 5783,    1]])}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# validate preprocess_function works as expected\n",
    "test_tokenization = preprocess_function(train_examples_1d[0])\n",
    "test_tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f04c7e66-0fe0-4520-a39b-c29822045030",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24dca63b9f5d4ea88e0e50052b408616",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1999998 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aaeb3336762149ac884e68b8ca55baeb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# tokenize our datasets\n",
    "tokenized_train_dataset = train_examples_1d.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    remove_columns=train_examples_1d.column_names, # remove old column names\n",
    ")\n",
    "tokenized_eval_dataset = eval_examples_1d.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    remove_columns=eval_examples_1d.column_names,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ea35d07d-5494-44bd-9fee-882cd272473a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(Dataset({\n",
       "     features: ['input_ids', 'attention_mask', 'labels'],\n",
       "     num_rows: 1999998\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['input_ids', 'attention_mask', 'labels'],\n",
       "     num_rows: 10000\n",
       " }))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_train_dataset, tokenized_eval_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7463cfff-57b7-4fa3-a3be-ffec4a7c057f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'input_ids': [5175,\n",
       "  162,\n",
       "  997,\n",
       "  3274,\n",
       "  898,\n",
       "  4542,\n",
       "  1935,\n",
       "  75,\n",
       "  3,\n",
       "  18,\n",
       "  898,\n",
       "  3076,\n",
       "  1935,\n",
       "  75,\n",
       "  21,\n",
       "  3,\n",
       "  75,\n",
       "  5,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " 'attention_mask': [1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " 'labels': [3, 5783, 1, 0]}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# peek a tokenized example\n",
    "# notice these are not tensors; the map function stores information in Apache Arrow format, and does not include the Python metadata\n",
    "# see below for setting the output format\n",
    "tokenized_train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f9f47a67-141e-4167-b1b2-b832dcef49d1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# we can use the .set_format(\"torch\", device=\"cuda\") attribute on the dataset to change the output format, (it will not change the data format - which is still Arrow)\n",
    "tokenized_train_dataset.set_format(\"torch\", device=\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e6a0dcda-f5ea-490f-b9ba-5971b1a0fec9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'input_ids': tensor([5175,  162,  997, 3274,  898, 4542, 1935,   75,    3,   18,  898, 3076,\n",
       "         1935,   75,   21,    3,   75,    5,    1,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0], device='cuda:0'),\n",
       " 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0'),\n",
       " 'labels': tensor([   3, 5783,    1,    0], device='cuda:0')}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now this gives us the tensors we desire\n",
    "tokenized_train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ef03ed85-65c2-44b9-b93f-ee74316c0ea8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['▁', '-6', '</s>', '<pad>']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# will having the pad tokens at the end of our labels effect our loss and backpropogation?\n",
    "tokenizer.convert_ids_to_tokens(tokenized_train_dataset[0][\"labels\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "adca1970-b629-49a5-9c0c-6e67243b00ec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "The above is likely a non-issue for the `exact_match` metric we will use, but some consideration need be given to possible evaluation of an `f1` metric for partial correctness. Furthermore, we will need to ultimately collate our data using the `DataCollatorForSeq2Seq` class. In doing so, we will pad and square off all of our `input_ids` and `labels` each to the same length across the dataset. Thus, padding tokens at the output of our labels and predictions is inevitable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fade3966-8b01-4de7-95de-2377b2d0bf72",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/.ipykernel/2223/command-158712734298437-2038954290:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  torch.tensor(tokenized_train_dataset[0][\"input_ids\"]).unsqueeze(0)\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[5175,  162,  997, 3274,  898, 4542, 1935,   75,    3,   18,  898, 3076,\n",
       "         1935,   75,   21,    3,   75,    5,    1,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# before passing these to the model, we need to unsqueeze the input_ids and the labels to add an extra dimension to our tensors\n",
    "torch.tensor(tokenized_train_dataset[0][\"input_ids\"]).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8dbb6257-9f64-4d92-a7b6-9bc6200b641e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted tokens: tensor([   3,    1,    1, 1401], device='cuda:0')\nlabel tokens: tensor([1401,    1,    0,    0], device='cuda:0')\npredicition: 21\nlabel: 21\n>>> The model loss as given by model().loss: 24.75\n>>> exact_match score: 1.0\n>>> f1 score: 0.25\n"
     ]
    }
   ],
   "source": [
    "# before collating our data, we examine the evaluation of the loss on a specific example\n",
    "# a good set of metrics are probably exact_match for complete correctness and f1 for partial correctness (which encodes both precision and recall)\n",
    "import math\n",
    "from evaluate import load\n",
    "\n",
    "# tokenized_example = tokenized_train_dataset[257636]\n",
    "tokenized_example = tokenized_train_dataset[234254]\n",
    "output = model(input_ids=tokenized_example[\"input_ids\"].unsqueeze(0), labels=tokenized_example[\"labels\"].unsqueeze(0))\n",
    "\n",
    "example_cross_entropy_loss = output.loss.item()\n",
    "model_prediction_raw = output.logits.argmax(-1)\n",
    "\n",
    "# peek model predicition vs label\n",
    "print(\"predicted tokens: \" + str(model_prediction_raw[0]))\n",
    "print(\"label tokens: \" + str(tokenized_example['labels']))\n",
    "\n",
    "# check out the model prediction upon decoding\n",
    "print(\"predicition: \" + str(tokenizer.decode(model_prediction_raw[0], skip_special_tokens=True)))\n",
    "# compare the prediction with the label\n",
    "print(\"label: \" + str(tokenizer.decode(tokenized_example['labels'], skip_special_tokens=True)))\n",
    "\n",
    "# the loss stored by the model\n",
    "print(f\">>> The model loss as given by model().loss: {example_cross_entropy_loss}\")\n",
    "\n",
    "f1 = load(\"f1\")\n",
    "exact_match = load(\"exact_match\")\n",
    "\n",
    "# compute exact match score for total correctness\n",
    "exact_match.add(predictions=tokenizer.decode(model_prediction_raw[0], skip_special_tokens=True), references=tokenizer.decode(tokenized_example['labels'], skip_special_tokens=True))\n",
    "print(\">>> exact_match score: \" + str(exact_match.compute()[\"exact_match\"]))\n",
    "\n",
    "# compute the f1 score for partial correctness\n",
    "# TODO what averaging method is best for f1? Micro appears to average on a token-to-token mapping\n",
    "# NOTE in some instances, the decoded output gives an exact match, where generally one would expect the f1 score to be 1.0, but occasionally the raw tensor output (containing special tokens) can be such that the f1 score is nontrivially not equal to 1.0\n",
    "# NOTE we need to consider the above and think about a solid method for rectifying this issue in order to properly intrepet the partial correctness score - one could potentially remove special tokens from perdicitons and labels, but this does not guarnatee that we have tensors of equal lenght for comparison\n",
    "print(\">>> f1 score: \" + str(f1.compute(predictions=model_prediction_raw[0], references=tokenized_example['labels'], average='micro')[\"f1\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "42ad6639-e75f-44d4-a1f2-b7b1ea4bfafb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['▁', '</s>', '</s>', '▁21']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the example above using record 234254 outputs the correct answer, but an f1 score of .25\n",
    "tokenizer.convert_ids_to_tokens([ 3,    1,    1, 1401])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4f53727e-7a84-41b0-8222-6af557ae3647",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'eos_token': '</s>',\n",
       " 'unk_token': '<unk>',\n",
       " 'pad_token': '<pad>',\n",
       " 'additional_special_tokens': ['<extra_id_0>',\n",
       "  '<extra_id_1>',\n",
       "  '<extra_id_2>',\n",
       "  '<extra_id_3>',\n",
       "  '<extra_id_4>',\n",
       "  '<extra_id_5>',\n",
       "  '<extra_id_6>',\n",
       "  '<extra_id_7>',\n",
       "  '<extra_id_8>',\n",
       "  '<extra_id_9>',\n",
       "  '<extra_id_10>',\n",
       "  '<extra_id_11>',\n",
       "  '<extra_id_12>',\n",
       "  '<extra_id_13>',\n",
       "  '<extra_id_14>',\n",
       "  '<extra_id_15>',\n",
       "  '<extra_id_16>',\n",
       "  '<extra_id_17>',\n",
       "  '<extra_id_18>',\n",
       "  '<extra_id_19>',\n",
       "  '<extra_id_20>',\n",
       "  '<extra_id_21>',\n",
       "  '<extra_id_22>',\n",
       "  '<extra_id_23>',\n",
       "  '<extra_id_24>',\n",
       "  '<extra_id_25>',\n",
       "  '<extra_id_26>',\n",
       "  '<extra_id_27>',\n",
       "  '<extra_id_28>',\n",
       "  '<extra_id_29>',\n",
       "  '<extra_id_30>',\n",
       "  '<extra_id_31>',\n",
       "  '<extra_id_32>',\n",
       "  '<extra_id_33>',\n",
       "  '<extra_id_34>',\n",
       "  '<extra_id_35>',\n",
       "  '<extra_id_36>',\n",
       "  '<extra_id_37>',\n",
       "  '<extra_id_38>',\n",
       "  '<extra_id_39>',\n",
       "  '<extra_id_40>',\n",
       "  '<extra_id_41>',\n",
       "  '<extra_id_42>',\n",
       "  '<extra_id_43>',\n",
       "  '<extra_id_44>',\n",
       "  '<extra_id_45>',\n",
       "  '<extra_id_46>',\n",
       "  '<extra_id_47>',\n",
       "  '<extra_id_48>',\n",
       "  '<extra_id_49>',\n",
       "  '<extra_id_50>',\n",
       "  '<extra_id_51>',\n",
       "  '<extra_id_52>',\n",
       "  '<extra_id_53>',\n",
       "  '<extra_id_54>',\n",
       "  '<extra_id_55>',\n",
       "  '<extra_id_56>',\n",
       "  '<extra_id_57>',\n",
       "  '<extra_id_58>',\n",
       "  '<extra_id_59>',\n",
       "  '<extra_id_60>',\n",
       "  '<extra_id_61>',\n",
       "  '<extra_id_62>',\n",
       "  '<extra_id_63>',\n",
       "  '<extra_id_64>',\n",
       "  '<extra_id_65>',\n",
       "  '<extra_id_66>',\n",
       "  '<extra_id_67>',\n",
       "  '<extra_id_68>',\n",
       "  '<extra_id_69>',\n",
       "  '<extra_id_70>',\n",
       "  '<extra_id_71>',\n",
       "  '<extra_id_72>',\n",
       "  '<extra_id_73>',\n",
       "  '<extra_id_74>',\n",
       "  '<extra_id_75>',\n",
       "  '<extra_id_76>',\n",
       "  '<extra_id_77>',\n",
       "  '<extra_id_78>',\n",
       "  '<extra_id_79>',\n",
       "  '<extra_id_80>',\n",
       "  '<extra_id_81>',\n",
       "  '<extra_id_82>',\n",
       "  '<extra_id_83>',\n",
       "  '<extra_id_84>',\n",
       "  '<extra_id_85>',\n",
       "  '<extra_id_86>',\n",
       "  '<extra_id_87>',\n",
       "  '<extra_id_88>',\n",
       "  '<extra_id_89>',\n",
       "  '<extra_id_90>',\n",
       "  '<extra_id_91>',\n",
       "  '<extra_id_92>',\n",
       "  '<extra_id_93>',\n",
       "  '<extra_id_94>',\n",
       "  '<extra_id_95>',\n",
       "  '<extra_id_96>',\n",
       "  '<extra_id_97>',\n",
       "  '<extra_id_98>',\n",
       "  '<extra_id_99>']}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NOTE this method is erroneous - we are not guaranteed predictions and labels of the same length\n",
    "# to account for the issue of special tokens skeweing the partial correctness f1 score as noted above, we can remove all special tokens in the predictions and labels tensors before computing the f1 score\n",
    "\n",
    "# first we examine all special tokens\n",
    "tokenizer.special_tokens_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e9f02af0-fa90-403f-a1e5-0bf8ee09d012",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'eos_token': 1, 'unk_token': 2, 'pad_token': 0}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the ids of the most common special tokens\n",
    "{k: tokenizer.convert_tokens_to_ids(v) for k, v in tokenizer.special_tokens_map.items() if k != 'additional_special_tokens'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a31a1c93-9385-42d1-b7ca-d434794831d1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[32099,\n",
       " 32098,\n",
       " 32097,\n",
       " 32096,\n",
       " 32095,\n",
       " 32094,\n",
       " 32093,\n",
       " 32092,\n",
       " 32091,\n",
       " 32090,\n",
       " 32089,\n",
       " 32088,\n",
       " 32087,\n",
       " 32086,\n",
       " 32085,\n",
       " 32084,\n",
       " 32083,\n",
       " 32082,\n",
       " 32081,\n",
       " 32080,\n",
       " 32079,\n",
       " 32078,\n",
       " 32077,\n",
       " 32076,\n",
       " 32075,\n",
       " 32074,\n",
       " 32073,\n",
       " 32072,\n",
       " 32071,\n",
       " 32070,\n",
       " 32069,\n",
       " 32068,\n",
       " 32067,\n",
       " 32066,\n",
       " 32065,\n",
       " 32064,\n",
       " 32063,\n",
       " 32062,\n",
       " 32061,\n",
       " 32060,\n",
       " 32059,\n",
       " 32058,\n",
       " 32057,\n",
       " 32056,\n",
       " 32055,\n",
       " 32054,\n",
       " 32053,\n",
       " 32052,\n",
       " 32051,\n",
       " 32050,\n",
       " 32049,\n",
       " 32048,\n",
       " 32047,\n",
       " 32046,\n",
       " 32045,\n",
       " 32044,\n",
       " 32043,\n",
       " 32042,\n",
       " 32041,\n",
       " 32040,\n",
       " 32039,\n",
       " 32038,\n",
       " 32037,\n",
       " 32036,\n",
       " 32035,\n",
       " 32034,\n",
       " 32033,\n",
       " 32032,\n",
       " 32031,\n",
       " 32030,\n",
       " 32029,\n",
       " 32028,\n",
       " 32027,\n",
       " 32026,\n",
       " 32025,\n",
       " 32024,\n",
       " 32023,\n",
       " 32022,\n",
       " 32021,\n",
       " 32020,\n",
       " 32019,\n",
       " 32018,\n",
       " 32017,\n",
       " 32016,\n",
       " 32015,\n",
       " 32014,\n",
       " 32013,\n",
       " 32012,\n",
       " 32011,\n",
       " 32010,\n",
       " 32009,\n",
       " 32008,\n",
       " 32007,\n",
       " 32006,\n",
       " 32005,\n",
       " 32004,\n",
       " 32003,\n",
       " 32002,\n",
       " 32001,\n",
       " 32000]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# examine the ids of the additional special tokens\n",
    "[tokenizer.convert_tokens_to_ids(k) for k in tokenizer.special_tokens_map[\"additional_special_tokens\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "71f8409e-ceaa-4d06-bb8f-1fff5ca02ea2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([   3, 1401], device='cuda:0')"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lets try removing the common special tokens from the tensor to compare f1 scores\n",
    "\n",
    "# we can use the torch.masked_select() fn to remove elements less than 2 - the eos and padding tokens\n",
    "mask = model_prediction_raw[0] >= 2\n",
    "new_tensor = torch.masked_select(model_prediction_raw[0], mask)\n",
    "\n",
    "new_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "be17d3b5-25ba-4fe6-8887-ca897f178236",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-158712734298444>, line 5\u001B[0m\n",
       "\u001B[1;32m      3\u001B[0m prediciton_mask \u001B[38;5;241m=\u001B[39m model_prediction_raw[\u001B[38;5;241m0\u001B[39m] \u001B[38;5;241m>\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m2\u001B[39m\n",
       "\u001B[1;32m      4\u001B[0m label_mask \u001B[38;5;241m=\u001B[39m tokenized_example[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlabels\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m>\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m2\u001B[39m\n",
       "\u001B[0;32m----> 5\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m>>> f1 score after masking special tokens: \u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mstr\u001B[39m(f1\u001B[38;5;241m.\u001B[39mcompute(predictions\u001B[38;5;241m=\u001B[39mtorch\u001B[38;5;241m.\u001B[39mmasked_select(model_prediction_raw[\u001B[38;5;241m0\u001B[39m], prediciton_mask), references\u001B[38;5;241m=\u001B[39mtorch\u001B[38;5;241m.\u001B[39mmasked_select(tokenized_example[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlabels\u001B[39m\u001B[38;5;124m'\u001B[39m], label_mask), average\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmicro\u001B[39m\u001B[38;5;124m\"\u001B[39m)))\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/evaluate/module.py:455\u001B[0m, in \u001B[0;36mEvaluationModule.compute\u001B[0;34m(self, predictions, references, **kwargs)\u001B[0m\n",
       "\u001B[1;32m    452\u001B[0m compute_kwargs \u001B[38;5;241m=\u001B[39m {k: kwargs[k] \u001B[38;5;28;01mfor\u001B[39;00m k \u001B[38;5;129;01min\u001B[39;00m kwargs \u001B[38;5;28;01mif\u001B[39;00m k \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_feature_names()}\n",
       "\u001B[1;32m    454\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28many\u001B[39m(v \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01mfor\u001B[39;00m v \u001B[38;5;129;01min\u001B[39;00m inputs\u001B[38;5;241m.\u001B[39mvalues()):\n",
       "\u001B[0;32m--> 455\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39madd_batch(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39minputs)\n",
       "\u001B[1;32m    456\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_finalize()\n",
       "\u001B[1;32m    458\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcache_file_name \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/evaluate/module.py:546\u001B[0m, in \u001B[0;36mEvaluationModule.add_batch\u001B[0;34m(self, predictions, references, **kwargs)\u001B[0m\n",
       "\u001B[1;32m    539\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[1;32m    540\u001B[0m     error_msg \u001B[38;5;241m=\u001B[39m (\n",
       "\u001B[1;32m    541\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mPredictions and/or references don\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mt match the expected format.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n",
       "\u001B[1;32m    542\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mExpected format: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mselected_feature_format\u001B[38;5;250m \u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m,\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n",
       "\u001B[1;32m    543\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mInput predictions: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00msummarize_if_long_list(predictions)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m,\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n",
       "\u001B[1;32m    544\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mInput references: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00msummarize_if_long_list(references)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n",
       "\u001B[1;32m    545\u001B[0m     )\n",
       "\u001B[0;32m--> 546\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(error_msg) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
       "\n",
       "\u001B[0;31mValueError\u001B[0m: Mismatch in the number of predictions (2) and references (1)"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": {
        "ename": "ValueError",
        "evalue": "Mismatch in the number of predictions (2) and references (1)"
       },
       "metadata": {
        "errorSummary": "<span class='ansi-red-fg'>ValueError</span>: Mismatch in the number of predictions (2) and references (1)"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [
        "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
        "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
        "File \u001B[0;32m<command-158712734298444>, line 5\u001B[0m\n\u001B[1;32m      3\u001B[0m prediciton_mask \u001B[38;5;241m=\u001B[39m model_prediction_raw[\u001B[38;5;241m0\u001B[39m] \u001B[38;5;241m>\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m2\u001B[39m\n\u001B[1;32m      4\u001B[0m label_mask \u001B[38;5;241m=\u001B[39m tokenized_example[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlabels\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m>\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m2\u001B[39m\n\u001B[0;32m----> 5\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m>>> f1 score after masking special tokens: \u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mstr\u001B[39m(f1\u001B[38;5;241m.\u001B[39mcompute(predictions\u001B[38;5;241m=\u001B[39mtorch\u001B[38;5;241m.\u001B[39mmasked_select(model_prediction_raw[\u001B[38;5;241m0\u001B[39m], prediciton_mask), references\u001B[38;5;241m=\u001B[39mtorch\u001B[38;5;241m.\u001B[39mmasked_select(tokenized_example[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlabels\u001B[39m\u001B[38;5;124m'\u001B[39m], label_mask), average\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmicro\u001B[39m\u001B[38;5;124m\"\u001B[39m)))\n",
        "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/evaluate/module.py:455\u001B[0m, in \u001B[0;36mEvaluationModule.compute\u001B[0;34m(self, predictions, references, **kwargs)\u001B[0m\n\u001B[1;32m    452\u001B[0m compute_kwargs \u001B[38;5;241m=\u001B[39m {k: kwargs[k] \u001B[38;5;28;01mfor\u001B[39;00m k \u001B[38;5;129;01min\u001B[39;00m kwargs \u001B[38;5;28;01mif\u001B[39;00m k \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_feature_names()}\n\u001B[1;32m    454\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28many\u001B[39m(v \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01mfor\u001B[39;00m v \u001B[38;5;129;01min\u001B[39;00m inputs\u001B[38;5;241m.\u001B[39mvalues()):\n\u001B[0;32m--> 455\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39madd_batch(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39minputs)\n\u001B[1;32m    456\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_finalize()\n\u001B[1;32m    458\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcache_file_name \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
        "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/evaluate/module.py:546\u001B[0m, in \u001B[0;36mEvaluationModule.add_batch\u001B[0;34m(self, predictions, references, **kwargs)\u001B[0m\n\u001B[1;32m    539\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    540\u001B[0m     error_msg \u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m    541\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mPredictions and/or references don\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mt match the expected format.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    542\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mExpected format: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mselected_feature_format\u001B[38;5;250m \u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m,\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    543\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mInput predictions: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00msummarize_if_long_list(predictions)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m,\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    544\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mInput references: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00msummarize_if_long_list(references)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    545\u001B[0m     )\n\u001B[0;32m--> 546\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(error_msg) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
        "\u001B[0;31mValueError\u001B[0m: Mismatch in the number of predictions (2) and references (1)"
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# NOTE this is a perfect example of where our analysis fails\n",
    "# now we apply this to the previous example\n",
    "prediciton_mask = model_prediction_raw[0] >= 2\n",
    "label_mask = tokenized_example['labels'] >= 2\n",
    "print(\">>> f1 score after masking special tokens: \" + str(f1.compute(predictions=torch.masked_select(model_prediction_raw[0], prediciton_mask), references=torch.masked_select(tokenized_example['labels'], label_mask), average=\"micro\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9a1e57f0-c3d3-4857-b3f9-b10987978293",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> f1 score after masking special tokens: {'f1': 1.0}\n>>> f1 score after masking special tokens: {'f1': 0.3333333333333333}\n"
     ]
    }
   ],
   "source": [
    "# to even get a notion of partial correctness, we need to keep track of each predicitons f1 score\n",
    "# one can envision doing this with the test example below\n",
    "for predictions, references in zip([[1,2,3],[1,2,3]], [[1,2,3],[3,2,1]]):\n",
    "    print(\">>> f1 score after masking special tokens: \" + str(f1.compute(predictions=predictions, references=references, average=\"micro\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3f0f2236-e4c8-4458-85fd-f088f2896b37",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 1, 2, 3]\n[1, 2, 3, 1, 2, 4]\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'f1': array([1.        , 1.        , 0.66666667, 0.        ])}"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# or one could compute an f1 score of the entire datasets token prediciton as so:\n",
    "from itertools import chain\n",
    "\n",
    "f1_test = load(\"f1\")\n",
    "test_preds = [[1,2,3],[1,2,3]]\n",
    "test_labels = [[1,2,3],[1,2,4]]\n",
    "predictions = []\n",
    "labels  = []\n",
    "for data in test_preds:\n",
    "    for i in data:\n",
    "        predictions.append(i)\n",
    "for data in test_labels:\n",
    "    for i in data:\n",
    "        labels.append(i)\n",
    "print(predictions)\n",
    "print(labels)\n",
    "f1_test.add_batch(predictions=predictions, references=labels)\n",
    "f1_test.compute(average=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3f30b127-02d6-4080-9f6d-f040c02b910b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Initial Erroneous Attempt** (see below): This seems like a fine method for comparing the relevant tokens upon decoding. We can apply this masking to each prediction in order to get a measure of the partial correctness relative to the label tokens in the form of an f1 score with a \"micro\" averaging, independent of the special padding and eos tokens. Yet, one small caveat remains - that of the space token denoted `\"_\"` with token id `3`. As demonstrated above in the comparison between the prediction and label of record # `234254`  (and perhaps many others) the full prediction including the space token gives an exact match upon decoding, but the token-to-token 'micro' averaged f1 score will still be less than the expected 1.0. We will leave this as such for now, just as we have left the desired unknown token, given that the space token could play a valuable role in decoding outputs. We will have to consider how to intrepret our f1 score with such a subtlety.\n",
    "\n",
    "**Correction to the above**: During preprocessing, an attempt to mask and remove special tokens from both predictions and labels seemed preliminarily to allow a better f1 score for intrepreting partial correctness of predictions. It turns out that such a method is erroneous given that the number of non-special predicition tokens does not inherently match the number of non-special label tokens leading to tensors or different length during computation of the f1 score - inevitably raising an error. As such, we will leave the f1 score as a comparison inclusive of special tokens and reconsider how one might intrepret such a metric. Perhaps one might learn something interesting about how the model predictions, both before and after decoding, shed light on the learning and mathematical problem solving abilities of the model. Nonetheless, the presence of special tokens in each of our predictions, even when decoded to the correct exact match output, will certainly bias our partial correctness scores.\n",
    "\n",
    "**TODO**: what remains to be decided is how we will measure the f1 score. There are various ways we can do this: \n",
    "- 1) Between each predicition and its label (allowing for a direct intrepretation of the partial correctness for each prediction), giving a total of **len(dataset)** f1 scores\n",
    "- 2) Across the amalgamated sets of predicted and label tokens (allowing for a large scale interpretation of how well the model predicted all individual tokens), giving a **single** f1 score \n",
    "- 3) Both of the above\n",
    "\n",
    "Such a consideration for measuring the partial correctness of predictions might lead us to also consider how the *precision* and *recall* metrics independetly score. Given the convulted nature of these metrics, perhaps it is best to account for partial correctness only in the benchmarking and evaluation cases where we are truly measuring the effectiveness of the model. The smaller size of the evaluation dataset might similarly allow us to efficiently keep track of each individual partial correctness scores (case #1 above).  After all, the model's inherent cross-entropy loss is the loss which our optimizer will improve the model through backpropogation during training while the exact match score for full correctness remains indeed our primary metric for performance intrepretation. We can retain the exact match score during training as an intermediate evaluation of its performance across training epochs.\n",
    "\n",
    "Returning to the preprocessing at hand, we note that we will ultimately publish our cleaned dataset to the \uD83E\uDD17 hub. This will ease the data loading process into each of the separate benchmarking, training, and evaluation notebooks - substantially so for the training processes which will utilize \uD83E\uDD17 accelerate for distributed training. In each step above, after downloading our training-ready data, we will configure our data loaders. Before then, and as a final preprocessing step, we must investigate the ultimate shape of our data a bit further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0c186786-6ff2-4e29-a067-ad9cefaae25b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# preliminarily, we convert our tokenized datasets' data format to numpy\n",
    "# this will ultimately be required under the hood by the DataCollatorForSeq2Seq class for padding the labels to the same length in each of our batches \n",
    "tokenized_train_dataset.set_format(\"numpy\")\n",
    "tokenized_eval_dataset.set_format(\"numpy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "740d624a-399f-4209-891e-06090ab902fa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40\n"
     ]
    }
   ],
   "source": [
    "# do we need to truncate any of our examples? the max context window of flan-t5-large is 1024\n",
    "max_length = 0\n",
    "for seq_tokens in tokenized_train_dataset[\"input_ids\"]:\n",
    "    if len(seq_tokens) > max_length: max_length = len(seq_tokens)\n",
    "print(max_length) # 40 tokens\n",
    "\n",
    "# this cells confirms that we do not have any questions which need to be truncated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9a1b60a0-b000-48dd-8da9-549a86168d7d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38\n"
     ]
    }
   ],
   "source": [
    "# similarly for our eval dataset\n",
    "max_length = 0\n",
    "for seq_tokens in tokenized_eval_dataset[\"input_ids\"]:\n",
    "    if len(seq_tokens) > max_length: max_length = len(seq_tokens)\n",
    "print(max_length) # 38 tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c3b770e9-330a-4796-b4e5-31c7a6dbd4bf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max label length: 4\nmin label length: 3\n"
     ]
    }
   ],
   "source": [
    "# now we need to investigate how we will handle labels with 3 tokens (or any number different from the majority of labels with 4 tokens)  \n",
    "# presumably the data collator will pad the labels in each of our batches to 4, but are there instances where we have either more than or less than 4 tokens?\n",
    "# let's examine the full datasets for the min and max token lenghts of our labels\n",
    "\n",
    "max_length = 0\n",
    "for seq_tokens in tokenized_train_dataset[\"labels\"]:\n",
    "    if len(seq_tokens) > max_length: max_length = len(seq_tokens)\n",
    "print(\"max label length: \" + str(max_length))\n",
    "\n",
    "min_length = 40 # number we previously found for max length\n",
    "for seq_tokens in tokenized_train_dataset[\"labels\"]:\n",
    "    if len(seq_tokens) < min_length: min_length = len(seq_tokens)\n",
    "print(\"min label length: \" + str(min_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1ffd6464-6e2f-45b7-a6aa-7c8c66e576fb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max label length: 4\nmin label length: 4\n"
     ]
    }
   ],
   "source": [
    "# repeat the same process for our eval set\n",
    "max_length = 0\n",
    "for seq_tokens in tokenized_eval_dataset[\"labels\"]:\n",
    "    if len(seq_tokens) > max_length: max_length = len(seq_tokens)\n",
    "print(\"max label length: \" + str(max_length))\n",
    "\n",
    "min_length = 40 # number we previously found for max length\n",
    "for seq_tokens in tokenized_eval_dataset[\"labels\"]:\n",
    "    if len(seq_tokens) < min_length: min_length = len(seq_tokens)\n",
    "print(\"min label length: \" + str(min_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6e0a592b-4aac-455a-a1e8-30f50580476b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "666000"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# all 10k records in the eval dataset have a label of 4 tokens, so we are square there, but\n",
    "# how many instances of 3 are there in the train dataset?\n",
    "\n",
    "count_of_3 = 0\n",
    "for seq_tokens in tokenized_train_dataset[\"labels\"]:\n",
    "    if len(seq_tokens) == 3: count_of_3 += 1\n",
    "count_of_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "764cc3a4-77a6-4f9b-b782-529918323151",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.333000333000333%\n"
     ]
    }
   ],
   "source": [
    "# this is clearly a highly nontrivial number of labels of length 3\n",
    "# indeed these labels make up huge percentage of the total train dataset\n",
    "\n",
    "print(str(666000 / len(tokenized_train_dataset)) +\"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4108acac-fbe5-4d53-9e2e-5770b9294434",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "In recognition of the large presence of tensor labels with length 3, it is necessary for us to pad all of our labels to length 4 at the very least. We utilize the powerful `pad_to_multiple_of` parameter in the `DataCollatorForSeq2Seq` class whose documentation references the following: \n",
    "\n",
    "\n",
    "*\"This is especially useful to enable the use of Tensor Cores on NVIDIA hardware with compute capability >= 7.0 (Volta).\"* \n",
    "\n",
    "**Note**: before this research project, the above documentation was slightly erroneous and previously listed the the Nvidia Volta architecture as given by compute capability *7.5*. Indeed, we researched this typo further,  put out an issue on the Hugging Face transformers GitHub repo, and fix this error ourselves. The corresponding issue and PR (both now closed) can be found here, respectively: [https://github.com/huggingface/transformers/issues/35174](https://github.com/huggingface/transformers/issues/35174) and [https://github.com/huggingface/transformers/pull/35188](https://github.com/huggingface/transformers/pull/35188).\n",
    "\n",
    "In passing the `pad_to_multiple_of=2` arg to our data collator, we guarantee that any labels of length 3 are padded to length 4 and can be easily compared with our predictions throughout training and evaluation. With this rectangular padding, we are able then to leverage the tensor cores of either a T4 GPU or a V100 GPU to expedite our training. The Nvidia documentation for the T4 and V100 GPUs states that, respectively, we can achieve up to 65 fp16 TFLOPS and up to 125 TFLOPS during computation.\n",
    "\n",
    "Furthermore, we cite here a nice article detailing how to improve performance on Nvidia tensor cores: [https://developer.nvidia.com/blog/optimizing-gpu-performance-tensor-cores/](https://developer.nvidia.com/blog/optimizing-gpu-performance-tensor-cores/). This article and the more general NVIDIA Deep Learning Performance Guide detail how configuring the batch size and the number of inputs and outputs as a multiple of 8 will accelerate computation.\n",
    "\n",
    "Lastly, we require mixed precision to leverage the acceleration of tensor cores. We have already double checked the default types of our tensors with the `.dtype` attribute. The weights in our model should already be `torch.bfloat16` from loading our flan-t5-large model with the `torch_dtype=torch.bfloat16` parameter passed. One can ensure the default tensor type by using the `torch.set_default_device()` command.\n",
    "\n",
    "With all of this information, we are now ready to properly and wisely configure our data loaders for accelerated benchmarking, training, and evaluation. Rather than doing this here, we will dedicate separate notebooks to each of these causes. We conclude this preprocessing notebook by publishing our cleaned data to the hub at `MarioBarbeque/DeepMind-LinAlg-1D-train` and `MarioBarbeque/DeepMind-LinAlg-1D-eval`. In each of the subsequent notebooks this dataset will be loaded and configured in dataloaders for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c62f0079-907c-4e0a-a771-94d13174fe3b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Now lets push this model to our Hub\n",
    "\n",
    "dbutils.widgets.text(\"hf_token\", \"\", \"hf_token\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5d9f9e2a-4512-423b-9af8-046d2effddca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\r\nToken is valid (permission: fineGrained).\r\nThe token `Personal Hub Token` has been saved to /Volumes/workspace_dogfood/jgr/hugging_face_cache/stored_tokens\r\nYour token has been saved to /Volumes/workspace_dogfood/jgr/hugging_face_cache/token\r\nLogin successful.\r\nThe current active token is: `Personal Hub Token`\r\n"
     ]
    }
   ],
   "source": [
    "hf_token = dbutils.widgets.get(\"hf_token\")\n",
    "!huggingface-cli login --token $hf_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2a3c5f45-3f2e-4899-b1dd-65cf8d62fe49",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca6f27bbf5d54df7b622f341449bf849",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c82a966cabce46d9b90e4d4dd78c0f8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/2000 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/MarioBarbeque/DeepMind-LinAlg-1D-train/commit/8620397bf3c0a116747c7c51772a0fcc06bcf45c', commit_message='Cleaned, tokenized, and DataLoader-ready 1D linear algebra TRAINING dataset from DeepMind; for use with FLAN-T5', commit_description='', oid='8620397bf3c0a116747c7c51772a0fcc06bcf45c', pr_url=None, repo_url=RepoUrl('https://huggingface.co/datasets/MarioBarbeque/DeepMind-LinAlg-1D-train', endpoint='https://huggingface.co', repo_type='dataset', repo_id='MarioBarbeque/DeepMind-LinAlg-1D-train'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_train_dataset.push_to_hub(\"DeepMind-LinAlg-1D-train\", commit_message=\"Cleaned, tokenized, and DataLoader-ready 1D linear algebra TRAINING dataset from DeepMind; for use with FLAN-T5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7dcd9f4e-eed9-4e82-af7d-41705b138cac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2cd715e7f6b84558be17da5f49c7b366",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c115e6ced9c4fcab98ac2bd2bdb03bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/10 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/MarioBarbeque/DeepMind-LinAlg-1D-eval/commit/a00a253a845116f4469082dc063e1c15ba193bee', commit_message='Cleaned, tokenized, and DataLoader-ready 1D linear algebra EVAL dataset from DeepMind; for use with FLAN-T5', commit_description='', oid='a00a253a845116f4469082dc063e1c15ba193bee', pr_url=None, repo_url=RepoUrl('https://huggingface.co/datasets/MarioBarbeque/DeepMind-LinAlg-1D-eval', endpoint='https://huggingface.co', repo_type='dataset', repo_id='MarioBarbeque/DeepMind-LinAlg-1D-eval'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_eval_dataset.push_to_hub(\"DeepMind-LinAlg-1D-eval\", commit_message=\"Cleaned, tokenized, and DataLoader-ready 1D linear algebra EVAL dataset from DeepMind; for use with FLAN-T5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "825837d5-5ad4-4a71-b627-f0434891b1dc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "See the Benchmarking notebook next."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "1"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Preprocessing and Inspection",
   "widgets": {
    "hf_token": {
     "currentValue": "",
     "nuid": "f8e5e807-70a2-46c8-bc5b-35654fd89e3c",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": "hf_token",
      "name": "hf_token",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": "hf_token",
      "name": "hf_token",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}